---
title: Dataset Preparation
draft: false
summary: Backtest statistics are essential for evaluating the efficacy of investment strategies.
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

# Dataset Preparation

## Dataset Preparation

The preparation of datasets is a fundamental step in the development and evaluation of recommender systems. This process involves acquiring, processing, and splitting data into appropriate sets for training and testing models. Here, we focus on the MovieLens dataset, a widely used resource in recommendation research, to illustrate these steps.

## The MovieLens Dataset

The MovieLens dataset is one of the most popular datasets for recommendation research. It was created in 1997 by GroupLens, a research lab at the University of Minnesota, to collect movie rating data for research purposes. The dataset has been instrumental in numerous studies, particularly in personalized recommendation and social psychology.

### Getting the Data

The MovieLens dataset is hosted by the GroupLens website. Several versions are available, but we will focus on the MovieLens 100K dataset. This dataset comprises 100,000 ratings, ranging from 1 to 5 stars, from 943 users on 1,682 movies. It has been cleaned to ensure that each user has rated at least 20 movies. Additionally, some demographic information, such as age and gender, and movie genres are available. To use this dataset, you can download the `ml-100k.zip` file and extract the `u.data` file, which contains the ratings in CSV format.

```python
import os
import pandas as pd
from mxnet import gluon, np
from d2l import mxnet as d2l

# Download and load the MovieLens 100k dataset
d2l.DATA_HUB['ml-100k'] = (
    'https://files.grouplens.org/datasets/movielens/ml-100k.zip',
    'cd4dcac4241c8a4ad7badc7ca635da8a69dddb83')

def read_data_ml100k():
    data_dir = d2l.download_extract('ml-100k')
    names = ['user_id', 'item_id', 'rating', 'timestamp']
    data = pd.read_csv(os.path.join(data_dir, 'u.data'), sep='\t',
                       names=names, engine='python')
    num_users = data.user_id.unique().shape[0]
    num_items = data.item_id.unique().shape[0]
    return data, num_users, num_items
```

### Statistics of the Dataset

Once the data is loaded, it's essential to inspect the first few records to understand its structure and ensure it has been loaded correctly. The dataset consists of four columns: `user_id`, `item_id`, `rating`, and `timestamp`. An interaction matrix can be constructed from this data, representing the ratings given by users to items. However, most of the matrix values are unknown due to the dataset's sparsity, which is approximately 93.7%. This sparsity poses a challenge in building recommender systems, often requiring additional information to improve the model's performance.

```python
data, num_users, num_items = read_data_ml100k()
sparsity = 1 - len(data) / (num_users * num_items)
print(f'number of users: {num_users}, number of items: {num_items}')
print(f'matrix sparsity: {sparsity:f}')
print(data.head(5))
```

The distribution of ratings typically follows a normal distribution, with most ratings concentrated around 3-4 stars.

```python
d2l.plt.hist(data['rating'], bins=5, ec='black')
d2l.plt.xlabel('Rating')
d2l.plt.ylabel('Count')
d2l.plt.title('Distribution of Ratings in MovieLens 100K')
d2l.plt.show()
```

### Splitting the Dataset

The dataset is split into training and test sets using two modes: `random` and `seq-aware`. In `random` mode, interactions are split randomly, while `seq-aware` mode considers the temporal order of ratings, using the most recent rating for testing.

```python
def split_data_ml100k(data, num_users, num_items, split_mode='random', test_ratio=0.1):
    if split_mode == 'seq-aware':
        train_items, test_items, train_list = {}, {}, []
        for line in data.itertuples():
            u, i, rating, time = line[1], line[2], line[3], line[4]
            train_items.setdefault(u, []).append((u, i, rating, time))
            if u not in test_items or test_items[u][-1] < time:
                test_items[u] = (i, rating, time)
        for u in range(1, num_users + 1):
            train_list.extend(sorted(train_items[u], key=lambda k: k[3]))
        test_data = [(key, *value) for key, value in test_items.items()]
        train_data = [item for item in train_list if item not in test_data]
        train_data = pd.DataFrame(train_data)
        test_data = pd.DataFrame(test_data)
    else:
        mask = [True if x == 1 else False for x in np.random.uniform(0, 1, (len(data))) < 1 - test_ratio]
        neg_mask = [not x for x in mask]
        train_data, test_data = data[mask], data[neg_mask]
    return train_data, test_data
```

### Loading the Data

After splitting, the data is loaded into lists and matrices for further processing. The function below reads the data and organizes it into user and item indices, scores, and interaction matrices, allowing for both explicit and implicit feedback handling.

```python
def load_data_ml100k(data, num_users, num_items, feedback='explicit'):
    users, items, scores = [], [], []
    inter = np.zeros((num_items, num_users)) if feedback == 'explicit' else {}
    for line in data.itertuples():
        user_index, item_index = int(line[1] - 1), int(line[2] - 1)
        score = int(line[3]) if feedback == 'explicit' else 1
        users.append(user_index)
        items.append(item_index)
        scores.append(score)
        if feedback == 'implicit':
            inter.setdefault(user_index, []).append(item_index)
        else:
            inter[item_index, user_index] = score
    return users, items, scores, inter
```

### Putting It All Together

The functions are combined to prepare the dataset for training and testing, utilizing `Dataset` and `DataLoader` to manage data handling efficiently.

```python
def split_and_load_ml100k(split_mode='seq-aware', feedback='explicit', test_ratio=0.1, batch_size=256):
    data, num_users, num_items = read_data_ml100k()
    train_data, test_data = split_data_ml100k(data, num_users, num_items, split_mode, test_ratio)
    train_u, train_i, train_r, _ = load_data_ml100k(train_data, num_users, num_items, feedback)
    test_u, test_i, test_r, _ = load_data_ml100k(test_data, num_users, num_items, feedback)
    train_set = gluon.data.ArrayDataset(np.array(train_u), np.array(train_i), np.array(train_r))
    test_set = gluon.data.ArrayDataset(np.array(test_u), np.array(test_i), np.array(test_r))
    train_iter = gluon.data.DataLoader(train_set, shuffle=True, last_batch='rollover', batch_size=batch_size)
    test_iter = gluon.data.DataLoader(test_set, batch_size=batch_size)
    return num_users, num_items, train_iter, test_iter
```
