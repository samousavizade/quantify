---
title: Linear Equations and Matrix Algebra
draft: false
summary: Backtest statistics are essential for evaluating the efficacy of investment strategies.
---

# Linear Equations and Matrix Algebra

## Overview

Many issues in economics and finance involve solving linear equations. In this lecture, we will explore linear equations and their applications.

To highlight the significance of linear equations, we start with a two-good model of supply and demand. This simple case allows for solutions to be computed by hand. However, in reality, we often need to analyze markets with many goods.

In such cases, we encounter large systems of linear equations with numerous equations and unknowns. To manage these systems, we need:

- Matrix algebra (and the ability to use it effectively)
- Computer code to apply matrix algebra to specific problems

This lecture will cover these aspects, and we will use the following packages:

```python
import numpy as np
import matplotlib.pyplot as plt
```

## A Two-Good Example

In this section, we will explore a simple two-good example and solve it using:

1. Pencil and paper
2. Matrix algebra

The second method is more general, as we will demonstrate.

### Pencil and Paper Methods

Consider two related goods, such as:

- Propane and ethanol, or
- Rice and wheat

For simplicity, we'll refer to them as Good 0 and Good 1.

The demand for each good depends on the prices of both goods:

$$
\begin{aligned}
q_0^d &= 100 - 10 p_0 - 5 p_1 \\
q_1^d &= 50 - p_0 - 10 p_1
\end{aligned} \tag{8.1}
$$

(We assume that demand decreases as the price of either good increases, though other scenarios are possible.)

Assume that the supply is given by:

$$
\begin{aligned}
q_0^s &= 10 p_0 + 5 p_1 \\
q_1^s &= 5 p_0 + 10 p_1
\end{aligned} \tag{8.2}
$$

Equilibrium occurs when supply equals demand ($q_0^s = q_0^d$ and $q_1^s = q_1^d$). This yields the following linear system:

$$
\begin{aligned}
100 - 10 p_0 - 5 p_1 &= 10 p_0 + 5 p_1 \\
50 - p_0 - 10 p_1 &= 5 p_0 + 10 p_1
\end{aligned} \tag{8.3}
$$

Solving this by hand, we get:

$$
p_0 = 4.41 \quad \text{and} \quad p_1 = 1.18.
$$

Substituting these values into either [(8.1)](#equation-two-eq-demand) or [(8.2)](#equation-two-eq-supply) gives the equilibrium quantities:

$$
q_0 = 50 \quad \text{and} \quad q_1 = 33.82.
$$

### Looking Forward

While pencil and paper methods are manageable for the two-good case, problems become more complex with multiple goods. For such scenarios, matrix algebra is necessary.

Before diving into matrix algebra, let’s review the basics of vectors and matrices, both theoretically and computationally.

## Vectors

A **vector** of length $n$ is a sequence (or array, or tuple) of $n$ numbers, written as $x = (x_1, \ldots, x_n)$ or $x = \begin{bmatrix}x_1 \\ \vdots \\ x_n\end{bmatrix}$.

Vectors can be written horizontally or vertically, but for matrix operations, we assume vectors are column vectors by default.

The set of all $n$-vectors is denoted by $\mathbb{R}^n$.

- $\mathbb{R}^2$ represents the plane — the set of pairs $(x_1, x_2)$.
- $\mathbb{R}^3$ represents three-dimensional space — the set of vectors $(x_1, x_2, x_3)$.

Vectors are often visualized as arrows from the origin to the point. Here is a visual representation.

```python
fig, ax = plt.subplots()
# Set the axes through the origin
for spine in ['left', 'bottom']:
    ax.spines[spine].set_position('zero')
for spine in ['right', 'top']:
    ax.spines[spine].set_color('none')

ax.set(xlim=(-5, 5), ylim=(-5, 5))

vecs = ((2, 4), (-3, 3), (-4, -3.5))
for v in vecs:
    ax.annotate('', xy=v, xytext=(0, 0),
                arrowprops=dict(facecolor='blue',
                shrink=0,
                alpha=0.7,
                width=0.5))
    ax.text(1.1 * v[0], 1.1 * v[1], str(v))
plt.show()
```

![png](/static/courses/Introduction-to-Quantitative-Economics/linear_equations_files/linear_equations_8_0.png)

### Vector Operations

Sometimes, we need to modify vectors. The two most common operations on vectors are addition and scalar multiplication, which we will discuss here.

When adding two vectors, we perform element-by-element addition:

$$
\begin{bmatrix}
4 \\
-2
\end{bmatrix}
+
\begin{bmatrix}
3 \\
3
\end{bmatrix}
=
\begin{bmatrix}
4 + 3 \\
-2 + 3
\end{bmatrix}
=
\begin{bmatrix}
7 \\
1
\end{bmatrix}.
$$

In general:

$$
x + y =
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}
+
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}
=
\begin{bmatrix}
x_1 + y_1 \\
x_2 + y_2 \\
\vdots \\
x_n + y_n
\end{bmatrix}.
$$

Vector addition in $ \mathbb{R}^2 $ can be visualized as follows.

```python
fig, ax = plt.subplots()
# Set the axes through the origin
for spine in ['left', 'bottom']:
    ax.spines[spine].set_position('zero')
for spine in ['right', 'top']:
    ax.spines[spine].set_color('none')

ax.set(xlim=(-2, 10), ylim=(-4, 4))
# ax.grid()
vecs = ((4, -2), (3, 3), (7, 1))
tags = ('(x1, x2)', '(y1, y2)', '(x1+x2, y1+y2)')
colors = ('blue', 'green', 'red')
for i, v in enumerate(vecs):
    ax.annotate('', xy=v, xytext=(0, 0),
                arrowprops=dict(color=colors[i],
                shrink=0,
                alpha=0.7,
                width=0.5,
                headwidth=8,
                headlength=15))
    ax.text(v[0] + 0.2, v[1] + 0.1, tags[i])

for i, v in enumerate(vecs):
    ax.annotate('', xy=(7, 1), xytext=v,
                arrowprops=dict(color='gray',
                shrink=0,
                alpha=0.3,
                width=0.5,
                headwidth=5,
                headlength=20))
plt.show()
```

![png](/static/courses/Introduction-to-Quantitative-Economics/linear_equations_files/linear_equations_11_0.png)

Scalar multiplication is an operation where a vector $x$ is multiplied by a scalar element-wise.

For example:

$$
-2
\begin{bmatrix}
3 \\
-7
\end{bmatrix}
=
\begin{bmatrix}
-2 \times 3 \\
-2 \times -7
\end{bmatrix}
=
\begin{bmatrix}
-6 \\
14
\end{bmatrix}.
$$

More generally, if $\gamma$ is a scalar and $x$ is a vector, scalar multiplication produces:

$$
\gamma x :=
\begin{bmatrix}
\gamma x_1 \\
\gamma x_2 \\
\vdots \\
\gamma x_n
\end{bmatrix}.
$$

Scalar multiplication is illustrated in the following figure.

```python
fig, ax = plt.subplots()
# Set the axes through the origin
for spine in ['left', 'bottom']:
    ax.spines[spine].set_position('zero')
for spine in ['right', 'top']:
    ax.spines[spine].set_color('none')

ax.set(xlim=(-5, 5), ylim=(-5, 5))
x = (2, 2)
ax.annotate('', xy=x, xytext=(0, 0),
            arrowprops=dict(facecolor='blue',
            shrink=0,
            alpha=1,
            width=0.5))
ax.text(x[0] + 0.4, x[1] - 0.2, '$x$', fontsize='16')

scalars = (-2, 2)
x = np.array(x)

for s in scalars:
    v = s * x
    ax.annotate('', xy=v, xytext=(0, 0),
                arrowprops=dict(facecolor='red',
                shrink=0,
                alpha=0.5,
                width=0.5))
    ax.text(v[0] + 0.4, v[1] - 0.2, f'${s} x$', fontsize='16')
plt.show()
```

![png](/static/courses/Introduction-to-Quantitative-Economics/linear_equations_files/linear_equations_14_0.png)

In Python, a vector can be represented using a list or tuple, such as `x = [2, 4, 6]` or `x = (2, 4, 6)`. One advantage of using NumPy arrays is that they provide a very natural syntax for operations like scalar multiplication and addition.

```python
x = np.ones(3)            # Vector of three ones
y = np.array((2, 4, 6))   # Converts tuple (2, 4, 6) into a NumPy array
x + y                     # Add (element-by-element)
```

    array([3., 5., 7.])

```python
4 * x                     # Scalar multiply
```

    array([4., 4., 4.])

### Inner Product and Norm

The **inner product** of vectors $ x, y \in \mathbb{R}^n $ is defined as

$$
x^\top y =
\begin{bmatrix}
x_1 & x_2 & \cdots & x_n
\end{bmatrix}
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}
= x_1 y_1 + x_2 y_2 + \cdots + x_n y_n
:= \sum_{i=1}^n x_i y_i.
$$

The **norm** of a vector $ x $ represents its “length” (i.e., its distance from
the zero vector) and is defined as

$$
\| x \| := \sqrt{x^\top x} := \left( \sum_{i=1}^n x_i^2 \right)^{1/2}.
$$

The expression $ \| x - y \| $ can be thought of as the “distance” between $ x $ and $ y $.

The inner product and norm can be computed as follows

```python
np.sum(x*y)      # Inner product of x and y
```

    12.0

```python
x @ y            # Another way to compute the inner product
```

    12.0

```python
np.sqrt(np.sum(x**2))  # Norm of x, method one
```

    1.7320508075688772

```python
np.linalg.norm(x)      # Norm of x, method two
```

    1.7320508075688772

## Matrix operations

When we discussed linear price systems, we mentioned using matrix algebra.

Matrix algebra is similar to algebra for numbers.

Let’s review some details.

### Addition and scalar multiplication

Just as was the case for vectors, we can add, subtract and scalar multiply
matrices.

Scalar multiplication and addition are generalizations of the vector case:

###

$$
3
\begin{bmatrix}
2 & -13 \\
0 & 5
\end{bmatrix}
=
\begin{bmatrix}
6 & -39 \\
0 & 15
\end{bmatrix}.
$$

In general for a number $ \gamma $ and any matrix $ A $,

$$
\gamma A =
\gamma
\begin{bmatrix}
a_{11} &  \cdots & a_{1k} \\
\vdots & \vdots  & \vdots \\
a_{n1} &  \cdots & a_{nk}
\end{bmatrix} :=
\begin{bmatrix}
\gamma a_{11} & \cdots & \gamma a_{1k} \\
\vdots & \vdots & \vdots \\
\gamma a_{n1} & \cdots & \gamma a_{nk}
\end{bmatrix}.
$$

###

Consider the following example of matrix addition:

$$
\begin{bmatrix}
1 & 5 \\
7 & 3 \\
\end{bmatrix}
+
\begin{bmatrix}
12 & -1 \\
0 & 9
\end{bmatrix}
=
\begin{bmatrix}
13 & 4 \\
7 & 12
\end{bmatrix}.
$$

In general,

$$
A + B =
\begin{bmatrix}
a_{11} & \cdots & a_{1k} \\
\vdots & \vdots & \vdots \\
a_{n1} & \cdots & a_{nk}
\end{bmatrix} +
\begin{bmatrix}
b_{11} & \cdots & b_{1k} \\
\vdots & \vdots & \vdots \\
b_{n1} & \cdots & b_{nk}
\end{bmatrix} =
\begin{bmatrix}
a_{11} + b_{11} & \cdots & a_{1k} + b_{1k} \\
\vdots & \vdots & \vdots \\
a_{n1} + b_{n1} & \cdots & a_{nk} + b_{nk}
\end{bmatrix}.
$$

In this case, the matrices must have the same dimensions for the addition to be valid.

### Matrix Multiplication

Matrix multiplication follows a specific rule that extends the concept of inner products discussed earlier.

If $ A $ and $ B $ are two matrices, then their product $ AB $ is determined by taking the inner product of the $ i $-th row of $ A $ and the $ j $-th column of $ B $ for the $ i,j $-th element of the product matrix.

If $ A $ is an $ n \times k $ matrix and $ B $ is a $ k \times m $ matrix, then $ A $ and $ B $ can be multiplied only if $ k = k $. The resulting matrix $ AB $ will be $ n \times m $.

Here’s an example of a $ 2 \times 2 $ matrix multiplied by a $ 2 \times 1 $ vector:

$$
Ax =
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
=
\begin{bmatrix}
a_{11}x_1 + a_{12}x_2 \\
a_{21}x_1 + a_{22}x_2
\end{bmatrix}
$$

As a special case, consider multiplying an $ n \times k $ matrix $ A $ by a $ k \times 1 $ column vector $ x $. According to the rule, this results in an $ n \times 1 $ column vector:

$$
Ax =
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1k} \\
\vdots & \vdots & & \vdots \\
a_{i1} & a_{i2} & \cdots & a_{ik} \\
\vdots & \vdots & & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nk}
\end{bmatrix}
\begin{bmatrix}
x_{1} \\
x_{2} \\
\vdots \\
x_{k}
\end{bmatrix}
=
\begin{bmatrix}
a_{11} x_1 + a_{12} x_2 + \cdots + a_{1k} x_k \\
\vdots \\
a_{i1} x_1 + a_{i2} x_2 + \cdots + a_{ik} x_k \\
\vdots \\
a_{n1} x_1 + a_{n2} x_2 + \cdots + a_{nk} x_k
\end{bmatrix}
$$

Here is an illustration of the multiplication of two matrices:

$$
AB =
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix}
\begin{bmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22}
\end{bmatrix}
=
\begin{bmatrix}
a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\
a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22}
\end{bmatrix}
$$

For further visualization, you can refer to:

- [this tutorial](http://www.mathsisfun.com/algebra/matrix-multiplying.html), or
- the [Wikipedia page](https://en.wikipedia.org/wiki/Matrix_multiplication).

> **Note:** Unlike numerical multiplication, $ AB $ and $ BA $ are generally not the same.

An important special case is the identity matrix, which has ones on the main diagonal and zeros elsewhere:

$$
I =
\begin{bmatrix}
1 & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & 1
\end{bmatrix}
$$

It is useful to check the following properties:

- If $ A $ is an $ n \times k $ matrix and $ I $ is the $ k \times k $ identity matrix, then $ AI = A $.
- If $ I $ is the $ n \times n $ identity matrix, then $ IA = A $.

### Matrices in NumPy

NumPy arrays can be used to represent matrices and come with efficient functions and methods for all standard matrix operations. They can be created manually from tuples of tuples (or lists of lists) as follows:

```python
A = ((1, 2),
     (3, 4))

type(A)
```

    tuple

```python
A = np.array(A)

type(A)
```

    numpy.ndarray

```python
A.shape
```

    (2, 2)

The `shape` attribute of a NumPy array is a tuple that indicates the number of rows and columns of the array.
To obtain the transpose of a matrix `A`, you can use `A.transpose()` or the shorter notation `A.T`.

NumPy provides various functions to create common matrices, such as matrices filled with zeros or ones.
By default, operations in NumPy are performed elementwise, which makes scalar multiplication and addition straightforward and intuitive.

```python
A = np.identity(3)    # 3 x 3 identity matrix
B = np.ones((3, 3))   # 3 x 3 matrix of ones
2 * A
```

    array([[2., 0., 0.],
           [0., 2., 0.],
           [0., 0., 2.]])

```python
A + B
```

    array([[2., 1., 1.],
           [1., 2., 1.],
           [1., 1., 2.]])

To perform matrix multiplication, we use the `@` symbol.

> **Note:** Specifically, `A @ B` represents matrix multiplication, while `A * B` denotes element-by-element multiplication.

### Two Good Model in Matrix Form

We can revisit the two good model and solve the system from equation (8.3) numerically using matrix algebra.

This method involves additional steps but is applicable to more complex cases with many goods.

First, rewrite equation (8.1) in matrix form as follows:

$$
q^d = Dp + h
\quad \text{where} \quad
q^d =
\begin{bmatrix}
q_0^d \\
q_1^d
\end{bmatrix}
\quad
D =
\begin{bmatrix}
-10 & -5 \\
-1 & -10
\end{bmatrix}
\quad \text{and} \quad
h =
\begin{bmatrix}
100 \\
50
\end{bmatrix}. \tag{8.5}
$$

Here, $ p \in \mathbb{R}^{2} $ represents the prices of the two goods.

(Verify that $ q^d = Dp + h $ corresponds to the equations given in (8.1).)

Next, rewrite equation (8.2) as:

$$
q^s = Cp
\quad \text{where} \quad
q^s =
\begin{bmatrix}
q_0^s \\
q_1^s
\end{bmatrix}
\quad \text{and} \quad
C =
\begin{bmatrix}
10 & 5 \\
5 & 10
\end{bmatrix}. \tag{8.6}
$$

The condition for equilibrium, where supply equals demand, can be expressed as $ q^s = q^d $, or:

$$
Cp = Dp + h.
$$

Rearranging the terms, we get:

$$
(C - D)p = h.
$$

While solving directly for prices would be straightforward if all terms were numbers (i.e., $ p = h / (C-D) $), matrix algebra allows us to solve for equilibrium prices using the inverse of $ C - D $:

$$
p = (C - D)^{-1} h. \tag{8.7}
$$

Before we implement this solution, let’s consider a more general scenario.

### More Goods

It is useful to extend the analysis to systems with more goods. For instance, within energy commodities, there are various goods like crude oil, gasoline, coal, natural gas, ethanol, and uranium, all of which are interconnected.

Handling large systems of goods manually can be cumbersome, but the matrix methods described remain applicable.

Generally, we can represent the demand equation as $ q^d = Dp + h $, where:

- $ q^d $ is an $ n \times 1 $ vector of demand quantities for $ n $ different goods.
- $ D $ is an $ n \times n $ coefficient matrix.
- $ h $ is an $ n \times 1 $ vector of constant values.

Similarly, the supply equation can be written as $ q^s = Cp + e $, where:

- $ q^s $ is an $ n \times 1 $ vector of supply quantities for the same goods.
- $ C $ is an $ n \times n $ coefficient matrix.
- $ e $ is an $ n \times 1 $ vector of constant values.

To find the equilibrium, solve $ Dp + h = Cp + e $, which simplifies to:

$$
(D - C)p = e - h. \tag{8.8}
$$

Thus, the price vector for the $ n $ goods is:

$$
p = (D - C)^{-1} (e - h).
$$

### General Linear Systems

A more general problem can be represented as follows:

$$
\begin{matrix}
a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n = b_1 \\
\vdots \\
a_{n1} x_1 + a_{n2} x_2 + \cdots + a_{nn} x_n = b_n
\end{matrix} \tag{8.9}
$$

The goal is to solve for the unknowns $ x*1, \ldots, x_n $, given the coefficients $ a*{11}, \ldots, a\_{nn} $ and constants $ b_1, \ldots, b_n $.

This scenario, where the number of unknowns equals the number of equations, is where a well-defined solution is most likely to exist.

(Other cases where the number of equations is not equal to the number of unknowns are known as [overdetermined](https://en.wikipedia.org/wiki/Overdetermined_system) and [underdetermined](https://en.wikipedia.org/wiki/Underdetermined_system) systems, which we will discuss in future lectures.)

In matrix form, the system (8.9) is:

$$
Ax = b
\quad \text{where} \quad
A =
\begin{bmatrix}
a_{11} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots \\
a_{n1} & \cdots & a_{nn}
\end{bmatrix}
\quad \text{and} \quad
b =
\begin{bmatrix}
b_1 \\
\vdots \\
b_n
\end{bmatrix}. \tag{8.10}
$$

For example, equation (8.8) takes the form:

$$
A = D - C,
\quad
b = e - h
\quad \text{and} \quad
x = p.
$$

When addressing problems like equation (8.10), we need to consider:

- Does a solution exist?
- If so, how should it be computed?

## Solving Systems of Equations

Recall the system of equations from equation (8.9), rewritten here as:

$$
Ax = b. \tag{8.11}
$$

The task is to find a vector $ x \in \mathbb{R}^n $ that solves equation (8.11), given $ A $ and $ b $.

A unique solution to equation (8.11) may not always exist.

### No Solution

Consider the following system of equations:

$$
\begin{aligned}
x + 3y &= 3 \\
2x + 6y &= -8.
\end{aligned}
$$

It can be verified that this system has no solution. To understand why, consider plotting the two lines.

```python
fig, ax = plt.subplots()
x = np.linspace(-10, 10)
plt.plot(x, (3-x)/3, label=f'$x + 3y = 3$')
plt.plot(x, (-8-2*x)/6, label=f'$2x + 6y = -8$')
plt.legend()
plt.show()
```

![png](/static/courses/Introduction-to-Quantitative-Economics/linear_equations_files/linear_equations_43_0.png)

Clearly, the lines are parallel, so there is no point $ x \in \mathbb{R}^2 $ where they intersect.

Thus, this system has no solution.

We can express this system in matrix form as:

$$
A x = b
\quad \text{where} \quad
A =
\begin{bmatrix}
1 & 3 \\
2 & 6
\end{bmatrix}
\quad \text{and} \quad
b =
\begin{bmatrix}
3 \\
-8
\end{bmatrix}. \tag{8.12}
$$

Note that the second row of matrix $ A = (2, 6) $ is a scalar multiple of the first row of matrix $ A = (1, 3) $.

The rows of matrix $ A $ in this case are called **linearly dependent**.

### Many Solutions

Consider the system:

$$
\begin{aligned}
x - 2y &= -4 \\
-2x + 4y &= 8.
\end{aligned}
$$

Any vector $ v = (x, y) $ where $ x = 2y - 4 $ satisfies this system.

Since there are infinitely many such vectors, this system has infinitely many solutions.

This is because the rows of the corresponding matrix:

$$
A =
\begin{bmatrix}
1 & -2 \\
-2 & 4
\end{bmatrix}. \tag{8.13}
$$

are linearly dependent — can you determine why?

We will now impose conditions on $ A $ in equation (8.11) to avoid these issues.

### Nonsingular Matrices

Each square matrix has a unique number called the [determinant](https://en.wikipedia.org/wiki/Determinant).

For $ 2 \times 2 $ matrices, the determinant is given by:

$$
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
=
ad - bc.
$$

A matrix $ A $ is _nonsingular_ if its determinant is not zero.

A square matrix $ A $ is nonsingular if and only if its rows and columns are linearly independent.

A more detailed explanation of matrix inverses can be found [here](https://www.mathsisfun.com/algebra/matrix-inverse.html).

You can verify that the matrices in equations (8.12) and (8.13) with linearly dependent rows are singular.

The determinant provides a useful summary to determine if a square matrix can be inverted.

Specifically, a square matrix $ A $ has a nonzero determinant if and only if it has an _inverse matrix_ $ A^{-1} $, satisfying $ A A^{-1} = A^{-1} A = I $.

Thus, if we pre-multiply both sides of $ Ax = b $ by $ A^{-1} $, we get:

$$
x = A^{-1} b. \tag{8.14}
$$

This is the solution to $ Ax = b $ — the solution we seek.

### Linear Equations with NumPy

In the two-good example, we derived the matrix equation:

$$
p = (C - D)^{-1} h.
$$

where $ C $, $ D $, and $ h $ are given by equations (8.5) and (8.6).

This equation is analogous to equation (8.14) with $ A = (C - D) $, $ b = h $, and $ x = p $.

We can solve for equilibrium prices using NumPy’s `linalg` submodule.

All of these routines are Python interfaces to well-tested and highly optimized FORTRAN code.

```python
C = ((10, 5),      # Matrix C
     (5, 10))
```

```python
C = np.array(C)
```

```python
D = ((-10, -5),     # Matrix D
     (-1, -10))
D = np.array(D)
```

```python
h = np.array((100, 50))   # Vector h
h.shape = 2,1             # Transforming h to a column vector
```

```python
from numpy.linalg import det, inv
A = C - D
# Check that A is nonsingular (non-zero determinant), and hence invertible
det(A)
```

    340.0000000000001

```python
A_inv = inv(A)  # compute the inverse
A_inv
```

    array([[ 0.05882353, -0.02941176],
           [-0.01764706,  0.05882353]])

```python
p = A_inv @ h  # equilibrium prices
p
```

    array([[4.41176471],
           [1.17647059]])

```python
q = C @ p  # equilibrium quantities
q
```

    array([[50.        ],
           [33.82352941]])

Notice that we obtain the same solutions as from the manual calculations.

We can also solve for $ p $ using the `solve` function in NumPy. Here’s how you can do it:

```python
import numpy as np

# Define matrices and vector
A = np.array([[10, 5], [5, 10]])
h = np.array([100, 50])

# Solve for p
p = np.linalg.solve(A, h)
p

# Or

from numpy.linalg import solve
p = solve(A, h)  # equilibrium prices
p
```

    array([[4.41176471],
           [1.17647059]])

```python
q = C @ p  # equilibrium quantities
q
```

    array([[50.        ],
           [33.82352941]])

Observe how we can solve for $ x = A^{-1} y $ either via `inv(A) @ y`, or by using `solve(A, y)`.

The latter method uses a different algorithm that is numerically more stable and should be the default option.

## Exercises

### Exercise 8.1

Consider a market with 3 commodities: good 0, good 1, and good 2.

The demand for each good depends on the price of the other two goods and is given by:

$$
\begin{aligned}
q_0^d & = 90 - 15p_0 + 5p_1 + 5p_2 \\
q_1^d & = 60 + 5p_0 - 10p_1 + 10p_2 \\
q_2^d & = 50 + 5p_0 + 5p_1 - 5p_2
\end{aligned}
$$

(The demand decreases when its own price increases but increases when the prices of other goods increase.)

The supply of each good is given by:

$$
\begin{aligned}
q_0^s & = -10 + 20p_0 \\
q_1^s & = -15 + 15p_1 \\
q_2^s & = -5 + 10p_2
\end{aligned}
$$

Equilibrium holds when supply equals demand, i.e., $ q_0^d = q_0^s $, $ q_1^d = q_1^s $, and $ q_2^d = q_2^s $.

1. Set up the market as a system of linear equations.
2. Use matrix algebra to solve for equilibrium prices. Do this using both the `numpy.linalg.solve` and `inv(A)` methods. Compare the solutions.

### Solution to Exercise 8.1

The generated system is:

$$
\begin{aligned}
35p_0 - 5p_1 - 5p_2 &= 100 \\
-5p_0 + 25p_1 - 10p_2 &= 75 \\
-5p_0 - 5p_1 + 15p_2 &= 55
\end{aligned}
$$

In matrix form, this is written as:

$$
Ap = b
\quad \text{where} \quad
A =
\begin{bmatrix}
35 & -5 & -5 \\
-5 & 25 & -10 \\
-5 & -5 & 15
\end{bmatrix}
, \quad p =
\begin{bmatrix}
p_0 \\
p_1 \\
p_2
\end{bmatrix}
\quad \text{and} \quad
b =
\begin{bmatrix}
100 \\
75 \\
55
\end{bmatrix}
$$

```python
import numpy as np
from numpy.linalg import det

A = np.array([[35, -5, -5],  # matrix A
              [-5, 25, -10],
              [-5, -5, 15]])

b = np.array((100, 75, 55))  # column vector b
b.shape = (3, 1)

det(A)  # check if A is nonsingular
```

    9999.99999999999

```python
# Using inverse
from numpy.linalg import det

A_inv = inv(A)

p = A_inv @ b
p
```

    array([[4.9625],
           [7.0625],
           [7.675 ]])

```python
# Using numpy.linalg.solve
from numpy.linalg import solve
p = solve(A, b)
p
```

    array([[4.9625],
           [7.0625],
           [7.675 ]])

The solution is given by:

$$
p_0 = 4.6925, \; p_1 = 7.0625 \;\; \text{and} \;\; p_2 = 7.675
$$

## Exercise 8.2

Earlier in the lecture, we discussed cases where the system of equations given by $ Ax = b $ has no solution.

In such cases, $ Ax = b $ is called an _inconsistent_ system of equations.

When faced with an inconsistent system, we try to find the best “approximate” solution. One such method is the **method of least squares**.

Suppose we have an inconsistent system

$$
Ax = b \tag{8.15}
$$

where $ A $ is an $ m \times n $ matrix and $ b $ is an $ m \times 1 $ column vector.

A **least squares solution** to [(8.15)](#equation-inconsistent) is an $ n \times 1 $ column vector $ \hat{x} $ such that, for all other vectors $ x \in \mathbb{R}^n $, the distance from $ A\hat{x} $ to $ b $ is less than or equal to the distance from $ Ax $ to $ b $. That is,

$$
\|A\hat{x} - b\| \leq \|Ax - b\|
$$

It can be shown that, for the system of equations $ Ax = b $, the least squares solution $ \hat{x} $ is

$$
\hat{x} = (A^T A)^{-1} A^T b \tag{8.16}
$$

Now consider the general equation of a linear demand curve of a good given by:

$$
p = m - nq
$$

where $ p $ is the price of the good and $ q $ is the quantity demanded.

Suppose we are trying to _estimate_ the values of $ m $ and $ n $. We do this by repeatedly observing the price and quantity (for example, each month) and then choosing $ m $ and $ n $ to fit the relationship between $ p $ and $ q $.

We have the following observations:

| Price | Quantity Demanded |
| :---: | :---------------: |
|   1   |         9         |
|   3   |         7         |
|   8   |         3         |

Requiring the demand curve $ p = m - nq $ to pass through all these points leads to the following three equations:

$$
\begin{aligned}
1 &= m - 9n \\
3 &= m - 7n \\
8 &= m - 3n
\end{aligned}
$$

Thus, we obtain a system of equations $ Ax = b $ where

$$
A = \begin{bmatrix}
1 & -9 \\
1 & -7 \\
1 & -3
\end{bmatrix}, \quad
x = \begin{bmatrix}
m \\
n
\end{bmatrix}, \quad
b = \begin{bmatrix}
1 \\
3 \\
8
\end{bmatrix}
$$

It can be verified that this system has no solutions. (The problem is that we have three equations and only two unknowns.)

We will thus try to find the best approximate solution for $ x $.

1. Use [(8.16)](#equation-least-squares) and matrix algebra to find the least squares solution $ \hat{x} $.
2. Find the least squares solution using `numpy.linalg.lstsq` and compare the results.

### Solution to Exercise 8.2

```python
import numpy as np
from numpy.linalg import inv
```

```python
# Using matrix algebra
A = np.array([[1, -9],  # matrix A
              [1, -7],
              [1, -3]])

A_T = np.transpose(A)  # transpose of matrix A

b = np.array((1, 3, 8))  # column vector b
b.shape = (3, 1)

x = inv(A_T @ A) @ A_T @ b
x
```

    array([[11.46428571],
           [ 1.17857143]])

```python
# Using numpy.linalg.lstsq
x, res, _, _ = np.linalg.lstsq(A, b, rcond=None)
```

```python
print(f"x\u0302 = {x}")
print(f"\u2016Ax\u0302 - b\u2016\u00B2 = {res[0]}")
```

    x̂ = [[11.46428571]
     [ 1.17857143]]
    ‖Ax̂ - b‖² = 0.07142857142857129

Here is a visualization of how the least squares method approximates the equation of a line connecting a set of points.

We can also describe this as “fitting” a line between a set of points.

The least squares method finds the line that minimizes the sum of the squared differences between the observed values and the values predicted by the line. This “best fit” line is the one that best represents the trend of the data points.

```python
fig, ax = plt.subplots()
p = np.array((1, 3, 8))
q = np.array((9, 7, 3))

a, b = x

ax.plot(q, p, 'o', label='observations', markersize=5)
ax.plot(q, a - b*q, 'r', label='Fitted line')
plt.xlabel('quantity demanded')
plt.ylabel('price')
plt.legend()
plt.show()
```

![png](/static/courses/Introduction-to-Quantitative-Economics/linear_equations_files/linear_equations_75_0.png)

### Additional Resources

You can explore the documentation for the `numpy.linalg` submodule [here](https://numpy.org/devdocs/reference/routines.linalg.html).
