---
title: Factor Investing and Causality
draft: false
summary: Backtest statistics are essential for evaluating the efficacy of investment strategies.
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

# 6 Causality in Factor Investing

The previous section outlined the prevailing state of confusion between associ- ation and causation in the field of econometrics. This section focuses on how financial economists have often (mis)applied econometrics to factor investing, leading to a discipline based on shaky foundations and plagued with false discoveries ([Harvey 2017](https://doi.org/10.1093/rfs/hhx044)).

Factor investing can be defined as the investment approach that targets the exposure to measurable characteristics (called "factors") that presumably explain differences in the performance of a set of securities.[^26] This is an evolution of the Asset Pricing Theory literature,[^27] inspired by the seminal work of [Schipper and Thompson (1981)](<https://doi.org/10.1016/0165-4101(81)90017-6>), that uses factor analysis and principal component analysis to validate those characteristics ([Ferson 2019, p. 130](https://doi.org/10.1093/rfs/hhz089)).

For example, proponents of the value factor believe that a portfolio composed of stocks with a high book-to-market equity (called "value stocks") will outper- form a portfolio composed of stocks with a low book-to-market equity (called "growth stocks"). In search of supportive empirical evidence, factor researchers generally follow one of two procedures. In the first procedure, inspired by [Fama and MacBeth (1973)](https://doi.org/10.1086/260061), a researcher gathers returns of securities ($Y$), explanatory factors ($X$), and control variables ($Z$). The researcher then estimates through least-squares the parameters (also called factor exposures or factor loadings) of a cross-sectional regression model with general form $Y = X\beta + Z\gamma + \epsilon$ for each time period, and computes the mean and standard deviation of those parameter estimates across all periods ([Cochrane 2005, pp. 245–251](https://press.princeton.edu/books/hardcover/9780691121376/asset-pricing)). In the second procedure, inspired by [Fama and French (1993)](https://doi.org/10.1086/261881), a researcher ranks securities in an investment universe according to a characteristic, and carries out two parallel operations on that ranking: (a) partition the investment universe into subsets delimited by quantiles, and compute the time series of average returns for each subset; and (b) compute the returns time series of a long-short portfolio, where top-ranked securities receive a positive weight and bottom-ranked securities receive a negative weight. A researcher interested in a multifactor analysis will apply operations (a) and (b) once for each factor (for operation (a), this means further partitioning each subset). For each subset, the researcher then estimates through least-squares the parameters of a time- series regression model with general form $Y = X\beta + Z\gamma + \epsilon$, where $Y$ repre- sents one time series computed in (a), $X$ represents the (possibly several) time series computed in (b), and $Z$ represents the times series of control variables chosen by the researcher.

The goal of both procedures is not to explain changes in average returns over time (a time-series analysis), but rather to explain differences in average returns across securities. The first procedure accomplishes this goal through averaging cross-sectional regressions coefficients computed on explanatory factors. The second procedure accomplishes this goal through a regression of quantile-averaged stock returns against the returns attributed to neutralized factors. Following the econometric canon, researchers state their case by showing that the estimated value of $\beta$ is statistically significant, with the interpretation that investors holding securities with exposure to factor $X$ are rewarded beyond the reward received from exposure to factors in $Z$.

## 6.1 Causal Content

Factor researchers almost never state explicitly the causal assumptions that they had in mind when they made various modeling decisions, and yet those assump- tions shape their analysis. A different set of causal assumptions would have led to different data pre-processing, choice of variables, model specification, choice of estimator, choice of tested hypotheses, interpretation of results, portfolio design, etc. Some of these causal assumptions are suggested by the data, and some are entirely extra-statistical. I denote causal content the set of causal assumptions, whether declared or undeclared, that are embedded in a factor model's specification, estimation, interpretation, and use. Factor investing strat- egies reveal part of their causal content in at least four ways.

First, the causal structure assumed by the researcher determines the model specification. A factor investing strategy is built on the claim that exposure to a particular factor ($X$) causes positive average returns above the market's ($Y$), and that this causal effect ($X \rightarrow Y$, a single link in the causal graph) is strong enough to be independently monetizable through a portfolio exposed to $X$. A researcher only interested in modelling the joint distribution $P(X,Y)$ would surely use more powerful techniques from the machine learning toolbox than a least-squares estimator, such as nonparametric regression methods (e.g., random forest regression, support-vector regression, kernel regression, or regression splines). Factor researchers' choice of least-squares, explanatory variables, and conditioning variables, is consistent with the causal structure that they wish to impose (Section 5.1).[^28]

Second, the estimation of $\beta$ prioritizes causal interpretation over predictive power. If factor researchers prioritized predictive power, they would: (a) use estimators with lower mean-square error than least-squares, by accepting some bias in exchange for lower variance ([Mullainathan and Spiess 2017](https://doi.org/10.1146/annurev-economics-080217-053407); [Athey and Imbens 2019](https://doi.org/10.1146/annurev-economics-080218-030343)). Examples of such estimators include ridge regression, LASSO, and elastic nets; or (b) use as loss function a measure of performance, such as the Sharpe ratio (for a recent example, see [Cong et al. 2021](https://doi.org/10.1016/j.jfineco.2021.01.003)). So not only researchers believe that $Y$ is a function of $X$ (a causal concept), but they are also willing to sacrifice as much predictive power (an associational concept) as necessary to remove all bias from $\hat{\beta}$. The implication is that factor researchers assume that the errors are exogenous causes of $Y$, uncorrelated to $X$ (the explicit exogeneity assumption). Factor researchers' choice of least-squares is consistent with their interpretation of the estimated $\beta$ as a causal effect (Section 5.2).

Third, factor researchers place strong emphasis on testing the null hypothesis of $H_0: \beta = 0$ (no causal effect) against the alternative $H_1: \beta \neq 0$ (causal effect), and expressing their findings through p-values. In contrast, machine-learners are rarely interested in estimating individual p-values, because they assess the importance of a variable in predictive (associational) terms, with the help of associational concepts such as mean-decrease accuracy (MDA), mean-decrease impurity (MDI), and Shapley values ([López de Prado 2018](https://doi.org/10.2139/ssrn.3166627)). Factor researchers' use of p-values is consistent with the claim of a significant causal effect.[^29]

Fourth, factor investors build portfolios that overweight stocks with a high exposure to $X$ and underweight stocks with a low exposure to $X$, at the tune of one separate portfolio for each factor. A factor investor may combine those separate factor portfolios into an aggregate multifactor portfolio, however the reason behind that action is diversification, not monetizing a multifactor prediction. This approach to building portfolios stands in contrast with how other investors use predictions to form portfolios. Investors who rely on predictive models build portfolios exposed to the residual ($\epsilon$) rather than portfolios exposed to a particular factor ($X$), hence for them biased estimates of $\beta$ are not a concern. Factor researchers' approach to portfolio design is consistent with the monetization of a causal claim rather than a predictive (associational) claim.

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![Figure 7](/static/courses/Causal-Factor-Investing/Figure-7.png)
</div>
> Figure 7 Causal graph for which the specification $Y=Xβ+Zγ+ε$ estimates the causal effect of $X$ on
$Y$ , while adjusting for the confounding effect of $Z$.

In conclusion, the objective of a factor model such as $Y = X\beta + Z\gamma + \epsilon$ is not to predict $Y$ conditioned on $X$ and $Z$ ($E[Y|X, Z]$), but to estimate the causal effect of $X$ on $Y$ ($E[Y|do(X)]$), which can be simulated on the observed sample by controlling for confounder $Z$. The implication is that researchers use factors as if they had assumed explicit exogeneity, and their chosen model specification $Y = X\beta + Z\gamma + \epsilon$ is consistent with a particular causal graph (see Section 5.2), of which Figure 7 is just one possibility among several. It is the responsibility of the researcher to declare and justify what particular causal graph informed the chosen specification, such that the exogeneity assumption holds true.

## 6.2 Omitted Mediation Analysis

Several papers have proposed alternative explanations for various factors, which can be grouped into two broad themes: (a) investment-based explan- ations; and (b) production-based explanations. For example, [Fama and French (1996)](https://doi.org/10.1111/j.1540-6261.1996.tb05202.x) argue that stocks approaching bankruptcy experience a price correction, which in turn is reflected as high value (a high book-to-market ratio). According to this explanation, investors holding portfolios of high-value stocks demand a premium for accepting a non-diversifiable risk of bankruptcy. [Berk et al. (1999)](https://doi.org/10.1111/0022-1082.00148) argue that, should firms' assets and growth options change in predictable ways, then firm characteristics will be associated with differences in average returns. These economic rationales are plausible, however they do not rise to the status of scientific theories, for three reasons: First, the authors do not declare the causal graph that informed their choice of model specification. Second, the authors do not propose an ideal interventional study (a Gedankenexperiment) that would establish the causal effect of interest. A Gedankenexperiment, even if unfeasible, has the benefit of communicating clearly the essence of the causal relationship, and the counterfactual implications under various scenarios. Third, when the ideal interventional study is unfeasible, the authors have not proposed a method to estimate the causal effect through observational data (a natural experiment, or a simulated intervention). Consequently, while these economic rationales are plausible, they are also experimentally unfalsifiable. Following [Pauli's criterion](https://en.wikipedia.org/wiki/Not_even_wrong), the explanations proposed by factor investing researchers are "not even wrong" ([Lipton 2016](https://doi.org/10.1073/pnas.1517559112)).

As discussed in Section 3, scientific knowledge is built on falsifiable theories that describe the precise causal mechanism by which $X$ causes $Y$. Value investors may truly receive a reward ($Y$) for accepting an undiversifiable risk of bankruptcy ($X$), but how precisely does this happen, and why is book-to-market the best proxy for bankruptcy risk? Despite of factor models' causal content, factor researchers rarely declare the causal mechanism by which $X$ causes $Y$. Factor papers do not explain precisely how a firm's (or collection of firms') exposure to a factor triggers a sequence of events that ends up impacting stock average returns; nor do those papers derive a causal structure from the observed data; nor do those papers analyze the causal structure (forks, chains, immoralities); nor do those papers make an effort to explain the role played by the declared variables (treatment, confounder, mediator, collider, etc.); nor do those papers justify their chosen model specification in terms of the identified causal structure (an instance of concealed assumptions).

### 6.2.1 Example of Factor Causal Mechanism

For illustrative purposes only, and without a claim of accuracy, consider the following hypothetical situation. A researcher observes the tendency of prices ($p_t$) to converge toward the value implied by fundamentals ($v_t$). The researcher theorizes that this convergence is driven by informed traders, who place orders that generate orderflow imbalance (OI), which in turn causes price changes (PC). The researcher further hypothesizes that the value factor (HML) is a proxy for the difference $v_t - p_t$, and that the momentum factor (MOM) interferes with the convergence process.(I use the acronyms HML and MOM, common in the literature, without loss of generality. [Fama and French (1993)](https://doi.org/10.1111/j.1540-6261.1993.tb04022.x) and [Carhart (1997)](https://doi.org/10.1111/j.1540-6261.1997.tb03808.x) proposed some of the best-known definitions of value and momentum, however, this causal theory is congruent with alternative definitions.) The researcher proposes the following causal mechanism: (1) HML causes OI; (2) OI causes PC; (3) PC is the first difference of prices; and (4) MOM dampens the effect of HML on OI. The researcher also notes that the causal effects are thus asymmetric (e.g., the right-hand side influences the left-hand side, and not the other way around). The researcher applies causal discovery tools on a representative dataset, and finds that the derived causal structure is compatible with his theorized data-generating process. Using the discovered causal graph, he estimates the effect of HML on OI, and the effect of OI on PC, with a backdoor adjustment for MOM. The empirical analysis suggests that HML causes PC, and that the effect is mediated by OI.

Encouraged by these results, the researcher submits an article to a prestigious academic journal. Upon review of the researcher's journal submission, a referee asks why the model does not control for bid-ask spread (BAS) and market liquidity factors (LIQ). The referee argues that OI is not directly observable, and its estimation may be biased by passive traders. For instance, a large fund may decide to place passive orders at the bid for weeks, rather than lift the offers, in order to conceal their buying intentions. Those trades will be labeled as sale-initiated by the exchange, even though the persistent OI comes from the passive buyer (a problem discussed in [Easley et al. 2016](https://doi.org/10.1017/S0022109016000727)). The referee argues that BAS is more directly observable, and perhaps a better proxy for the presence of informed traders.

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![Figure 8](/static/courses/Causal-Factor-Investing/Figure-8.png)
</div>
> Figure 8 Example of a hypothesized causal mechanism of HML (in the box) within a hypothesized causal
graph.

The researcher counter-argues that he agrees that (5) OI causes market makers to widen BAS, however (6) PC also forces market makers to realize losses, as prices trend, and market makers' reaction to those losses is also the widening of BAS. Two consequences of BAS widening are (7) lower liquidity provision and (8) greater volatility. Accordingly, BAS is a collider, and controlling for it would open the noncausal path of association $HML \leftarrow OI \rightarrow BAS \leftarrow PC$ (see Section 6.4.2.2). While the referee is not convinced with the relevance of (6), he is satisfied that the researcher has clearly stated his assumptions through a causal graph. Readers may disagree with the stated assumptions, which the causal graph makes explicit, however, under the proposed causal graph everyone agrees that controlling for either BAS or LIQ or VOL would be a mistake. The final causal path and causal graph are reflected in Figure 8.

By providing this causal graph and mechanism, the researcher has opened himself to falsification. Referees and readers may propose experiments designed to challenge every link in the causal graph. For example, researchers can test link (1) through a natural experiment, by taking advantage that fundamental data is updated at random time differences between stocks. The treatment effect for link (1) may be estimated as the difference in OI over a given period between stocks where HML has been updated versus stocks where HML has not been updated yet. Links (2), (5), (6), (7), and (8) may be tested through controlled and natural experiments similar to those mentioned in Section 3.3. Link (3) is a mathematical statement that requires no empirical testing. To test link (4), a researcher may split stocks with similar HML into two groups (a cohort study, see Section 4.2): the first group is composed of stocks where MOM is increasing HML, and the second group is composed of stocks where MOM is reducing HML. Since the split is not random, the researcher must verify that the two groups are comparable in all respects other than MOM's direction. The treatment effect may be measured as the two groups' difference in: (a) sentiment extracted from text, such as analyst reports, financial news, social media (see [Das and Chen 2007](https://doi.org/10.1016/j.jbankfin.2007.02.010); [Baker and Wurgler 2007](https://doi.org/10.1257/jep.21.2.129)); (b) sentiment from surveys; or (c) exposures reported by institutional investors in their quarterly reports in SEC 13F forms. If link (4) is true, MOM dampens investors' appetite for HML's contrarian bets, which is reflected in the groups' difference.

These experiments are by no means unique, and many alternatives exist. The opportunities for debunking this theory will only grow as more alternative datasets become available. Contrast this openness with the narrow opportunities offered by factor investing articles currently published in journals, which are essentially limited to: (a) in-sample replication of a backtest, and (b) structural break analyses for in-sample versus out-of-sample performance.[^30]

## 6.3 Causal Denial

Associational investment strategies do not have causal content. Examples include statistical arbitrage ([Rad et al. 2016](https://doi.org/10.1016/j.jbankfin.2016.04.003)), sentiment analysis ([Katayama and Tsuda 2020](https://doi.org/10.1007/978-981-15-3412-6_10)), or alpha capture ([Isichenko 2021, pp. 129–154](https://doi.org/10.1002/9781119821892.ch5)). Authors of associational investment strategies state their claims in terms of distributional properties, for example, stationarity, ergodicity, normality, homoscedasticity, serial independence, and linearity. The presence of causal content sets factor investing strategies apart, because these investment strategies make causal claims.

A causal claim implies knowledge of the data-generating process responsible, among other attributes, for all distributional properties claimed by associational studies. Causal claims therefore require stronger empirical evidence and level of disclosure than mere associational claims. In the context of investment strategies, this translates among other disclosures into: (i) making all causal assumptions explicit through a causal graph; (ii) stating the falsifiable causal mechanism responsible for a claimed causal effect; and (iii) providing empirical evidence in support of (i) and (ii).

Should factor researchers declare causal graphs and causal mechanisms, they would enjoy two benefits essential to scientific discovery. First, causal graphs like the one displayed in Figure 8 would allow researchers to make their causal assumptions explicit, communicate clearly the role played by each variable in the hypothesized phenomenon, and apply do-calculus rules for debiasing estimates. This information is indispensable for justifying the proposed model specification. Second, stating the causal mechanism would provide an opportunity for falsifying a factor theory without resorting to backtests. Even if a researcher p-hacked the factor model, the research community would still be able to design creative experiments aimed at testing independently the implications of every link in the theorized causal path, employing alternative datasets. Peer-reviewers' work would not be reduced to mechanical attempts at reproducing the author's calculations.

The omission of causal graphs and causal mechanisms highlights the logical inconsistency at the heart of the factor investing literature: on one hand, researchers inject causal content into their models, and use those models in a way consistent with a causal interpretation (Section 6.1). On the other hand, researchers almost never state a causal graph or falsifiable causal mechanism, in denial or ignorance of the causal content of factor models, hence depriving the scientific community of the opportunity to design experiments that challenge the underlying theory and assumptions (Section 6.2). Under the current state of causal confusion, researchers report the estimated $\beta$ devoid of its causal meaning (the effect on $Y$ of an intervention on $X$), and present p-values as if they merely conveyed the strength of associations of unknown origin (causal and noncausal combined).

The practical implication of this logical inconsistency is that the factor investing literature remains at a phenomenological stage, where spurious claims of investment factors are accepted without challenge. Put simply: without a causal mechanism, there is no investment theory; without an investment theory, there is no scientific discovery.

This does not mean that investment factors do not exist; however, it means that the empirical evidence presented by factor researchers is insufficient and flawed by scientific standards. Causal denial (or ignorance) is a likely reason for the prolifer- ation of spurious claims in the factor investing studies, and the poor performance delivered by the factor-based investment funds, for the reasons explained next.

### 6.4 Spurious Investment Factors

The out-of-sample performance of factor investing has been disappointing. One of the broadest factor investing indices is the Bloomberg–Goldman Sachs Asset Management US Equity Multi-Factor Index (BBG code: BGSUSEMF Index). It tracks the long/short performance of the momentum, value, quality, and low-risk factors in US stocks ([Bloomberg 2021](https://assets.bbhub.io/professional/sites/10/Bloomberg-GSAM-US-Equity-Multi-Factor-Index-Fact-Sheet.pdf)). Its annualized Sharpe ratio from May 2, 2007 (the inception date) to December 2, 2022 (this Element's submission date) has been 0.29 (t-stat = 1.16, p-value = 0.12), and the average annualized return has been 1.13 percent. This performance does not include: (a) transaction costs; (b) market impact of order execution; (c) cost of borrowing stocks for shorting positions; (d) management and incentive fees. Also, this performance assigns a favorable 0 percent risk-free rate when computing the excess returns. Using the 6-month US Government bond rates (BBG code: USGG6M Index) as the risk-free rates, the Sharpe ratio turns negative. Figure 9 plots the performance of this broad factor index from inception, without charging for the above costs (a)–(d).

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![Figure 9](/static/courses/Causal-Factor-Investing/Figure-9.png)
</div>
> Figure 9 Performance of the Bloomberg – Goldman Sachs Asset Management US Equity Multi-Factor Index,
since index inception (base 100 on May 2, 2007)

It takes over 31 years of daily observations for an investment strategy with an annualized Sharpe ratio of 0.29 to become statistically significant at a 95 percent confidence level (see [Bailey and López de Prado (2012)](https://doi.org/10.21314/JOR.2012.226) for details of this calculation). If the present Sharpe ratio does not decay (e.g., due to overcrowding, or hedge funds preempting factor portfolio rebalances), researchers will have to wait until the year 2039 to reject the null hypothesis that factor investing is unprofitable, and even then, they will be earning a gross annual return of 1.13 percent before paying for costs (a)–(d).

There is a profound disconnect between the unwavering conviction expressed by academic authors and the underwhelming performance experienced by factor investors. A root cause of this disconnect is that factor investing studies usually make spurious claims, of two distinct types.

#### 6.4.1 Type-A Spuriosity

I define an empirical claim to be of type-A spurious when a researcher mistakes random variability (noise) for signal, resulting in a false association. Selection bias under multiple testing is a leading cause for type-A spuriosity. Type-A spuriosity has several distinct attributes: (a) it results in type-1 errors (false positives); (b) for the same number of trials, it has a lower probability to take place as the sample size grows ([López de Prado 2022b](https://doi.org/10.2139/ssrn.4025181)); and (c) it can be corrected through multiple-testing adjustments, such as [Hochberg (1988)](https://doi.org/10.1093/biomet/75.4.800) or [Bailey and López de Prado (2014)](https://doi.org/10.3905/jpm.2014.40.5.094). In the absence of serial correlation, the expected return of a type-A spurious investment factor is zero, before transaction costs and fees ([Bailey et al. 2014](https://doi.org/10.1090/noti1105)). Next, I discuss the two main reasons for type-A spuriosity in the factor investing literature.

##### 6.4.1.1 P-Hacking

The procedures inspired by [Fama and MacBeth (1973)](https://doi.org/10.1086/260061) and [Fama and French (1993)](https://doi.org/10.1111/j.1540-6261.1993.tb04022.x) involve a large number of subjective decisions, such as fit window length, fit frequency, number of quantiles, definition of long-short portfolios, choice of controls, choice of factors, choice of investment universe, data cleaning and outlier removal decisions, start and end dates, etc. There are millions of potential combinations to pick from, many of which could be defended on logical grounds. Factor researchers routinely run multiple regressions before selecting a model with p-values below their null-rejection threshold. Authors report those minimal p-values without adjusting for selection bias under multiple testing, a malpractice known as p-hacking. The problem is compounded by publication bias, whereby journals accept papers without accounting for: (a) the number of previously rejected papers; and (b) the number of previously accepted papers. [Harvey et al. (2016)](https://doi.org/10.1093/rfs/hhw044) conclude that "most claimed research findings in financial economics are likely false." The consequence is, factor investments do not perform as expected, and results are not replicated out-of-sample.

Other fields of research have addressed p-hacking decades ago. Statisticians have developed methods to determine the familywise error rate ([Hochberg 1988](https://doi.org/10.1093/biomet/75.4.800); [White 2000](https://doi.org/10.1111/1468-0262.00153); [Romano and Wolf 2005](https://doi.org/10.1198/016214504000000539)) and false discovery rate ([Benjamini and Hochberg 1995](https://doi.org/10.1111/j.2517-6161.1995.tb02031.x)).[^31] Medical journals routinely demand the logging, reporting, and adjustment of results from all trials. Since 2008, laboratories are required by U.S. law to publish the results from all trials within a year of completion of a clinical study (Section 801 of the Food and Drug Administration Amendments Act of 2007). While most disciplines are taking action to tackle the replication crisis, the majority of members of the factor investing research community remain unwaveringly committed to p-hacking. There are two possible explanations for their choice: ignorance and malpractice. Factor researchers have not been trained to control for multiple testing. To this day, all major econometrics textbooks fail to discuss solutions to the problem of conducting inference when more than one trial has taken place. As [Harvey (2017, p. 1402)](https://doi.org/10.1093/rfs/hhx044) lamented, "our standard testing methods are often ill equipped to answer the questions that we pose. Other fields have thought deeply about testing" (emphasis added). However, ignorance alone does not explain why some factor investing authors argue that multiple testing is not a problem, against the advice of mathematical societies ([Wasserstein and Lazar 2016](https://doi.org/10.1080/00031305.2016.1154108)). [Harvey (2022)](https://doi.org/10.1093/rfs/hhac072) explains the stance of p-hacking deniers by pointing at the commercial interests that control financial academia.

##### 6.4.1.2 Backtest Overfitting

A backtest is commonly defined as a historical simulation of how a systematic strategy would have performed in the past ([López de Prado 2018, chapter 11](https://www.wiley.com/en-us/Advances+in+Financial+Machine+Learning-p-9781119482086)). Factor researchers often present backtests as evidence that a claimed causal effect is real. However, a backtest is neither a controlled experiment, nor an RCT, nor a natural experiment, because it does not allow the researcher to intervene on the data-generating process (a do-operation), and a simulation does not involve the researcher's or Nature's random assignment of units to groups. Accordingly, a backtest has no power to prove or disprove a causal mechanism. At best, a backtest informs investors of the economic potential of an investment strategy, under the assumption that history repeats itself (a distributional inductive belief, hence associational and noncausal).

Factor researchers rarely report or adjust for the number of trials involved in a backtest ([Fabozzi and López de Prado 2018](https://doi.org/10.3905/joi.2018.27.3.013); [López de Prado and Lewis 2019](https://doi.org/10.3905/joi.2019.1.082); [Bailey and López de Prado 2021](https://doi.org/10.1111/1740-9713.01614)). This is a problem, because backtest overfitting is a leading cause of type-A spuriosity ([Bailey et al. 2014](https://doi.org/10.1090/noti1105)). The more trials involved in a backtest, the greater the chances that the researcher will find a false positive. The number of trials involved in a backtest can be astronomically large, due to the many combinations of parameters that can be used to specify a strategy. Backtest overfitting is a problem even when the researcher has not engaged in p-hacking. The reason is, the number of trials involved in a backtest is determined by the number of combinations of parameters, not by the number of regressions run by the researcher.

The problem of backtest overfitting is exacerbated by the fact that factor researchers often use the same dataset to discover a factor and backtest the strategy that monetizes that factor. This practice is known as in-sample backtesting, and it is a leading cause of false discoveries ([Bailey and López de Prado 2014](https://doi.org/10.3905/jpm.2014.40.5.094)). The correct procedure is to use one dataset to discover a factor, and a separate dataset to backtest the strategy that monetizes that factor (out-of-sample backtesting). The discovery dataset and the backtesting dataset must be separated by a firewall, to prevent the researcher from using information from the backtesting dataset to improve the strategy's performance on the discovery dataset.

Factor researchers often claim that their findings are robust because they have been replicated in multiple asset classes and geographies. However, this claim is misleading, because the discovery of a factor is often the result of p-hacking and backtest overfitting on a global multi-asset dataset. The subsequent backtests on subsets of that global multi-asset dataset are not out-of-sample, because the researcher has already used information from those subsets to discover the factor. A true out-of-sample backtest would require the researcher to discover a factor on one dataset (e.g., US stocks), and then backtest the strategy that monetizes that factor on a completely separate dataset (e.g., European stocks), without ever using information from the second dataset to improve the strategy's performance on the first dataset.

#### 6.4.2 Type-B Spuriosity

I define an empirical claim to be of type-B spurious when a researcher mistakes association for causation, resulting in a false causal claim. The association is true, however the causal claim is false. Type-B spuriosity has several distinct attributes: (a) it results in false positives and false negatives; (b) it is not corrected by multiple testing adjustments; (c) it is not corrected by out-of-sample testing; and (d) the expected return of a type-B spurious investment factor is nonzero before transaction costs and fees, and it may exhibit time-varying risk premia (more on this in Section 6.4.2.1).

Type-A and type-B spuriosity are mutually exclusive. For type-B spuriosity to take place, the association must be noncausal but true, which precludes that association from being type-A spurious. While type-A spuriosity has been studied with some depth in the factor investing literature, relatively little has been written about type-B spuriosity. Next, I discuss the main reasons for type-B spuriosity in factor investing.

##### 6.4.2.1 Under-Controlling

Consider a data-generating process where one of its equations is $Y :=X\beta + Z\gamma + u$, such that $\gamma \neq 0$ and $u$ is white noise. The process is unknown to a researcher, who attempts to estimate the causal effect of $X$ on $Y$ by fitting the equation $Y = X\beta + \epsilon$ on a sample $\{X_t, Y_t\}_{t=1,...,T}$ produced by the process. This incorrect specification choice makes $\epsilon = Z\gamma + u$, and $E[\epsilon|X]=E[Z\gamma + u|X]=\gamma E[Z|X]$. However, if $Z$ is correlated with $X$, $E[Z|X]\neq 0$, hence $E[\epsilon|X]\neq 0$. This is a problem, because the least-squares method assumes $E[\epsilon|X]=0$ (the exogeneity assumption, see Section 5.2). Missing one or several relevant variables biases the estimate of $\beta$, potentially leading to spurious claims of causality. A false positive occurs when $|\hat{\beta}| \gg0$ for $\beta \approx 0$, and a false negative occurs when $\hat{\beta} \approx 0$ for $|\beta| \gg 0$.

Econometrics textbooks do not distinguish between types of missing variables (see, for example, [Greene 2012, section 4.3.2](https://www.pearson.com/en-us/subject-catalog/p/econometric-analysis/P200000006421/9780134461366)), yet not all missing variables are created equal. There are two distinct cases that researchers must consider. In the first case, the second equation of the data-generating process is $Z := X\delta + v$, where $\delta \neq 0$ and $v$ is white noise. In this case, $Z$ is a mediator ($X$ causes $Z$, and $Z$ causes $Y$), and the chosen specification biases the estimation of the direct effect $\hat{\beta}$; however, $\hat{\beta}$ can still be interpreted as a total causal effect (through two causal paths with the same origin and end). The causal graph for this first case is displayed at the top of Figure 10. In the second case, the second equation of the data-generating process is $X := Z\delta + v$, where $\delta \neq 0$ and $v$ is white noise. In this case, $Z$ is a confounder ($Z$ causes $X$ and $Y$), the chosen specification also biases $\hat{\beta}$, and $\hat{\beta}$ does not measure a causal effect (whether total or direct).[^32] The causal graph for this second case is displayed at the bottom of Figure 10.

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![Figure 10](/static/courses/Causal-Factor-Investing/Figure-10.png)
</div>
> Figure 10 Variable $Z$ as mediator (top) and confounder (bottom)

Assuming that the white noise is Gaussian, the expression $E[\hat{\beta}|X]$ reduces to

$$E[\hat{\beta}|X] = (X'X)^{-1}X'E[X\beta + Z\gamma + u|X] = \beta + \gamma\delta(1+\delta^2)^{-1} = \beta + \theta$$

where $\theta = \gamma\delta(1+\delta^2)^{-1}$ is the bias due to the missing confounder. The Appendix contains a proof of the above proposition. The intuition behind $\theta$ is that a necessary and sufficient condition for a biased estimate of $\beta$ is that $\gamma \neq 0$ and $\delta \neq 0$, because when both parameters are nonzero, variable $Z$ is a confounder.

A first consequence of missing a confounder is incorrect performance attribution and risk management. Part of the performance experienced by the investor comes from a misattributed risk characteristic $Z$, which should have been hedged by a correctly specified model. The investor is exposed to both, causal association (from $\beta$), as intended by the model's specification, and noncausal association (from $\theta$), which is not intended by the model's specification.

A second consequence of missing a confounder is time-varying risk premia. Consider the case where the market rewards exposure to $X$ and $Z$ ($\beta > 0$, $\gamma > 0$). Even if the two risk premia remain constant, changes over time in $\delta$ will change $\hat{\beta}$. In particular, for a sufficiently negative value of $\delta$, then $\hat{\beta} < 0$. Performance misattribution will mislead investors into believing that the market penalizes exposure to $X$, when in fact the market rewards exposure to $X$. This phenomenon explains the time-varying nature of risk premia reported in canonical factor investing papers, such as [Fama and French (1993)](https://doi.org/10.1111/j.1540-6261.1993.tb04022.x), [Carhart (1997)](https://doi.org/10.1111/j.1540-6261.1997.tb03808.x), and [Fama and French (2015)](https://doi.org/10.1016/j.jfineco.2014.10.010). The time-varying nature of risk premia is a likely consequence of under-controlling, not a true characteristic of the market. For further reading on time-varying risk premia, see [Anderson (2011)](https://doi.org/10.1016/j.jmateco.2011.03.001).

The partial correlations method is the most common approach for de-confounding the effect of $X$ on $Y$ from the bias introduced by $Z$. The partial correlations method involves adding $Z$ as an explanatory variable in an equation set to model the effect of $X$ on $Y$. Accordingly, the new model specification for estimating the effect of $X$ on $Y$ is $Y = X\beta + Z\gamma + \epsilon$. This is a particular application of the more general backdoor adjustment (Section 4.3.2.2), and by far the most common confounder bias correction method used in regression analysis. This adjustment method relies on a linear regression, thus inheriting its assumptions and limitations. In particular, the partial correlations method is not robust when the explanatory variables exhibit high correlation (positive or negative) with each other (multicollinearity).

### 6.4.2.2 Over-Controlling

The previous section explained the negative consequences of under-controlling (e.g., missing a confounder). However, over-controlling is not less pernicious. Statisticians have been trained for decades to control for any variable $Z$ associ- ated with $Y$ that is not $X$ ([Pearl and MacKenzie 2018, pp. 139, 152, 154, 163](https://www.basicbooks.com/titles/judea-pearl/the-book-of-why/9780465097616/)), regardless of the role of $Z$ in the causal graph (the so-called omitted variable problem). Econometrics textbooks dismiss as a harmless error the inclusion of an irrelevant variable, regardless of the variable's role in the causal graph. For example, [Greene (2012, section 4.3.3)](https://www.pearson.com/en-us/subject-catalog/p/econometric-analysis/P200000006421/9780134461366) states that the only downside to adding superfluous variables is a reduction in the precision of the estimates. This grave misunderstanding has certainly led to countless type-B spurious claims in economics.

In recent years, do-calculus has revealed that some variables should not be controlled for, even if they are associated with $Y$. Figure 11 shows two examples of causal graphs where controlling for $Z$ will lead to biased estimates of the effect of $X$ on $Y$. Common examples of over-controlling include controlling for variables that are mediators or colliders relative to the causal path from $X$ to $Y$.[^33] Controlling for a collider is a mistake, as it opens a backdoor path that biases the effect's estimation (Berkson's fallacy, see [Berkson 1946](https://doi.org/10.2307/3001899)). Controlling for a mediator inter- feres with the mediated effect ($X \rightarrow Z \rightarrow Y$) and the total causal effect ($X \rightarrow Z \rightarrow Y$ plus $X \rightarrow Y$) that the researcher may wish to assess, leaving only the direct effect $X \rightarrow Y$. In the case of the top causal graph in Figure 11, a researcher could estimate the mediated effect $X \rightarrow Z \rightarrow Y$ as the difference between the total effect ($X \rightarrow Z \rightarrow Y$ plus $X \rightarrow Y$) and the direct effect ($X \rightarrow Y$).

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![Figure 11](/static/courses/Causal-Factor-Investing/Figure-11.png)
</div>
> Figure 11 Variable $Z$ as controlled collider (bottom) and controlled mediator (top)

Over-controlling a collider and under-controlling a confounder have the same impact on the causal graph: allowing the flow of association through a backdoor path (Section 4.3.2.2). Consequently, over-controlled models can suffer from the same conditions as under-controlled models, namely (i) biased estimates, as a result of noncompliance with the exogeneity assumption; and (ii) time- varying risk premia. Black-box investment strategies take over-controlling to the extreme. Over-controlling explains why quantitative funds that deploy black-box investment strategies routinely transition from delivering systematic profits to delivering systematic losses, and there is not much fund managers or investors can do to detect that transition until it is too late.

The only way to determine precisely which variables a researcher must control for, in order to block (or keep blocked) noncausal paths of association, is through the careful analysis of a causal graph (e.g., front-door criterion and backdoor criterion). The problem is, factor researchers almost never estimate or declare the causal graphs associated with the phenomenon under study (Section 6.2). Odds are, factor researchers have severely biased their estimates of $\beta$ by controlling for the wrong variables, which in turn has led to false positives and false negatives.

#### 6.4.2.3 Specification-Searching

Specification-searching is the popular practice among factor researchers of choos- ing a model's specification (including the selection of variables and functional forms) based on the resulting model's explanatory power. To cite one example, consider the three-factor model introduced by [Fama and French (1993)](https://doi.org/10.1111/j.1540-6261.1993.tb04022.x), and the five-factor model introduced by [Fama and French (2015)](https://doi.org/10.1016/j.jfineco.2014.10.010). Fama and French (2015)'s argument for adding two factors to their initial model specification was that "the five-factor model performs better than the three-factor model when used to explain average returns." These authors' line of argumentation is self-contradictory. The use of explanatory power (an associational, noncausal concept) for selecting the specification of a predictive model is consistent with the associational goal of that analysis; however, it is at odds with the causal content of a factor model.

In the context of factor models, specification-searching commingles two separate and sequential stages of the causal analysis: (1) causal discovery (Section 4.3.1); and (2) control (Section 4.3.2). Stage (2) should be informed by stage (1), not the other way around. Unlike a causal graph, a coefficient of determination cannot convey the extra-statistical information needed to de- confound the estimate of a causal effect, hence the importance of keeping stages (1) and (2) separate. Stage (1) discovers the causal graph that best explains the phenomenon as a whole, including observational evidence and extra-statistical information. In stage (2), given the discovered causal graph, the specification of a factor model should be informed exclusively by the aim to estimate one of the causal effects (one of the arrows or causal paths) declared in the causal graph, applying the tools of do-calculus. In a causal model, the correct specification is not the one that predicts $Y$ best, but the one that debiases $\hat{\beta}$ best, for a single treatment variable, in agreement with the causal graph.

Choosing a factor model's specification based on its explanatory power incurs the risk of biasing the estimated causal effects. For example, a researcher may achieve higher explanatory power by combining multiple causes of $Y$, at the expense of biasing the multiple parameters' estimates due to multicollinearity or over-controlling for a collider.[^34] It is easy to find realistic causal structures where specification-searching leads to false positives, and misspecified factor models that misattribute risk and performance (see Section 7.3).

There are two possible counter-arguments to the above reasoning: (a) A researcher may want to combine multiple causes of $Y$ in an attempt to model an interaction effect. However, such attempt is a stage (2) analysis that should be justified with the causal graph derived from stage (1), showing that the total effect involves several variables that are observed separately, but that need to be modeled jointly; and (b) a researcher may want to show that the two causes are not mutually redundant (a multifactor explanation, see [Fama and French 1996](https://doi.org/10.1111/j.1540-6261.1996.tb05202.x)). However, there exist far more sophisticated tools for making that case, such as mutual information or variation of information analyses ([López de Prado 2020, chapter 3](https://www.cambridge.org/core/books/machine-learning-for-asset-managers/6D9211305EA2E425D33A9F38D0AE3545)).

While specification-searching may involve multiple testing, specification- searching is not addressed by multiple testing corrections, as it has to do with the proper modeling of causal relationships, regardless of the number of trials involved in improving the model's explanatory power. Accordingly, specification-searching is a source of spuriosity that is distinct from p-hacking, and whose consequence is specification bias rather than selection bias. As argued in an earlier section, investors interested in predictive power should apply machine learning algorithms, which model association, not causation.

#### 6.4.2.4 Failure to Account for Temporal Properties

In the context of time-series analysis, two independent variables may appear to be associated when: (a) their time series are nonstationary ([Granger and Newbold 1974](https://doi.org/10.2307/2344517)); and (b) their time series are stationary, however they exhibit strong temporal properties, such as positively autocorrelated autoregressive series or long moving averages ([Granger et al. 2001](<https://doi.org/10.1016/S0304-4076(00)00071-7>)). This occurs regardless of the sample size and for various distributions of the error terms. Unit root and cointegration analyses help address concerns regarding the distribution of residuals, however they cannot help mitigate the risk of making type-B spurious claims. Like their cross-sectional counterparts, time-series models also require proper model specification through causal analysis, as discussed in the earlier sections. Section 5.3 exemplified one way in which econometricians mistake association for causation in time-series models.

### 6.5 Hierarchy of Evidence

Not all types of empirical evidence presented in support of a scientific claim are equally strong. The reason is, some types of evidence are more susceptible to being spurious than other types. Table 12 ranks the types of empirical evidence often used in financial research, in accordance with their scientific rigor. Categories colored in red support associational claims, and hence are phenomenological. Categories colored in green make use of the formal language of causal inference, hence enabling the statistical falsification of a causal claim (see Section 3.4).

At the bottom of the hierarchy is the expert opinion, such as the discretionary view of an investment guru, which relies on rules of thumb and educated guesses (heuristics) to reach a conclusion. A case study proposes a rationale to explain multiple aspects of a phenomenon (variative induction), however it typically lacks academic rigor and suffers from confirmation or selection biases. An econometric (observational) study, such as an investment factor model or backtest, relies primarily on statistical patterns observed on numerous instances (enumerative induction). Econometric studies can be academically rigorous, however they are afflicted by the pitfalls explained in Section 6.4. These three associational types of evidence are highly susceptible to type-A and type-B spuriosity.

A simulated intervention is qualitatively different from the bottom three categories because it uses the formal language of causal inference to communicate a falsifiable theory. The deduced causal effects rely on the strong assumption that the causal graph is correct.[^35] Natural experiments are yet superior to simulated experiments because the former involve an actual do-operation. The deduced causal effects rely on the weaker assumption that Nature's assignment of units to the treatment and control groups has been random. Finally, the top spot belongs to RCTs, because they offer the greatest level of transparency and reproducibility. The deduced causal effects rely on the assumption that the underlying causal mechanism will continue to operate (a form of induction). At the present, controlled experiments on financial systems are not possible, due to the complexity of these systems, but also due to ethical and regulatory considerations.

| Rank | Type                                | Inference                           | Rigor     | Example                                                                             |
| ---- | ----------------------------------- | ----------------------------------- | --------- | ----------------------------------------------------------------------------------- |
| 1    | Randomized controlled trials        | Deduction (with partial induction)  | Very high | Algo-wheel experiments (e.g., Section 3.3)                                          |
| 2    | Natural experiments                 | Deduction (with weak assumptions)   | High      | Market-maker reaction to random spikes in order imbalance (e.g., Section 3.3)       |
| 3    | Simulated interventions             | Deduction (with strong assumptions) | Medium    | Estimate effect of HML using a causal graph (e.g., Section 6.2.1)                   |
| 4    | Econometric (observational) studies | Enumerative induction               | Low       | Factor investing literature; backtested investment strategies (e.g., Section 6.4.1) |
| 5    | Case studies                        | Variative induction                 | Very low  | Broker report / analysis                                                            |
| 6    | Expert opinion                      | Heuristic                           | Anecdotal | Investment guru's prediction                                                        |

> Table 12 Hierarchy of evidence in financial research, ranked by scientific rigor

The reader should not conclude from Table 12 that associational evidence is useless. As explained in Section 3.1, associations play a critical role in the phenomenological step of the scientific method. Furthermore, the causal mechanism embedded in a theory implies the existence of key associations which, if not found, falsify the theory (see Section 3.3). In standard sequent notation, the claim that $C \Rightarrow A$ is not enough to assert $A \Rightarrow C$, however it is enough to assert that $\neg A \Rightarrow \neg C$, where $C$ stands for causation and $A$ stands for association. The reason is, causation is a special kind of association (i.e., the kind that flows through a causal path), hence the absence of association is enough to debunk the claim of causation by modus tollens.

Table 12 does not include out-of-sample evidence as a category, because "out-of-sample" is not a type of causal evidence but rather a description of when the data was collected or used. Evidence collected out-of-sample is of course preferable to evidence collected in-sample, as the former is more resilient to type-A spuriosity, however evidence collected out-of-sample is not necessarily more resilient to type-B spuriosity. For example, a researcher may collect out-of-sample evidence of the correlation between stocks and bonds, and from that measurement be tempted to deduce that changes in one's price cause changes in the other's price. While a causal link between stocks and bonds would be a possible explanation for the observed association, the existence of correlation does not suffice to claim a direct causal relationship, regardless of whether the measurement was taken in-sample or out-of-sample.

[^26]: The term “factor investing” is another misnomer. The word “factor” has its origin in the Latin language, with the literal meaning of “doer” or “maker.” Semantically, a factor is a cause responsible, in total or in part, for an effect. Ironically, the factor investing literature has not attempted to explain what does or makes the observed cross-section of expected returns.
[^27]: The field of Asset Pricing Theory uses the term “theory” in the mathematical sense, not in the scientific sense (see Section 3.2). For example, modern portfolio theory (MPT) derives results in risk diversification from the set of axioms proposed by Harry Markowitz’s landmark 1952 paper. Modern portfolio theory results are true in a mathematical sense, by virtue of proven theorems, however they are not necessarily true in a physical sense. Modern portfolio theory was not derived through the process described in Section 3. Assessing the scientific validity of MPT’s claims would require falsification of hypothesized causal mechanisms through testable implications (Section 3.3).
[^28]: This is not to say that least-squares is the only approach to model causality. The point is that least-squares in particular implies that $Y$ is a function of $X$ (a particular direction of causation), unlike other types of regression methods, such as Deming regression.
[^29]: Such a causal claim is conditional on satisfying several assumptions, including that the model is correctly specified, and that p-values are adjusted for multiple testing. Section 6.4 explains why factor investing models typically do not satisfy these assumptions.
[^30]: I use the acronyms HML and MOM, common in the literature, without loss of generality. Fama and French (1993) and Carhart (1997) proposed some of the best-known definitions of value and momentum, however, this causal theory is congruent with alternative definitions.
[^31]: For an introduction to the concepts of familywise error rate and false discovery rate, see Efron and Hastie (2021, chapter 15).
[^32]: This example illustrates once again that a causal graph conveys extra-statistical information, in this case through the direction of the causal link between $X$ and $Z$. The correct model specification depends on the direction implied by the proposed theory.
[^33]: In the words of Pearl and MacKenzie (2018, p. 276): “Mistaking a mediator for a confounder is one of the deadliest sins in causal inference and may lead to the most outrageous errors. The latter invited adjustment; the former forbids it.”
[^34]: When the Gauss-Markov assumptions hold, multicollinearity does not introduce bias, and it only inflates standard errors. However, when those assumptions do not hold, multicollinearity can amplify the bias introduced by a misspecified model (Kalnins, 2022).
[^35]: I use here the term “strong assumption” to denote assumptions whose validity implies the validity of other (weaker) assumptions. However, the validity of weak assumptions does not imply the validity of strong assumptions. For example, the validity of a causal graph is a strong assumption that implies weaker assumptions, such as invariance, stationarity, and ergodicity.
