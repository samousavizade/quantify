---
title: Ensemble Techniques in Finance
draft: false
summary: When dealing with a set of training observations and outcomes, you would want to estimate a function that closely matches the true function.
---

<TOCInline toc={props.toc} exclude="Ensemble Techniques in Finance" asDisclosure />
<div style={{ marginBottom: '2rem' }} />

# Ensemble Techniques in Finance

When dealing with a set of training observations and outcomes $\{(x_i, y_i)\}$, you would want to estimate a function $\hat{f}[x]$ that closely matches the true function $f(x)$.

The function is generally modeled as:

$$
y = f(x) + \varepsilon
$$

where $\varepsilon$ is white noise.

The aim is to minimize the mean squared error given by:

$$
\mathbb{E}[(y_i - \hat{f}[x_i])^2]
$$

This can be further decomposed into three terms: Bias squared, Variance, and the noise term $\sigma_{\varepsilon}^2$.

## The Power of Ensemble Methods

Ensemble techniques, like bagging (Bootstrap Aggregation), aim to build a robust model by averaging the predictions of multiple base models. Bagging is particularly effective in reducing the variance of the prediction, hence addressing overfitting.

The variance of a bagged prediction depends on the number of base models $(N)$, their average variance $(\bar{\sigma})$, and the average correlation between their predictions $(\bar{\rho})$.

The benefit of bagging is quantifiable. It's effective as long as:

$$
\bar{\rho} < 1
$$

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![Standard Deviation of the bagged
  prediction](/static/Figs/financial-modeling/StandardDeviationofthebaggedprediction.png)
</div>

## Accuracy Considerations in Bagging Classifiers

For a bagging classifier predicting $k$ classes, the accuracy depends on the number of base classifiers $N$ and their individual accuracy $p$. Under certain conditions, bagging classifiers can outperform individual classifiers.

The mathematical equation that demonstrates this is:

$$
p > \frac{1}{k} \Rightarrow \mathbb{P}\left[X > \frac{N}{k}\right] > p
$$

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![](/static/Figs/financial-modeling/BaggingClassifierAccuracy(functionofpandN).png)
</div>

Both Python and Julia functionalities for bagging classifier accuracy are available in the RiskLabAI library.

<div style={{display: 'flex', justifyContent: 'center'}}>
    <table style={{padding: 0, margin: 0, tableLayout: 'fixed', width: '100%'}}>
        <thead>
        <tr>
            <th style={{width: '50%', textAlign: 'center'}}>Python</th>
            <th style={{width: '50%', textAlign: 'center'}}>Julia</th>
        </tr>
        </thead>
        <tbody>
        <tr>
            <td style={{border: '1px solid transparent'}}>

                ```python
                def bagging_classifier_accuracy(
                N: int,
                p: float,
                k: int = 2
                ) -> float:
                ```

            </td>
            <td style={{border: '1px solid transparent'}}>

                ```julia
                function baggingClassifierAccuracy(
                N::Int,
                p::Float64,
                k::Int
                )::Float64
                ```

            </td>
        </tr>
        </tbody>
    </table>

</div>

View More: [Python](https://www.github.com/risklabai/RiskLabAI.py) | [Julia](https://www.github.com/risklabai/RiskLabAI.jl)

## Handling Dependency in Observations: A Challenge to Bagging

Financial observations often exhibit dependency, challenging the assumption that data points are independent and identically distributed (IIDs). This dependency affects bagging in two ways:

1. The samples in replacement sets are similar, reducing the efficiency of bagging in lowering prediction variance.
2. The 'out-of-bag' accuracy becomes inflated, meaning the model may seem more accurate than it actually is.

<div style={{display: 'flex', justifyContent: 'center'}}>
    <table style={{tableLayout: 'fixed', width: '90%'}}>
        <thead>
        <tr>
            <th style={{width: '50%', textAlign: 'center'}}>Python</th>
            <th style={{width: '50%', textAlign: 'center'}}>Julia</th>
        </tr>
        </thead>
        <tbody>
        <tr>
            <td style={{border: '1px solid transparent'}}>

                ```python
                def calculate_bagging_variance(
                observations: List[float],
                num_samples: int
                ) -> float:
                ```

            </td>
            <td style={{border: '1px solid transparent'}}>

                ```julia
                function calculate_bagging_variance(
                observations::Vector{Float64},
                num_samples::Int
                )::Float64
                ```

            </td>
        </tr>
        </tbody>
    </table>

</div>

View More: [Python](https://www.github.com/risklabai/RiskLabAI.py) | [Julia](https://www.github.com/risklabai/RiskLabAI.jl)

## Advantages of Using Random Forests

Random Forests (RF) are an extension of bagging that introduce another layer of randomness to combat overfitting and dependency in observations. RF offers the following advantages:

1. Reduced prediction variance
2. Feature significance analysis
3. Reliable out-of-the-bag accuracy estimates

<div style={{display: 'flex', justifyContent: 'center'}}>
    <table style={{tableLayout: 'fixed', width: '90%'}}>
        <thead>
        <tr>
            <th style={{width: '50%', textAlign: 'center'}}>Python</th>
            <th style={{width: '50%', textAlign: 'center'}}>Julia</th>
        </tr>
        </thead>
        <tbody>
        <tr>
            <td style={{border: '1px solid transparent'}}>

                ```python
                def train_random_forest(
                dataset: DataFrame,
                num_trees: int
                ) -> RandomForestClassifier:
                ```

            </td>
            <td style={{border: '1px solid transparent'}}>

                ```julia
                function train_random_forest(
                dataset::DataFrame,
                num_trees::Int
                )::RandomForestClassifier
                ```

            </td>
        </tr>
        </tbody>
    </table>

</div>

View More: [Python](https://www.github.com/risklabai/RiskLabAI.py) | [Julia](https://www.github.com/risklabai/RiskLabAI.jl)

## Boosting Poor Estimators for Accuracy

Boosting is an iterative method of improving model accuracy by combining poor estimators. It involves adjusting sample weights based on their classification results and produces a weighted average of individual forecasts.

<div style={{display: 'flex', justifyContent: 'center'}}>
    <table style={{tableLayout: 'fixed', width: '90%'}}>
        <thead>
        <tr>
            <th style={{width: '50%', textAlign: 'center'}}>Python</th>
            <th style={{width: '50%', textAlign: 'center'}}>Julia</th>
        </tr>
        </thead>
        <tbody>
        <tr>
            <td style={{border: '1px solid transparent'}}>

                ```python
                def train_boosting(
                dataset: DataFrame,
                num_estimators: int,
                threshold: float
                ) -> BoostingClassifier:
                ```

            </td>
            <td style={{border: '1px solid transparent'}}>

                ```julia
                function train_boosting(
                dataset::DataFrame,
                num_estimators::Int,
                threshold::Float64
                )::BoostingClassifier
                ```

            </td>
        </tr>
        </tbody>
    </table>

</div>

View More: [Python](https://www.github.com/risklabai/RiskLabAI.py) | [Julia](https://www.github.com/risklabai/RiskLabAI.jl)

## Bagging vs. Boosting in Finance

While boosting minimizes both prediction variance and bias, it is more prone to overfittingâ€”especially in financial applications where data is noisy. In contrast, bagging is less prone to overfitting and can be parallelized, making it generally more effective for financial data.

## Leveraging Parallelism for Scalability

For algorithms that don't scale well, like Support Vector Machines (SVMs), bagging can be parallelized to enhance efficiency. This allows you to run multiple weaker models concurrently, making bagging a scalable option for large datasets.

<div style={{display: 'flex', justifyContent: 'center'}}>
    <table style={{tableLayout: 'fixed', width: '90%'}}>
        <thead>
        <tr>
            <th style={{width: '50%', textAlign: 'center'}}>Python</th>
            <th style={{width: '50%', textAlign: 'center'}}>Julia</th>
        </tr>
        </thead>
        <tbody>
        <tr>
            <td style={{border: '1px solid transparent'}}>

                ```python
                def train_bagging_svm(
                dataset: DataFrame,
                num_estimators: int,
                max_iter: int,
                tol: float
                ) -> BaggingClassifier:
                ```

            </td>
            <td style={{border: '1px solid transparent'}}>

                ```julia
                function train_bagging_svm(
                dataset::DataFrame,
                num_estimators::Int,
                max_iter::Int,
                tol::Float64
                )::BaggingClassifier
                ```

            </td>
        </tr>
        </tbody>
    </table>

</div>

View More: [Python](https://www.github.com/risklabai/RiskLabAI.py) | [Julia](https://www.github.com/risklabai/RiskLabAI.jl)

### References

1. De Prado, M. L. (2018). Advances in financial machine learning. John Wiley & Sons.
2. De Prado, M. M. L. (2020). Machine learning for asset managers. Cambridge University Press.
