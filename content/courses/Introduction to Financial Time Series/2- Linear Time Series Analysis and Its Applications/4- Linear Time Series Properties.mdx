---
title: Linear Time Series Analysis and Its Applications
draft: false
summary: test
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

## Linear Time Series Analysis and Its Applications

Linear time series analysis provides a natural framework to study the dynamic structure of asset returns. The theories discussed include stationarity, dynamic dependence, autocorrelation function, modeling, and forecasting. This chapter introduces some basic concepts and simple econometric models useful for analyzing financial data.

### Stationarity

The foundation of time series analysis is stationarity. A time series $\{r_t\}$ is said to be strictly stationary if the joint distribution of $(r_{t_1},...,r_{t_k})$ is identical to that of $(r_{t_1+t},...,r_{t_k+t})$ for all $t$, where $k$ is an arbitrary positive integer and $(t_1,...,t_k)$ is a collection of $k$ positive integers. In other words, strict stationarity requires that the joint distribution of $(r_{t_1},...,r_{t_k})$ is invariant under time shift.

A weaker version of stationarity is often assumed. A time series $\{r_t\}$ is weakly stationary if both the mean of $r_t$ and the covariance between $r_t$ and $r_{t-l}$ are time invariant, where $l$ is an arbitrary integer. More specifically, $\{r_t\}$ is weakly stationary if:

(a) $E(r_t) = \mu$, which is a constant, and
(b) $Cov(r_t, r_{t-l}) = \gamma_l$, which only depends on $l$.

In practice, suppose that we have observed $T$ data points $\{r_t|t = 1,...,T\}$. Weak stationarity implies that the time plot of the data would show that the $T$ values fluctuate with constant variation around a fixed level.

Implicitly, in the condition of weak stationarity, we assume that the first two moments of $r_t$ are finite. From the definitions, if $r_t$ is strictly stationary and its first two moments are finite, then $r_t$ is also weakly stationary. The converse is not true in general. However, if the time series $r_t$ is normally distributed, then weak stationarity is equivalent to strict stationarity.

The covariance $\gamma_l = Cov(r_t, r_{t-l})$ is called the lag-$l$ autocovariance of $r_t$. It has two important properties:

(a) $\gamma_0 = Var(r_t)$ and
(b) $\gamma_{-l} = \gamma_l$.

The second property holds because:

$Cov(r_t, r_{t-(-l)}) = Cov(r_{t-(-l)}, r_t) = Cov(r_{t+l}, r_t) = Cov(r_{t_1}, r_{t_1-l})$

where $t_1 = t + l$.

In the finance literature, it is common to assume that an asset return series is weakly stationary. This assumption can be checked empirically provided that a sufficient number of historical returns are available. For example, one can divide the data into subsamples and check the consistency of the results obtained across the subsamples.

### Correlation and Autocorrelation Function

The correlation coefficient between two random variables $X$ and $Y$ is defined as:

$$
\rho_{x,y} = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}} = \frac{E[(X-\mu_x)(Y-\mu_y)]}{\sqrt{E(X-\mu_x)^2E(Y-\mu_y)^2}}
$$

where $\mu_x$ and $\mu_y$ are the mean of $X$ and $Y$, respectively, and it is assumed that the variances exist. This coefficient measures the strength of linear dependence between $X$ and $Y$, and it can be shown that $-1 \leq \rho_{x,y} \leq 1$ and $\rho_{x,y} = \rho_{y,x}$. The two random variables are uncorrelated if $\rho_{x,y} = 0$. In addition, if both $X$ and $Y$ are normal random variables, then $\rho_{x,y} = 0$ if and only if $X$ and $Y$ are independent.

When the sample $\{(x_t, y_t)\}_{t=1}^T$ is available, the correlation can be consistently estimated by its sample counterpart:

$$
\hat{\rho}_{x,y} = \frac{\sum_{t=1}^T (x_t - \bar{x})(y_t - \bar{y})}{\sqrt{\sum_{t=1}^T (x_t - \bar{x})^2 \sum_{t=1}^T (y_t - \bar{y})^2}}
$$

where $\bar{x} = \sum_{t=1}^T x_t/T$ and $\bar{y} = \sum_{t=1}^T y_t/T$ are the sample mean of $X$ and $Y$, respectively.

#### Autocorrelation Function (ACF)

Consider a weakly stationary return series $r_t$. When the linear dependence between $r_t$ and its past values $r_{t-i}$ is of interest, the concept of correlation is generalized to autocorrelation. The correlation coefficient between $r_t$ and $r_{t-l}$ is called the lag-$l$ autocorrelation of $r_t$ and is commonly denoted by $\rho_l$, which under the weak stationarity assumption is a function of $l$ only. Specifically, we define:

$$
\rho_l = \frac{Cov(r_t, r_{t-l})}{\sqrt{Var(r_t)Var(r_{t-l})}} = \frac{Cov(r_t, r_{t-l})}{Var(r_t)} = \frac{\gamma_l}{\gamma_0}
$$

where the property $Var(r_t) = Var(r_{t-l})$ for a weakly stationary series is used. From the definition, we have $\rho_0 = 1$, $\rho_l = \rho_{-l}$, and $-1 \leq \rho_l \leq 1$. In addition, a weakly stationary series $r_t$ is not serially correlated if and only if $\rho_l = 0$ for all $l > 0$.

For a given sample of returns $\{r_t\}_{t=1}^T$, let $\bar{r}$ be the sample mean (i.e., $\bar{r} = \sum_{t=1}^T r_t/T$). Then the lag-1 sample autocorrelation of $r_t$ is:

$$
\hat{\rho}_1 = \frac{\sum_{t=2}^T (r_t - \bar{r})(r_{t-1} - \bar{r})}{\sum_{t=1}^T (r_t - \bar{r})^2}
$$

Under some general conditions, $\hat{\rho}_1$ is a consistent estimate of $\rho_1$. For example, if $\{r_t\}$ is an independent and identically distributed (iid) sequence and $E(r_t^2) < \infty$, then $\hat{\rho}_1$ is asymptotically normal with mean zero and variance $1/T$.

In general, the lag-$l$ sample autocorrelation of $r_t$ is defined as:

$$
\hat{\rho}_l = \frac{\sum_{t=l+1}^T (r_t - \bar{r})(r_{t-l} - \bar{r})}{\sum_{t=1}^T (r_t - \bar{r})^2}, \quad 0 \leq l < T-1
$$

If $\{r_t\}$ is an iid sequence satisfying $E(r_t^2) < \infty$, then $\hat{\rho}_l$ is asymptotically normal with mean zero and variance $1/T$ for any fixed positive integer $l$.

More generally, if $r_t$ is a weakly stationary time series satisfying $r_t = \mu + \sum_{i=0}^q \psi_i a_{t-i}$, where $\psi_0 = 1$ and $\{a_j\}$ is a sequence of iid random variables with mean zero, then $\hat{\rho}_l$ is asymptotically normal with mean zero and variance $(1 + 2\sum_{i=1}^q \rho_i^2)/T$ for $l > q$. This is referred to as Bartlett's formula in the time series literature.

#### Testing Individual ACF

For a given positive integer $l$, the previous result can be used to test $H_0: \rho_l = 0$ vs. $H_a: \rho_l \neq 0$. The test statistic is:

$$
t \text{ ratio} = \frac{\hat{\rho}_l}{\sqrt{(1 + 2\sum_{i=1}^{l-1} \hat{\rho}_i^2)/T}}
$$

If $\{r_t\}$ is a stationary Gaussian series satisfying $\rho_j = 0$ for $j > l$, the $t$ ratio is asymptotically distributed as a standard normal random variable. Hence, the decision rule of the test is to reject $H_0$ if $|t \text{ ratio}| > Z_{\alpha/2}$, where $Z_{\alpha/2}$ is the $100(1-\alpha/2)$th percentile of the standard normal distribution.

#### Portmanteau Test

Financial applications often require to test jointly that several autocorrelations of $r_t$ are zero. Box and Pierce (1970) propose the Portmanteau statistic:

$$
Q^*(m) = T \sum_{l=1}^m \hat{\rho}_l^2
$$

as a test statistic for the null hypothesis $H_0: \rho_1 = \cdots = \rho_m = 0$ against the alternative hypothesis $H_a: \rho_i \neq 0$ for some $i \in \{1,...,m\}$. Under the assumption that $\{r_t\}$ is an iid sequence with certain moment conditions, $Q^*(m)$ is asymptotically a chi-squared random variable with $m$ degrees of freedom.

Ljung and Box (1978) modify the $Q^*(m)$ statistic as below to increase the power of the test in finite samples:

$$
Q(m) = T(T+2) \sum_{l=1}^m \frac{\hat{\rho}_l^2}{T-l}
$$

The decision rule is to reject $H_0$ if $Q(m) > \chi_\alpha^2$, where $\chi_\alpha^2$ denotes the $100(1-\alpha)$th percentile of a chi-squared distribution with $m$ degrees of freedom.

In practice, the choice of $m$ may affect the performance of the $Q(m)$ statistic. Several values of $m$ are often used. Simulation studies suggest that the choice of $m \approx \ln(T)$ provides better power performance. This general rule needs modification in analysis of seasonal time series for which autocorrelations with lags at multiples of the seasonality are more important.

### White Noise and Linear Time Series

#### White Noise

A time series $r_t$ is called a white noise if $\{r_t\}$ is a sequence of independent and identically distributed random variables with finite mean and variance. In particular, if $r_t$ is normally distributed with mean zero and variance $\sigma^2$, the series is called a Gaussian white noise. For a white noise series, all the ACFs are zero. In practice, if all sample ACFs are close to zero, then the series is a white noise series.

#### Linear Time Series

A time series $r_t$ is said to be linear if it can be written as:

$$
r_t = \mu + \sum_{i=0}^{\infty} \psi_i a_{t-i}
$$

where $\mu$ is the mean of $r_t$, $\psi_0 = 1$, and $\{a_t\}$ is a sequence of iid random variables with mean zero and a well-defined distribution (i.e., $\{a_t\}$ is a white noise series). It will be seen later that $a_t$ denotes the new information at time $t$ of the time series and is often referred to as the innovation or shock at time $t$. In this book, we are mainly concerned with the case where $a_t$ is a continuous random variable.

For a linear time series, the dynamic structure of $r_t$ is governed by the coefficients $\psi_i$, which are called the $\psi$ weights of $r_t$ in the time series literature. If $r_t$ is weakly stationary, we can obtain its mean and variance easily by using the independence of $\{a_t\}$ as:

$$
E(r_t) = \mu, \quad Var(r_t) = \sigma_a^2 \sum_{i=0}^{\infty} \psi_i^2
$$

where $\sigma_a^2$ is the variance of $a_t$. Because $Var(r_t) < \infty$, $\{\psi_i^2\}$ must be a convergent sequence, that is, $\psi_i^2 \to 0$ as $i \to \infty$. Consequently, for a stationary series, impact of the remote shock $a_{t-i}$ on the return $r_t$ vanishes as $i$ increases.

The lag-$l$ autocovariance of $r_t$ is:

$$
\gamma_l = Cov(r_t, r_{t-l}) = E\left[\left(\sum_{i=0}^{\infty} \psi_i a_{t-i}\right)\left(\sum_{j=0}^{\infty} \psi_j a_{t-l-j}\right)\right] = \sigma_a^2 \sum_{j=0}^{\infty} \psi_j \psi_{j+l}
$$

Consequently, the $\psi$ weights are related to the autocorrelations of $r_t$ as follows:

$$
\rho_l = \frac{\gamma_l}{\gamma_0} = \frac{\sum_{i=0}^{\infty} \psi_i \psi_{i+l}}{1 + \sum_{i=1}^{\infty} \psi_i^2}, \quad l \geq 0
$$

where $\psi_0 = 1$. Linear time series models are econometric and statistical models used to describe the pattern of the $\psi$ weights of $r_t$. For a weakly stationary time series, $\psi_i \to 0$ as $i \to \infty$ and, hence, $\rho_l$ converges to zero as $l$ increases. For asset returns, this means that, as expected, the linear dependence of current return $r_t$ on the remote past return $r_{t-l}$ diminishes for large $l$.
