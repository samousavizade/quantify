---
title: Von Neumann Growth Model (and a Generalization)
draft: false
summary: test
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

# The Von Neumann Economic Growth Model (with a Generalization)

This lecture employs the `Neumann` class to compute key elements of a
linear growth model developed by John von Neumann [[von Neumann, 1937](/courses/Introduction-to-Quantitative-Economics/References#id76)] and later expanded by
Kemeny, Morgenstern and Thompson [[Kemeny _et al._, 1956](/courses/Introduction-to-Quantitative-Economics/References#id79)].

The key elements of interest include the maximum growth rate ($ \alpha $), the
interest factor ($ β $), the optimal activity levels ($ x $), and
prices ($ p $).

Beyond showcasing John von Neumann's brilliant formulation of an equilibrium model for price and quantity vectors in
balanced growth, this lecture demonstrates the effective use of these important tools:

- a zero-sum two-player game
- linear programming
- the Perron-Frobenius theorem

Let's start with some imports:

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import fsolve, linprog
from textwrap import dedent

np.set_printoptions(precision=2)
```

The following code defines the `Neumann` class

```python
class Neumann(object):

    """
    This class describes the Generalized von Neumann growth model as it was
    discussed in Kemeny et al. (1956, ECTA) and Gale (1960, Chapter 9.5):

    Let:
    n ... number of goods
    m ... number of activities
    A ... input matrix is m-by-n
        a_{i,j} - amount of good j consumed by activity i
    B ... output matrix is m-by-n
        b_{i,j} - amount of good j produced by activity i

    x ... intensity vector (m-vector) with non-negative entries
        x'B - the vector of goods produced
        x'A - the vector of goods consumed
    p ... price vector (n-vector) with non-negative entries
        Bp - the revenue vector for every activity
        Ap - the cost of each activity

    Both A and B have non-negative entries. Moreover, we assume that
    (1) Assumption I (every good which is consumed is also produced):
        for all j, b_{.,j} > 0, i.e. at least one entry is strictly positive
    (2) Assumption II (no free lunch):
        for all i, a_{i,.} > 0, i.e. at least one entry is strictly positive

    Parameters
    ----------
    A : array_like or scalar(float)
        Part of the state transition equation.  It should be `n x n`
    B : array_like or scalar(float)
        Part of the state transition equation.  It should be `n x k`
    """

    def __init__(self, A, B):

        self.A, self.B = list(map(self.convert, (A, B)))
        self.m, self.n = self.A.shape

        # Check if (A, B) satisfy the basic assumptions
        assert self.A.shape == self.B.shape, 'The input and output matrices \
              must have the same dimensions!'
        assert (self.A >= 0).all() and (self.B >= 0).all(), 'The input and \
              output matrices must have only non-negative entries!'

        # (1) Check whether Assumption I is satisfied:
        if (np.sum(B, 0) <= 0).any():
            self.AI = False
        else:
            self.AI = True

        # (2) Check whether Assumption II is satisfied:
        if (np.sum(A, 1) <= 0).any():
            self.AII = False
        else:
            self.AII = True

    def __repr__(self):
        return self.__str__()

    def __str__(self):

        me = """
        Generalized von Neumann expanding model:
          - number of goods          : {n}
          - number of activities     : {m}

        Assumptions:
          - AI:  every column of B has a positive entry    : {AI}
          - AII: every row of A has a positive entry       : {AII}

        """
        # Irreducible                                       : {irr}
        return dedent(me.format(n=self.n, m=self.m,
                                AI=self.AI, AII=self.AII))

    def convert(self, x):
        """
        Convert array_like objects (lists of lists, floats, etc.) into
        well-formed 2D NumPy arrays
        """
        return np.atleast_2d(np.asarray(x))


    def bounds(self):
        """
        Calculate the trivial upper and lower bounds for alpha (expansion rate)
        and beta (interest factor). See the proof of Theorem 9.8 in Gale (1960)
        """

        n, m = self.n, self.m
        A, B = self.A, self.B

        f = lambda α: ((B - α * A) @ np.ones((n, 1))).max()
        g = lambda β: (np.ones((1, m)) @ (B - β * A)).min()

        UB = fsolve(f, 1).item()  # Upper bound for α, β
        LB = fsolve(g, 2).item()  # Lower bound for α, β

        return LB, UB


    def zerosum(self, γ, dual=False):
        """
        Given gamma, calculate the value and optimal strategies of a
        two-player zero-sum game given by the matrix

                M(gamma) = B - gamma * A

        Row player maximizing, column player minimizing

        Zero-sum game as an LP (primal --> α)

            max (0', 1) @ (x', v)
            subject to
            [-M', ones(n, 1)] @ (x', v)' <= 0
            (x', v) @ (ones(m, 1), 0) = 1
            (x', v) >= (0', -inf)

        Zero-sum game as an LP (dual --> beta)

            min (0', 1) @ (p', u)
            subject to
            [M, -ones(m, 1)] @ (p', u)' <= 0
            (p', u) @ (ones(n, 1), 0) = 1
            (p', u) >= (0', -inf)

        Outputs:
        --------
        value: scalar
            value of the zero-sum game

        strategy: vector
            if dual = False, it is the intensity vector,
            if dual = True, it is the price vector
        """

        A, B, n, m = self.A, self.B, self.n, self.m
        M = B - γ * A

        if dual == False:
            # Solve the primal LP (for details see the description)
            # (1) Define the problem for v as a maximization (linprog minimizes)
            c = np.hstack([np.zeros(m), -1])

            # (2) Add constraints :
            # ... non-negativity constraints
            bounds = tuple(m * [(0, None)] + [(None, None)])
            # ... inequality constraints
            A_iq = np.hstack([-M.T, np.ones((n, 1))])
            b_iq = np.zeros((n, 1))
            # ... normalization
            A_eq = np.hstack([np.ones(m), 0]).reshape(1, m + 1)
            b_eq = 1

            res = linprog(c, A_ub=A_iq, b_ub=b_iq, A_eq=A_eq, b_eq=b_eq,
                          bounds=bounds)

        else:
            # Solve the dual LP (for details see the description)
            # (1) Define the problem for v as a maximization (linprog minimizes)
            c = np.hstack([np.zeros(n), 1])

            # (2) Add constraints :
            # ... non-negativity constraints
            bounds = tuple(n * [(0, None)] + [(None, None)])
            # ... inequality constraints
            A_iq = np.hstack([M, -np.ones((m, 1))])
            b_iq = np.zeros((m, 1))
            # ... normalization
            A_eq = np.hstack([np.ones(n), 0]).reshape(1, n + 1)
            b_eq = 1

            res = linprog(c, A_ub=A_iq, b_ub=b_iq, A_eq=A_eq, b_eq=b_eq,
                          bounds=bounds)

        if res.status != 0:
            print(res.message)

        # Pull out the required quantities
        value = res.x[-1]
        strategy = res.x[:-1]

        return value, strategy


    def expansion(self, tol=1e-8, maxit=1000):
        """
        The algorithm used here is described in Hamburger-Thompson-Weil
        (1967, ECTA). It is based on a simple bisection argument and utilizes
        the idea that for a given γ (= α or β), the matrix "M = B - γ * A"
        defines a two-player zero-sum game, where the optimal strategies are
        the (normalized) intensity and price vector.

        Outputs:
        --------
        alpha: scalar
            optimal expansion rate
        """

        LB, UB = self.bounds()

        for iter in range(maxit):

            γ = (LB + UB) / 2
            ZS = self.zerosum(γ=γ)
            V = ZS[0]     # value of the game with γ

            if V >= 0:
                LB = γ
            else:
                UB = γ

            if abs(UB - LB) < tol:
                γ = (UB + LB) / 2
                x = self.zerosum(γ=γ)[1]
                p = self.zerosum(γ=γ, dual=True)[1]
                break

        return γ, x, p

    def interest(self, tol=1e-8, maxit=1000):
        """
        The algorithm used here is described in Hamburger-Thompson-Weil
        (1967, ECTA). It is based on a simple bisection argument and utilizes
        the idea that for a given gamma (= alpha or beta),
        the matrix "M = B - γ * A" defines a two-player zero-sum game,
        where the optimal strategies are the (normalized) intensity and price
        vector

        Outputs:
        --------
        beta: scalar
            optimal interest rate
        """

        LB, UB = self.bounds()

        for iter in range(maxit):
            γ = (LB + UB) / 2
            ZS = self.zerosum(γ=γ, dual=True)
            V = ZS[0]

            if V > 0:
                LB = γ
            else:
                UB = γ

            if abs(UB - LB) < tol:
                γ = (UB + LB) / 2
                p = self.zerosum(γ=γ, dual=True)[1]
                x = self.zerosum(γ=γ)[1]
                break

        return γ, x, p
```

## Symbols and Conventions

We'll use the following notation and conventions.

$ \mathbf{0} $ represents
a vector of zeros.

A positive $ n $-vector is denoted as
$ x\gg \mathbf{0} $ if $ x_i>0 $ for all $ i=1,2,\dots,n $.

A non-negative vector is written as $ x\geq \mathbf{0} $ if $ x_i\geq 0 $ for
all $ i=1,2,\dots,n $.

A semi-positive vector is denoted as $ x > \mathbf{0} $ if
$ x\geq \mathbf{0} $ and $ x\neq \mathbf{0} $.

For two compatible vectors $ x $ and $ y $, $ x\gg y $,
$ x\geq y $ and $ x> y $ signify $ x-y\gg \mathbf{0} $,
$ x-y \geq \mathbf{0} $, and $ x-y > \mathbf{0} $, respectively.

All vectors in this lecture are column vectors; $ x^{T} $ denotes the
transpose of $ x $ (i.e., a row vector).

$ \iota_n $ represents a
column vector of $ n $ ones, i.e.
$ \iota_n = (1,1,\dots,1)^T $.

$ e^i $ denotes a vector (of
arbitrary size) with zeros everywhere except for a one in the $ i $ th position.

Matrices are represented by capital letters. For any matrix
$ A $, $ a\_{i,j} $ denotes the entry in its $ i $ th
row and $ j $ th column.

$ a*{\cdot j} $ and $ a*{i\cdot} $
represent the $ j $ th column and $ i $ th row of $ A $,
respectively.

## Model Components and Premises

An economy is defined by a pair $ (A,B) $ of $ m\times n $ non-negative matrices.

- $ m $ represents the number of _activities_ (or sectors)
- $ n $ is the number of _goods_ (produced and/or consumed)
- $ A $ is termed the _input matrix_; $ a\_{i,j} $ indicates the
  quantity of good $ j $ used by activity $ i $
- $ B $ is called the _output matrix_; $ b\_{i,j} $ represents
  the quantity of good $ j $ produced by activity $ i $

Two key assumptions constrain the economy $ (A,B) $:

- **Assumption I:** (every consumed good is also produced)
  $$
  b_{.,j} > \mathbf{0}\hspace{5mm}\forall j=1,2,\dots,n
  $$
- **Assumption II:** (no free lunch)
  $$
  a_{i,.} > \mathbf{0}\hspace{5mm}\forall i=1,2,\dots,m
  $$

A semi-positive _intensity_ $ m $-vector $ x $ indicates the levels at which
activities are operated.

Consequently,

- vector $ x^TA $ gives the total amount of _goods consumed in
  production_
- vector $ x^TB $ gives _total outputs_

An economy $ (A,B) $ is considered _productive_ if there exists a
non-negative intensity vector $ x \geq 0 $ such
that $ x^T B > x^TA $.

The semi-positive $ n $-vector $ p $ contains prices assigned to
the $ n $ goods.

The $ p $ vector implies _cost_ and _revenue_ vectors

- the vector $ Ap $ represents _costs_ of the vector of activities
- the vector $ Bp $ represents _revenues_ from the vector of activities

Fulfillment of a property of an input-output pair $ (A,B) $ called _irreducibility_
(or indecomposability) determines whether an economy can be broken down
into multiple "sub-economies".

**Definition:** For an economy $ (A,B) $, the set of goods
$ S\subset \{1,2,\dots,n\} $ is called an _independent subset_ if
it's possible to produce every good in $ S $ without using
goods from outside $ S $. Formally, the set $ S $ is independent if
$ \exists T\subset \{1,2,\dots,m\} $ (a subset of activities) such
that $ a*{i,j}=0 $ $ \forall i\in T $ and $ j\in S^c $ and
for all $ j\in S $, $ \exists i\in T $ for which $ b*{i,j}>0 $.
The economy is **irreducible** if there are no proper independent
subsets.

We'll examine two examples, both from Chapter 9.6 of Gale [[Gale, 1989](/courses/Introduction-to-Quantitative-Economics/References#id81)]

```python
# (1) Irreducible (A, B) example: α_0 = β_0
A1 = np.array([[0, 1, 0, 0],
               [1, 0, 0, 1],
               [0, 0, 1, 0]])

B1 = np.array([[1, 0, 0, 0],
               [0, 0, 2, 0],
               [0, 1, 0, 1]])

# (2) Reducible (A, B) example: β_0 < α_0
A2 = np.array([[0, 1, 0, 0, 0, 0],
               [1, 0, 1, 0, 0, 0],
               [0, 0, 0, 1, 0, 0],
               [0, 0, 1, 0, 0, 1],
               [0, 0, 0, 0, 1, 0]])

B2 = np.array([[1, 0, 0, 1, 0, 0],
               [0, 1, 0, 0, 0, 0],
               [0, 0, 1, 0, 0, 0],
               [0, 0, 0, 0, 2, 0],
               [0, 0, 0, 1, 0, 1]])
```

The code below sets up our first Neumann economy or `Neumann`
instance

```python
n1 = Neumann(A1, B1)
n1
```

    Generalized von Neumann expanding model:
      - number of goods          : 4
      - number of activities     : 3

    Assumptions:
      - AI:  every column of B has a positive entry    : True
      - AII: every row of A has a positive entry       : True

Here's a second instance of a Neumann economy

```python
n2 = Neumann(A2, B2)
n2
```

    Generalized von Neumann expanding model:
      - number of goods          : 6
      - number of activities     : 5

    Assumptions:
      - AI:  every column of B has a positive entry    : True
      - AII: every row of A has a positive entry       : True

## Temporal Interpretation

Assign a time index $ t $ to the preceding objects, view an economy
as a dynamic system, and examine sequences

$$
\{(A_t,B_t)\}_{t\geq 0}, \hspace{1cm}\{x_t\}_{t\geq 0},\hspace{1cm} \{p_t\}_{t\geq 0}
$$

An intriguing special case keeps the technology process constant and
investigates only the dynamics of quantities and prices.

Accordingly, for the remainder of this lecture, we assume that
$ (A_t,B_t)=(A,B) $ for all $ t\geq 0 $.

A crucial aspect of the dynamic interpretation involves the timing of
production.

We assume that production (input consumption) occurs in period
$ t $, while the resulting output materializes in period
$ t+1 $, i.e., consumption of $ x*{t}^TA $ in period $ t $
results in $ x^T*{t}B $ amounts of output in period $ t+1 $.

These timing conventions imply the following feasibility condition:

$$
\begin{aligned}
x^T_{t}B \geq x^T_{t+1} A \hspace{1cm}\forall t\geq 1
\end{aligned}
$$

which states that no more goods can be used today than were produced
yesterday.

Accordingly, $ Ap_t $ represents the costs of production in period
$ t $ and $ Bp_t $ represents revenues in period $ t+1 $.

### Balanced Growth

We follow John von Neumann in examining "balanced growth".

Let $ ./ $ denote an elementwise division of one vector by another and let
$ \alpha >0 $ be a scalar.

Then _balanced growth_ is a situation in which

$$
x_{t+1}./x_t = \alpha , \quad \forall t \geq 0
$$

With balanced growth, the law of motion of $ x $ is clearly $ x\_{t+1}=\alpha x_t $
and so we can restate the feasibility constraint as

$$
x^T_{t}B \geq \alpha x^T_t A \hspace{1cm}\forall t
$$

In the same vein, define $ \beta\in\mathbb{R} $ as the **interest
factor** per unit of time.

We assume that it's always possible to earn a gross return equal to the
constant interest factor $ \beta $ by investing "outside the model".

Under this assumption about external investment opportunities, a
no-arbitrage condition gives rise to the following (no profit)
restriction on the price sequence:

$$
\beta Ap_{t} \geq B p_{t} \hspace{1cm}\forall t
$$

This indicates that production cannot yield a return greater than that
offered by the external investment opportunity (here we compare values in
period $ t+1 $).

The balanced growth assumption allows us to drop time subscripts and
conduct an analysis solely in terms of a time-invariant growth rate
$ \alpha $ and interest factor $ \beta $.

## Duality

Two problems are connected by a remarkable dual
relationship between technological and valuation characteristics of
the economy:

**Definition:** The _technological expansion problem_ (TEP) for the economy
$ (A,B) $ is to find a semi-positive $ m $-vector $ x>0 $
and a number $ \alpha\in\mathbb{R} $ that satisfy

$$
\begin{aligned}
&\max_{\alpha} \hspace{2mm} \alpha\\
&\text{s.t. }\hspace{2mm}x^T B \geq \alpha x^T A
\end{aligned}
$$

Theorem 9.3 of David Gale’s book [[Gale, 1989](/courses/Introduction-to-Quantitative-Economics/References#id81)] asserts that if Assumptions I and II are
both satisfied, then a maximum value of $ \alpha $ exists and that it is
positive.

The maximal value is called the _technological expansion rate_ and is denoted
by $ \alpha_0 $. The associated intensity vector $ x_0 $ is the
_optimal intensity vector_.

**Definition:** The economic expansion problem (EEP) for
$ (A,B) $ is to find a semi-positive $ n $-vector $ p>0 $
and a number $ \beta\in\mathbb{R} $ that satisfy

$$
\begin{aligned}
&\min_{\beta} \hspace{2mm} \beta\\
&\text{s.t. }\hspace{2mm}Bp \leq \beta Ap
\end{aligned}
$$

Assumptions I and II imply existence of a minimum value
$ \beta_0>0 $ called the _economic expansion rate_.

The corresponding price vector $ p_0 $ is the _optimal price vector_.

Because the criterion functions in the _technological expansion_ problem
and the _economical expansion problem_ are both linearly homogeneous,
the optimality of $ x_0 $ and $ p_0 $ are defined only up to a
positive scale factor.

For convenience (and to emphasize a close connection to zero-sum games), we normalize both vectors
$ x_0 $ and $ p_0 $ to have unit length.

A standard duality argument (see Lemma 9.4. in (Gale, 1960) [[Gale, 1989](/courses/Introduction-to-Quantitative-Economics/References#id81)]) implies
that under Assumptions I and II, $ \beta_0\leq \alpha_0 $.

But to deduce that $ \beta_0\geq \alpha_0 $,
Assumptions I and II are not sufficient.

Therefore, von Neumann [[von Neumann, 1937](/courses/Introduction-to-Quantitative-Economics/References#id76)] went on to prove the following remarkable
“duality” result that connects TEP and EEP.

**Theorem 1 (von Neumann):** If the economy $ (A,B) $ satisfies
Assumptions I and II, then there exist
$ \left(\gamma^{_}, x_0, p_0\right) $, where
$ \gamma^{_}\in[\beta_0, \alpha_0]\subset\mathbb{R} $, $ x_0>0 $
is an $ m $-vector, $ p_0>0 $ is an $ n $-vector, and the
following arbitrage true

$$
\begin{aligned}
x_0^T B &\geq \gamma^{* } x_0^T A \\
Bp_0 &\leq \gamma^{* } Ap_0 \\
x_0^T\left(B-\gamma^{* } A\right)p_0 &= 0
\end{aligned}
$$

> **Note\***Proof (Sketch):_ Assumption I and II imply that there exist $ (\alpha_0,
> x_0) $ and $ (\beta_0, p_0) $ that solve the TEP and EEP, respectively. If
> $ \gamma^_>\alpha_0 $, then by definition of $ \alpha_0 $, there cannot
exist a semi-positive $ x $ that satisfies $ x^T B \geq \gamma^{* }
x^T A $.  Similarly, if $ \gamma^*<\beta_0 $, there is no semi-positive
$ p $ for which $ Bp \leq \gamma^{_ } Ap $. Let $ \gamma^{_
> }\in[\beta_0, \alpha_0] $, then $ x_0^T B \geq \alpha_0 x_0^T A \geq
> \gamma^{_ } x_0^T A $. Moreover, $ Bp_0\leq \beta_0 A p_0\leq \gamma^_ A
> p_0 $. These two inequalities imply $ x_0\left(B - \gamma^{\* } A\right)p_0
> = 0 $.

Here the constant $ \gamma^{\*} $ is both an expansion factor and an interest
factor (not necessarily optimal).

We have already encountered and
discussed the first two inequalities that represent feasibility and
no-profit conditions.

Moreover, the equality $ x_0^T\left(B-\gamma^{_ } A\right)p_0 = 0 $ concisely expresses the
requirements that if any good grows at a rate larger than
$ \gamma^{_} $ (i.e., if it is _oversupplied_), then its price
must be zero; and that if any activity provides negative profit, it must
be unused.

Therefore, the conditions stated in Theorem I ex encode all equilibrium conditions.

So Theorem I essentially states that under Assumptions I and II there
always exists an equilibrium $ \left(\gamma^{\*}, x_0, p_0\right) $
with balanced growth.

Note that Theorem I is silent about uniqueness of the equilibrium. In
fact, it does not rule out (trivial) cases with $ x_0^TBp_0 = 0 $ so
that nothing of value is produced.

To exclude such uninteresting cases,
Kemeny, Morgenstern and Thomspson [[Kemeny _et al._, 1956](/courses/Introduction-to-Quantitative-Economics/References#id79)] add an extra requirement

$$
x^T_0 B p_0 > 0
$$

and call the associated equilibria _economic solutions_.

They show that
this extra condition does not affect the existence result, while it
significantly reduces the number of (relevant) solutions.

## Interpretation as Two-player Zero-sum Game

To compute the equilibrium $ (\gamma^{\*}, x_0, p_0) $, we follow the
approach suggested by Hamburger, Thompson and Weil (1967), leveraging
the key insight that an equilibrium (with balanced growth) can be
found by solving a specific two-player zero-sum game.
First, we introduce some notation.

Consider the $ m\times n $ matrix $ C $ as a payoff matrix,
with the entries representing payoffs from the **minimizing** column
player to the **maximizing** row player and assume that the players can
use mixed strategies. Thus,

- the row player selects the $ m $-vector $ x > \mathbf{0} $ subject to $ \iota_m^T x = 1 $
- the column player selects the $ n $-vector $ p > \mathbf{0} $ subject to $ \iota_n^T p = 1 $.

**Definition:** The $ m\times n $ matrix game $ C $ has the
_solution_ $ (x^_, p^_, V(C)) $ in mixed strategies if

$$
\begin{aligned}
(x^* )^T C e^j \geq V(C)\quad \forall j\in\{1, \dots, n\}\quad \quad
\text{and}\quad\quad (e^i)^T C p^* \leq V(C)\quad \forall i\in\{1, \dots, m\}
\end{aligned}
$$

The number $ V(C) $ is referred to as the _value_ of the game.

From the above definition, it's evident that the value $ V(C) $ has
two alternative interpretations:

- by employing the appropriate mixed
  strategy, the maximizing player can guarantee at least $ V(C) $
  (regardless of the column player's choice)
- by employing the appropriate
  mixed strategy, the minimizing player can ensure that the maximizing
  player will not receive more than $ V(C) $ (regardless of the
  maximizing player's choice)

A famous theorem by Nash (1951) tells us that there always
exists a mixed strategy Nash equilibrium for any _finite_ two-player
zero-sum game.

Moreover, von Neumann's Minmax Theorem [[von Neumann, 1928](/courses/Introduction-to-Quantitative-Economics/References#id77)] implies that

$$
V(C) = \max_x \min_p \hspace{2mm} x^T C p = \min_p \max_x \hspace{2mm} x^T C p = (x^*)^T C p^*
$$

### Connection with Linear Programming (LP)

Nash equilibria of a finite two-player zero-sum game can be found by solving a linear programming problem.

To illustrate this, we introduce
the following notation

- For a given $ x $, let $ v $ be the value of the minimization problem: $ v \equiv \min_p x^T C p = \min_j x^T C e^j $
- For a given $ p $, let $ u $ be the value of the maximization problem: $ u \equiv \max_x x^T C p = \max_i (e^i)^T C p $

Then the _max-min problem_ (the game from the maximizing player's perspective)
can be expressed as the _primal_ LP

$$
\begin{aligned}
V(C) = & \max \hspace{2mm} v \\
\text{s.t. } \hspace{2mm} v \iota_n^T &\leq x^T C \\
x &\geq \mathbf{0} \\
\iota_n^T x & = 1
\end{aligned}
$$

while the _min-max problem_ (the game from the minimizing player's perspective)
is the _dual_ LP

$$
\begin{aligned}
V(C) = &\min \hspace{2mm} u \\
\text{s.t. } \hspace{2mm}u \iota_m &\geq Cp \\
p &\geq \mathbf{0} \\
\iota_m^T p & = 1
\end{aligned}
$$

Hamburger, Thompson and Weil [[Hamburger _et al._, 1967](/courses/Introduction-to-Quantitative-Economics/References#id80)] interpret the input-output pair of the
economy as payoff matrices of two-player zero-sum games.

Using this interpretation, they restate Assumption I and II as follows

$$
V(-A) < 0\quad\quad \text{and}\quad\quad V(B)>0
$$

> **Note\***Proof (Outline)\*:

- $ \Rightarrow $ $ V(B)>0 $ implies
  $ x_0^T B \gg \mathbf{0} $, where $ x_0 $ is a maximizing
  vector. Since $ B $ is non-negative, this requires that each
  column of $ B $ has at least one positive entry, which is
  Assumption I.
- $ \Leftarrow $ From Assumption I and the fact
  that $ p>\mathbf{0} $, it follows that $ Bp > \mathbf{0} $.
  This implies that the maximizing player can always choose $ x $
  so that $ x^TBp>0 $ so that it must be the case
  that $ V(B)>0 $.

To (re)state Theorem I in terms of a particular two-player
zero-sum game, we define a matrix for $ \gamma\in\mathbb{R} $

$$
M(\gamma) \equiv B - \gamma A
$$

For fixed $ \gamma $, treating $ M(\gamma) $ as a matrix game,
solving the game implies

- If $ \gamma > \alpha_0 $, then for all $ x>0 $, there
$ \exists j\in\{1, \dots, n\} $, s.t.
$ [x^T M(\gamma)]\_j < 0 $ implying
  that $ V(M(\gamma)) < 0 $.
- If $ \gamma < \beta_0 $, then for all $ p>0 $, there
$ \exists i\in\{1, \dots, m\} $, s.t.
$ [M(\gamma)p]\_i > 0 $ implying that $ V(M(\gamma)) > 0 $.
- If $ \gamma \in \{\beta_0, \alpha_0\} $, then (by Theorem I) the
  optimal intensity and price vectors $ x_0 $ and $ p_0 $
  satisfy

$$
\begin{aligned}
x_0^T M(\gamma) \geq \mathbf{0}^T \quad \quad \text{and}\quad\quad M(\gamma) p_0 \leq \mathbf{0}
\end{aligned}
$$

That is, $ (x_0, p_0, 0) $ is a solution of the game
$ M(\gamma) $ so
that $ V\left(M(\beta_0)\right) = V\left(M(\alpha_0)\right) = 0 $.

- If $ \beta_0 < \alpha_0 $ and
  $ \gamma \in (\beta_0, \alpha_0) $, then $ V(M(\gamma)) = 0 $.

Moreover, if $ x' $ is optimal for the maximizing player in
$ M(\gamma') $ for $ \gamma'\in(\beta_0, \alpha_0) $ and
$ p'' $ is optimal for the minimizing player in $ M(\gamma'') $
where $ \gamma''\in(\beta_0, \gamma') $, then $ (x', p'', 0) $
is a solution for $ M(\gamma) $ $ \forall \gamma\in (\gamma'', \gamma') $.

_Proof (Outline):_ If $ x' $ is optimal for a maximizing player in
game $ M(\gamma') $, then $ (x')^T M(\gamma')\geq \mathbf{0}^T $ and so for all $ \gamma<\gamma' $.

$$
(x')^T M(\gamma) = (x')^T M(\gamma') + (x')^T(\gamma' - \gamma)A \geq \mathbf{0}^T
$$

hence $ V(M(\gamma))\geq 0 $. If $ p'' $ is optimal for a
minimizing player in game $ M(\gamma'') $, then $ M(\gamma)p \leq \mathbf{0} $
and so for all $ \gamma''<\gamma $

$$
M(\gamma)p'' = M(\gamma'') + (\gamma'' - \gamma)Ap'' \leq \mathbf{0}
$$

hence $ V(M(\gamma))\leq 0 $.

It's clear from the above argument that $ \beta_0 $, $ \alpha_0 $ are the minimal and maximal $ \gamma $ for which
$ V(M(\gamma))=0 $.

Furthermore, Hamburger et al. [[Hamburger _et al._, 1967](/courses/Introduction-to-Quantitative-Economics/References#id80)] demonstrate that the
function $ \gamma \mapsto V(M(\gamma)) $ is continuous and
nonincreasing in $ \gamma $.

This suggests an algorithm to compute
$ (\alpha_0, x_0) $ and $ (\beta_0, p_0) $ for a given
input-output pair $ (A, B) $.

### Algorithm

Hamburger, Thompson and Weil [[Hamburger _et al._, 1967](/courses/Introduction-to-Quantitative-Economics/References#id80)] suggest a straightforward bisection algorithm
to determine the minimal and maximal roots (i.e. $ \beta_0 $ and
$ \alpha_0 $) of the function $ \gamma \mapsto V(M(\gamma)) $.

#### Step 1

First, observe that we can readily find trivial upper and lower bounds for
$ \alpha_0 $ and $ \beta_0 $.

- TEP necessitates that
  $ x^T(B-\alpha A)\geq \mathbf{0}^T $ and $ x > \mathbf{0} $, so
if $ \alpha $ is sufficiently large that
$ \max_i\{[(B-\alpha A)\iota_n]\_i\} < 0 $, then TEP no longer has a
  solution.

Accordingly, let **`UB`** be the $ \alpha^{_} $ that
solves $ \max_i\{[(B-\alpha^{_} A)\iota_n]\_i\} = 0 $.

- Similarly to
  the upper bound, if $ \beta $ is so small that
  $ \min_j\{[\iota^T_m(B-\beta A)]\_j\}>0 $, then the EEP has no
  solution and so we can define **`LB`** as the $ \beta^{_} $ that
  solves $ \min_j\{[\iota^T_m(B-\beta^{_} A)]\_j\}=0 $.

The _bounds_ method computes these trivial bounds for us

```python
n1.bounds()
```

    (1.0, 2.0)

#### Step 2

Determine $ \alpha_0 $ and $ \beta_0 $

- Finding $ \alpha_0 $

1.  Set $ \gamma = \frac{UB + LB}{2} $ and solve the two-player zero-sum game associated
    with $ M(\gamma) $. We can employ either the primal or the dual
    LP problem.
1.  If $ V(M(\gamma)) \geq 0 $, then set $ LB = \gamma $,
    otherwise set $ UB = \gamma $.
1.  Repeat steps 1. and 2. until $ |UB - LB| < \epsilon $.

- Finding $ \beta_0 $

1.  Set $ \gamma = \frac{UB + LB}{2} $ and solve the two-player zero-sum game associated
    with $ M(\gamma) $. We can employ either the primal or the dual
    LP problem.
1.  If $ V(M(\gamma)) > 0 $, then set $ LB = \gamma $,
    otherwise set $ UB = \gamma $.
1.  Repeat steps 1. and 2. until $ |UB - LB| < \epsilon $.

- _Existence_: Since $ V(M(LB))>0 $ and $ V(M(UB))<0 $ and
  $ V(M(\cdot)) $ is a continuous, nonincreasing function, there exists
  at least one $ \gamma\in[LB, UB] $, s.t. $ V(M(\gamma))=0 $.

The _zerosum_ method calculates the value and optimal strategies
associated with a given $ \gamma $.

```python
γ = 2

print(f'Value of the game with γ = {γ}')
print(n1.zerosum(γ=γ)[0])
print('Intensity vector (from the primal)')
print(n1.zerosum(γ=γ)[1])
print('Price vector (from the dual)')
print(n1.zerosum(γ=γ, dual=True)[1])
```

    Value of the game with γ = 2
    -0.24
    Intensity vector (from the primal)
    [0.32 0.28 0.4 ]
    Price vector (from the dual)
    [0.4  0.32 0.28 0.  ]

```python
numb_grid = 100
γ_grid = np.linspace(0.4, 2.1, numb_grid)

value_ex1_grid = np.asarray([n1.zerosum(γ=γ_grid[i])[0]
                            for i in range(numb_grid)])
value_ex2_grid = np.asarray([n2.zerosum(γ=γ_grid[i])[0]
                            for i in range(numb_grid)])

fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)
fig.suptitle(r'The function $V(M(\gamma))$', fontsize=16)

for ax, grid, N, i in zip(axes, (value_ex1_grid, value_ex2_grid),
                          (n1, n2), (1, 2)):
    ax.plot(γ_grid, grid)
    ax.set(title=f'Example {i}', xlabel='$\gamma$')
    ax.axhline(0, c='k', lw=1)
    ax.axvline(N.bounds()[0], c='r', ls='--', label='lower bound')
    ax.axvline(N.bounds()[1], c='g', ls='--', label='upper bound')

plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/output_23_0.png)
</div>

The _expansion_ method implements the bisection algorithm for
$ \alpha_0 $ (and utilizes the primal LP problem for $ x_0 $)

```python
α_0, x, p = n1.expansion()
print(f'α_0 = {α_0}')
print(f'x_0 = {x}')
print(f'The corresponding p from the dual = {p}')
```

    α_0 = 1.2599210478365421
    x_0 = [0.33 0.26 0.41]
    The corresponding p from the dual = [0.41 0.33 0.26 0.  ]

The _interest_ method implements the bisection algorithm for
$ \beta_0 $ (and utilizes the dual LP problem for $ p_0 $)

```python
β_0, x, p = n1.interest()
print(f'β_0 = {β_0}')
print(f'p_0 = {p}')
print(f'The corresponding x from the primal = {x}')
```

    β_0 = 1.2599210478365421
    p_0 = [0.41 0.33 0.26 0.  ]
    The corresponding x from the primal = [0.33 0.26 0.41]

Naturally, when $ \gamma^\* $ is unique, it is immaterial which of the two methods we use – both are effective.

Specifically, as will be demonstrated below, in
the case of an irreducible $ (A,B) $ (like in Example 1), the maximal
and minimal roots of $ V(M(\gamma)) $ necessarily coincide, implying
a 'full duality' result, i.e. $ \alpha_0 = \beta_0 = \gamma^_ $ so that the expansion (and interest) rate $ \gamma^_ $ is unique.

### Uniqueness and Irreducibility

As an illustration, let's first compute the maximal and minimal roots of
$ V(M(\cdot)) $ for our Example 2 that has a reducible
input-output pair $ (A, B) $

```python
α_0, x, p = n2.expansion()
print(f'α_0 = {α_0}')
print(f'x_0 = {x}')
print(f'The corresponding p from the dual = {p}')
```

    α_0 = 1.259921052493155
    x_0 = [5.27e-10 0.00e+00 3.27e-01 2.60e-01 4.13e-01]
    The corresponding p from the dual = [0.   0.21 0.33 0.26 0.21 0.  ]

```python
β_0, x, p = n2.interest()
print(f'β_0 = {β_0}')
print(f'p_0 = {p}')
print(f'The corresponding x from the primal = {x}')
```

    β_0 = 1.0000000009313226
    p_0 = [ 5.00e-01  5.00e-01 -1.55e-09 -1.24e-09 -9.31e-10  0.00e+00]
    The corresponding x from the primal = [-0.    0.    0.25  0.25  0.5 ]

As we can observe, with a reducible $ (A,B) $, the roots found by the
bisection algorithms may differ, so there might be multiple
$ \gamma^_ $ that make the value of the game
with $ M(\gamma^_) $ zero. (refer to the figure above).

Indeed, while the von Neumann theorem ensures existence of the
equilibrium, Assumptions I and II are insufficient for uniqueness.
Nonetheless, Kemeny et al. (1967) demonstrate that there are at most finitely
many economic solutions, meaning that there are only finitely many
$ \gamma^_ $ that satisfy $ V(M(\gamma^_)) = 0 $ and
$ x_0^TBp_0 > 0 $ and that for each such $ \gamma^_\_i $, there
exists a self-contained part of the economy (a sub-economy) that in
equilibrium can expand independently with the expansion
coefficient $ \gamma^_\_i $.

The following theorem (see Theorem 9.10. in Gale [[Gale, 1989](/courses/Introduction-to-Quantitative-Economics/References#id81)]) states that
imposing irreducibility is sufficient for uniqueness of
$ (\gamma^\*, x_0, p_0) $.

**Theorem II:** Given the conditions of Theorem 1. If the economy
$ (A,B) $ is irreducible, then $ \gamma^\*=\alpha_0=\beta_0 $.

### A Special Case

There exists a special $ (A,B) $ that allows us to simplify the solution
method considerably by invoking the powerful Perron-Frobenius theorem
for non-negative matrices.

**Definition:** We term an economy _simple_ if it satisfies

- $ n=m $
- Each activity produces exactly one good
- Each good is produced by one and only one activity

These assumptions imply that $ B=I_n $, i.e., that $ B $ can be
represented as an identity matrix (possibly after rearranging its rows and
columns).

The simple model possesses the following special property (Theorem 9.11. in Gale [[Gale, 1989](/courses/Introduction-to-Quantitative-Economics/References#id81)]): if $ x_0 $ and $ \alpha_0>0 $ solve the TEP
with $ (A,I_n) $, then

$$
x_0^T = \alpha_0 x_0^T A\hspace{1cm}\Leftrightarrow\hspace{1cm}x_0^T
A=\left(\frac{1}{\alpha_0}\right)x_0^T
$$

The latter indicates that $ 1/\alpha_0 $ is a positive eigenvalue of
$ A $ and $ x_0 $ is the corresponding non-negative left
eigenvector.

The classic result of **Perron and Frobenius** states
that a non-negative matrix has a non-negative
eigenvalue-eigenvector pair.

Moreover, if $ A $ is irreducible, then
the optimal intensity vector $ x_0 $ is positive and _unique_ up to
multiplication by a positive scalar.

Suppose that $ A $ is reducible with $ k $ irreducible subsets
$ S_1,\dots,S_k $. Let $ A_i $ be the submatrix corresponding to
$ S_i $ and let $ \alpha_i $ and $ \beta_i $ be the
associated expansion and interest factors, respectively. Then we have

$$
\alpha_0 = \max_i \{\alpha_i\}\hspace{1cm}\text{and}\hspace{1cm}\beta_0 = \min_i \{\beta_i\}
$$
