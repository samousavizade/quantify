---
title: Linear State Space Models
draft: false
summary: test
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

# Linear State Space Models

“We may regard the present state of the universe as the effect of its past and the cause of its future” – Marquis de Laplace

In addition to what’s in Anaconda, this lecture will need the following libraries:

```python
!pip install quantecon
```

## Overview

This lecture introduces the **linear state space** dynamic system.

The linear state space system is a generalization of the scalar AR(1) process we studied before.

This model is a workhorse that carries a powerful theory of prediction.

Its many applications include:

- representing dynamics of higher-order linear systems
- predicting the position of a system $ j $ steps into the future
- predicting a geometric sum of future values of a variable like
- non-financial income
- dividends on a stock
- the money supply
- a government deficit or surplus, etc.
- key ingredient of useful models
- Friedman’s permanent income model of consumption smoothing.
- Barro’s model of smoothing total tax collections.
- Rational expectations version of Cagan’s model of hyperinflation.
- Sargent and Wallace’s “unpleasant monetarist arithmetic,” etc.

Let’s start with some imports:

```python
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (11, 5)  #set default figure size
import numpy as np
from quantecon import LinearStateSpace
from scipy.stats import norm
import random
```

    OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.

## The Linear State Space Model

The objects in play are:

- An $ n \times 1 $ vector $ x_t $ denoting the **state** at time $ t = 0, 1, 2, \ldots $.
- An IID sequence of $ m \times 1 $ random vectors $ w_t \sim N(0,I) $.
- A $ k \times 1 $ vector $ y_t $ of **observations** at time $ t = 0, 1, 2, \ldots $.
- An $ n \times n $ matrix $ A $ called the **transition matrix**.
- An $ n \times m $ matrix $ C $ called the **volatility matrix**.
- A $ k \times n $ matrix $ G $ sometimes called the **output matrix**.

Here is the linear state-space system

$$
\begin{aligned}
x_{t+1} & =  A x_t + C w_{t+1}   \\
y_t &  =  G x_t \nonumber \\
x_0 & \sim N(\mu_0, \Sigma_0) \nonumber
\end{aligned} \tag{21.1}
$$

### Primitives

The primitives of the model are

1. the matrices $ A, C, G $
1. shock distribution, which we have specialized to $ N(0,I) $
1. the distribution of the initial condition $ x_0 $, which we have set to $ N(\mu_0, \Sigma_0) $

Given $ A, C, G $ and draws of $ x_0 $ and $ w_1, w_2, \ldots $, the
model [(21.1)](#equation-st-space-rep) pins down the values of the sequences $ \{x_t\} $ and $ \{y_t\} $.

Even without these draws, the primitives 1–3 pin down the _probability distributions_ of $ \{x_t\} $ and $ \{y_t\} $.

Later we’ll see how to compute these distributions and their moments.

#### Martingale Difference Shocks

We’ve made the common assumption that the shocks are independent standardized normal vectors.

But some of what we say will be valid under the assumption that $ \{w\_{t+1}\} $ is a **martingale difference sequence**.

A martingale difference sequence is a sequence that is zero mean when conditioned on past information.

In the present case, since $ \{x_t\} $ is our state sequence, this means that it satisfies

$$
\mathbb E (w_{t+1} | x_t, x_{t-1}, \ldots ) = 0
$$

This is a weaker condition than that $ \{w*t\} $ is IID with $ w*{t+1} \sim N(0,I) $.

### Examples

By appropriate choice of the primitives, a variety of dynamics can be represented in terms of the linear state space model.

The following examples help to highlight this point.

They also illustrate the wise dictum _finding the state is an art_.

#### Second-order Difference Equation

Let $ \{y_t\} $ be a deterministic sequence that satisfies

$$
y_{t+1} =  \phi_0 + \phi_1 y_t + \phi_2 y_{t-1}
\quad \text{s.t.} \quad
y_0, y_{-1} \text{ given} \tag{21.2}
$$

To map [(21.2)](#equation-st-ex-1) into our state space system [(21.1)](#equation-st-space-rep), we set

$$
x_t=
\begin{bmatrix}
1 \\
y_t \\
y_{t-1}
\end{bmatrix}
\qquad
A = \begin{bmatrix}
1 & 0 & 0 \\
\phi_0 & \phi_1 & \phi_2  \\
0 & 1 & 0
\end{bmatrix}
\qquad
C= \begin{bmatrix}
0 \\
0 \\
0
\end{bmatrix}
\qquad
G = \begin{bmatrix} 0 & 1 & 0 \end{bmatrix}
$$

You can confirm that under these definitions, [(21.1)](#equation-st-space-rep) and [(21.2)](#equation-st-ex-1) agree.

The next figure shows the dynamics of this process when $ \phi*0 = 1.1, \phi_1=0.8, \phi_2 = -0.8, y_0 = y*{-1} = 1 $.

```python
def plot_lss(A,
         C,
         G,
         n=3,
         ts_length=50):

    ar = LinearStateSpace(A, C, G, mu_0=np.ones(n))
    x, y = ar.simulate(ts_length)

    fig, ax = plt.subplots()
    y = y.flatten()
    ax.plot(y, 'b-', lw=2, alpha=0.7)
    ax.grid()
    ax.set_xlabel('time', fontsize=12)
    ax.set_ylabel('$y_t$', fontsize=12)
    plt.show()
```

```python
ϕ_0, ϕ_1, ϕ_2 = 1.1, 0.8, -0.8

A = [[1,     0,     0  ],
     [ϕ_0,   ϕ_1,   ϕ_2],
     [0,     1,     0  ]]

C = np.zeros((3, 1))
G = [0, 1, 0]

plot_lss(A, C, G)
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/21.%20Linear%20State%20Space%20Models_files/21.%20Linear%20State%20Space%20Models_13_0.png)
</div>

Later you’ll be asked to recreate this figure.

#### Univariate Autoregressive Processes

We can use [(21.1)](#equation-st-space-rep) to represent the model

$$
y_{t+1} = \phi_1 y_{t} + \phi_2 y_{t-1} + \phi_3 y_{t-2} + \phi_4  y_{t-3} + \sigma w_{t+1} \tag{21.3}
$$

where $ \{w_t\} $ is IID and standard normal.

To put this in the linear state space format we take $ x*t = \begin{bmatrix} y_t & y*{t-1} & y*{t-2} & y*{t-3} \end{bmatrix}' $ and

$$
A =
\begin{bmatrix}
\phi_1 & \phi_2 & \phi_3 & \phi_4 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{bmatrix}
\qquad
C = \begin{bmatrix}
\sigma \\
0 \\
0 \\
0
\end{bmatrix}
\qquad
G = \begin{bmatrix}
1 & 0  & 0 & 0
\end{bmatrix}
$$

The matrix $ A $ has the form of the _companion matrix_ to the vector
$ \begin{bmatrix}\phi_1 & \phi_2 & \phi_3 & \phi_4 \end{bmatrix} $.

The next figure shows the dynamics of this process when

$$
\phi_1 = 0.5, \phi_2 = -0.2, \phi_3 = 0, \phi_4 = 0.5, \sigma = 0.2, y_0 = y_{-1} = y_{-2} =
y_{-3} = 1
$$

```python
ϕ_1, ϕ_2, ϕ_3, ϕ_4 = 0.5, -0.2, 0, 0.5
σ = 0.2

A_1 = [[ϕ_1,   ϕ_2,   ϕ_3,   ϕ_4],
       [1,     0,     0,     0  ],
       [0,     1,     0,     0  ],
       [0,     0,     1,     0  ]]

C_1 = [[σ],
       [0],
       [0],
       [0]]

G_1 = [1, 0, 0, 0]

plot_lss(A_1, C_1, G_1, n=4, ts_length=200)
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/21.%20Linear%20State%20Space%20Models_files/21.%20Linear%20State%20Space%20Models_16_0.png)
</div>

#### Vector Autoregressions

Now suppose that

- $ y_t $ is a $ k \times 1 $ vector
- $ \phi_j $ is a $ k \times k $ matrix and
- $ w_t $ is $ k \times 1 $

Then [(21.3)](#equation-eq-ar-rep) is termed a _vector autoregression_.

To map this into [(21.1)](#equation-st-space-rep), we set

$$
x_t =
\begin{bmatrix}
y_t \\
y_{t-1} \\
y_{t-2} \\
y_{t-3}
\end{bmatrix}
\quad
A =
\begin{bmatrix}
\phi_1 & \phi_2 & \phi_3 & \phi_4 \\
I & 0        & 0        & 0       \\
0 & I        & 0        & 0       \\
0 & 0        & I        & 0
\end{bmatrix}
\quad
C =
\begin{bmatrix}
\sigma \\
0 \\
0 \\
0
\end{bmatrix}
\quad
G =
\begin{bmatrix}
I & 0 & 0 & 0
\end{bmatrix}
$$

where $ I $ is the $ k \times k $ identity matrix and $ \sigma $ is a $ k \times k $ matrix.

#### Seasonals

We can use [(21.1)](#equation-st-space-rep) to represent

1. the _deterministic seasonal_ $ y*t = y*{t-4} $
1. the _indeterministic seasonal_ $ y*t = \phi_4 y*{t-4} + w_t $

In fact, both are special cases of [(21.3)](#equation-eq-ar-rep).

With the deterministic seasonal, the transition matrix becomes

$$
A = \begin{bmatrix}
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{bmatrix}
$$

It is easy to check that $ A^4 = I $, which implies that $ x_t $ is strictly periodic with period 4:<sup><a href="#foot1" id="foot1-link"></a></sup>

$$
x_{t+4} = x_t
$$

Such an $ x_t $ process can be used to model deterministic seasonals in quarterly time series.

The _indeterministic_ seasonal produces recurrent, but aperiodic, seasonal fluctuations.

#### Time Trends

The model $ y_t = a t + b $ is known as a _linear time trend_.

We can represent this model in the linear state space form by taking

$$
A
= \begin{bmatrix}
1 & 1  \\
0 & 1
\end{bmatrix}
\qquad
C
= \begin{bmatrix}
0 \\
0
\end{bmatrix}
\qquad
G
= \begin{bmatrix}
a & b
\end{bmatrix} \tag{21.4}
$$

and starting at initial condition $ x_0 = \begin{bmatrix} 0 & 1\end{bmatrix}' $.

In fact, it’s possible to use the state-space system to represent polynomial trends of any order.

For instance, we can represent the model $ y_t = a t^2 + bt + c $ in the linear state space form by taking

$$
A
= \begin{bmatrix}
1 & 1 & 0 \\
0 & 1 & 1 \\
0 & 0 & 1
\end{bmatrix}
\qquad
C
= \begin{bmatrix}
0 \\
0 \\
0
\end{bmatrix}
\qquad
G
= \begin{bmatrix}
2a & a + b & c
\end{bmatrix}
$$

and starting at initial condition $ x_0 = \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}' $.

It follows that

$$
A^t =
\begin{bmatrix}
1 & t & t(t-1)/2 \\
0 & 1 & t \\
0 & 0 & 1
\end{bmatrix}
$$

Then $ x_t^\prime = \begin{bmatrix} t(t-1)/2 &t & 1 \end{bmatrix} $. You can now confirm that $ y_t = G x_t $ has the correct form.

### Moving Average Representations

A nonrecursive expression for $ x_t $ as a function of
$ x_0, w_1, w_2, \ldots, w_t $ can be derived by using [(21.1)](#equation-st-space-rep) iteratively to get

$$
\begin{aligned}
x_t & = Ax_{t-1} + Cw_t \\
& = A^2 x_{t-2} + ACw_{t-1} + Cw_t \nonumber \\
& \qquad \vdots \nonumber \\
& = \sum_{j=0}^{t-1} A^j Cw_{t-j}  + A^t x_0 \nonumber
\end{aligned} \tag{21.5}
$$

This form [(21.5)](#equation-eqob5) is known as a _moving average_ representation.

It represents $ \{x_t\} $ as a linear function of

1. current and past values of the process $ \{w_t\} $ and
1. the initial condition $ x_0 $

For instance, consider a moving average representation where the model is

$$
A
= \begin{bmatrix}
1 & 1  \\
0 & 1
\end{bmatrix}
\qquad
C
= \begin{bmatrix}
1 \\
0
\end{bmatrix}
$$

You can demonstrate that $ A^t = \begin{bmatrix} 1 & t \cr 0 & 1 \end{bmatrix} $ and $ A^j C = \begin{bmatrix} 1 & 0 \end{bmatrix}' $.

Substituting into the moving average representation [(21.5)](#equation-eqob5), we have

$$
x_{1t} = \sum_{j=0}^{t-1} w_{t-j} +
\begin{bmatrix}
1 & t
\end{bmatrix}
x_0
$$

where $ x\_{1t} $ is the first entry of $ x_t $.

The first term on the right is a cumulative sum of martingale differences and is thus a [martingale](https://en.wikipedia.org/wiki/Martingale_%28probability_theory%29).

The second term is a linear function of time with a shift.

Therefore, $ x\_{1t} $ is termed a _martingale with drift_.

## Distributions and Moments

### Unconditional Moments

Using [(21.1)](#equation-st-space-rep), we can easily derive expressions for the
(unconditional) means of $ x_t $ and $ y_t $.

We will soon clarify what _unconditional_ and _conditional_ signify.

Defining $ \mu_t := \mathbb E (x_t) $ and utilizing the linearity of expectations, we find

$$
\mu_{t+1} = A \mu_t
\quad \text{with} \quad \mu_0 \text{ given} \tag{21.6}
$$

Here $ \mu_0 $ is a primitive given in [(21.1)](#equation-st-space-rep).

The variance-covariance matrix of $ x_t $ is $ \Sigma_t := \mathbb E ( (x_t - \mu_t) (x_t - \mu_t)') $.

Using $ x*{t+1} - \mu*{t+1} = A (x*t - \mu_t) + C w*{t+1} $, we can
compute this matrix recursively via

$$
\Sigma_{t+1}  = A \Sigma_t A' + C C'
\quad \text{with} \quad \Sigma_0 \text{ given} \tag{21.7}
$$

As with $ \mu_0 $, the matrix $ \Sigma_0 $ is a primitive given in [(21.1)](#equation-st-space-rep).

In terms of terminology, we may sometimes refer to

- $ \mu_t $ as the _unconditional mean_ of $ x_t $
- $ \Sigma_t $ as the _unconditional variance-covariance matrix_ of $ x_t $

This is to differentiate $ \mu_t $ and $ \Sigma_t $ from related entities that use conditioning
information, to be defined later.

However, you should be aware that these “unconditional” moments do depend on
the initial distribution $ N(\mu_0, \Sigma_0) $.

#### Moments of the Observables

Using linearity of expectations once more, we have

$$
\mathbb E (y_t) = \mathbb E (G x_t) = G \mu_t \tag{21.8}
$$

The variance-covariance matrix of $ y_t $ is shown to be

$$
\textrm{Var} (y_t) = \textrm{Var} (G x_t) = G \Sigma_t G' \tag{21.9}
$$

### Distributions

In general, knowing the mean and variance-covariance matrix of a random vector
is not as informative as knowing the full distribution.

However, there are scenarios where these moments alone provide all the necessary information.

These are cases where the mean vector and covariance matrix are the sole **parameters** that define the population distribution.

One such scenario is when the vector is Gaussian (i.e., normally
distributed).

This holds here, given

1. our Gaussian assumptions on the primitives
1. the fact that normality is maintained under linear operations

In fact, it’s [well-known](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Affine_transformation) that

$$
u \sim N(\bar u, S)
\quad \text{and} \quad
v = a + B u
\implies
v \sim N(a + B \bar u, B S B') \tag{21.10}
$$

In particular, given our Gaussian assumptions on the primitives and the
linearity of [(21.1)](#equation-st-space-rep), we can immediately see that both $ x_t $ and
$ y_t $ are Gaussian for all $ t \geq 0 $ <sup><a href="#fn-ag" id="fn-ag-link"></a></sup>.

Since $ x_t $ is Gaussian, to find the distribution, we only need to
find its mean and variance-covariance matrix.

But in fact, we’ve already done this, in [(21.6)](#equation-lss-mut-linear-models) and [(21.7)](#equation-eqsigmalaw-linear-models).

Letting $ \mu_t $ and $ \Sigma_t $ be as defined by these equations,
we have

$$
x_t \sim N(\mu_t, \Sigma_t) \tag{21.11}
$$

By similar reasoning combined with [(21.8)](#equation-lss-umy) and [(21.9)](#equation-lss-uvy),

$$
y_t \sim N(G \mu_t, G \Sigma_t G') \tag{21.12}
$$

### Ensemble Interpretations

How should we interpret the distributions defined by [(21.11)](#equation-lss-mgs-x)–[(21.12)](#equation-lss-mgs-y)?

Intuitively, the probabilities in a distribution correspond to relative frequencies in a large population drawn from that distribution.

Let’s apply this concept to our setting, focusing on the distribution of $ y_T $ for fixed $ T $.

We can generate independent draws of $ y_T $ by repeatedly simulating the
evolution of the system up to time $ T $, using an independent set of
shocks each time.

The next figure shows 20 simulations, producing 20 time series for $ \{y_t\} $, and hence 20 draws of $ y_T $.

The system in question is the univariate autoregressive model [(21.3)](#equation-eq-ar-rep).

The values of $ y_T $ are represented by black dots in the left-hand figure

```python
def cross_section_plot(A,
                   C,
                   G,
                   T=20,                 # Set the time
                   ymin=-0.8,
                   ymax=1.25,
                   sample_size = 20,     # 20 observations/simulations
                   n=4):                 # The number of dimensions for the initial x0

    ar = LinearStateSpace(A, C, G, mu_0=np.ones(n))

    fig, axes = plt.subplots(1, 2, figsize=(16, 5))

    for ax in axes:
        ax.grid(alpha=0.4)
        ax.set_ylim(ymin, ymax)

    ax = axes[0]
    ax.set_ylim(ymin, ymax)
    ax.set_ylabel('$y_t$', fontsize=12)
    ax.set_xlabel('time', fontsize=12)
    ax.vlines((T,), -1.5, 1.5)

    ax.set_xticks((T,))
    ax.set_xticklabels(('$T$',))

    sample = []
    for i in range(sample_size):
        rcolor = random.choice(('c', 'g', 'b', 'k'))
        x, y = ar.simulate(ts_length=T+15)
        y = y.flatten()
        ax.plot(y, color=rcolor, lw=1, alpha=0.5)
        ax.plot((T,), (y[T],), 'ko', alpha=0.5)
        sample.append(y[T])

    y = y.flatten()
    axes[1].set_ylim(ymin, ymax)
    axes[1].set_ylabel('$y_t$', fontsize=12)
    axes[1].set_xlabel('relative frequency', fontsize=12)
    axes[1].hist(sample, bins=16, density=True, orientation='horizontal', alpha=0.5)
    plt.show()
```

```python
ϕ_1, ϕ_2, ϕ_3, ϕ_4 = 0.5, -0.2, 0, 0.5
σ = 0.1

A_2 = [[ϕ_1, ϕ_2, ϕ_3, ϕ_4],
       [1,     0,     0,     0],
       [0,     1,     0,     0],
       [0,     0,     1,     0]]

C_2 = [[σ], [0], [0], [0]]

G_2 = [1, 0, 0, 0]

cross_section_plot(A_2, C_2, G_2)
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/21.%20Linear%20State%20Space%20Models_files/21.%20Linear%20State%20Space%20Models_27_0.png)
</div>

In the right-hand figure, these values are transformed into a rotated histogram
that displays relative frequencies from our sample of 20 $ y_T $’s.

Here is another figure, this time with 100 observations

```python
t = 100
cross_section_plot(A_2, C_2, G_2, T=t)
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/21.%20Linear%20State%20Space%20Models_files/21.%20Linear%20State%20Space%20Models_29_0.png)
</div>

Let’s now attempt with 500,000 observations, showing only the histogram (without rotation)

```python
T = 100
ymin=-0.8
ymax=1.25
sample_size = 500_000

ar = LinearStateSpace(A_2, C_2, G_2, mu_0=np.ones(4))
fig, ax = plt.subplots()
x, y = ar.simulate(sample_size)
mu_x, mu_y, Sigma_x, Sigma_y, Sigma_yx = ar.stationary_distributions()
f_y = norm(loc=float(mu_y), scale=float(np.sqrt(Sigma_y)))
y = y.flatten()
ygrid = np.linspace(ymin, ymax, 150)

ax.hist(y, bins=50, density=True, alpha=0.4)
ax.plot(ygrid, f_y.pdf(ygrid), 'k-', lw=2, alpha=0.8, label=r'true density')
ax.set_xlim(ymin, ymax)
ax.set_xlabel('$y_t$', fontsize=12)
ax.set_ylabel('relative frequency', fontsize=12)
ax.legend(fontsize=12)
plt.show()
```

    /var/folders/zj/sz5g50055l9gj8nhh_6md1cw0000gn/T/ipykernel_31167/1034809053.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
      f_y = norm(loc=float(mu_y), scale=float(np.sqrt(Sigma_y)))

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/21.%20Linear%20State%20Space%20Models_files/21.%20Linear%20State%20Space%20Models_31_1.png)
</div>

The black line is the population density of $ y_T $ calculated from [(21.12)](#equation-lss-mgs-y).

The histogram and population distribution are close, as anticipated.

By examining the figures and experimenting with parameters, you will gain a
sense of how the population distribution depends on the model primitives [listed above](#lss-pgs), as intermediated by the distribution’s parameters.

#### Ensemble Means

In the preceding figure, we approximated the population distribution of $ y_T $ by

1. generating $ I $ sample paths (i.e., time series) where $ I $ is a large number
2. recording each observation $ y^i_T $
3. creating a histogram of this sample

Just as the histogram approximates the population distribution, the _ensemble_ or
_cross-sectional average_

$$
\bar y_T := \frac{1}{I} \sum_{i=1}^I y_T^i
$$

approximates the expectation $ \mathbb E (y_T) = G \mu_T $ (as implied by the law of large numbers).

Here’s a simulation comparing the ensemble averages and population means at time points $ t=0,\ldots,50 $.

The parameters are the same as for the preceding figures,
and the sample size is relatively small ($ I=20 $).

```python
I = 20
T = 50
ymin = -0.5
ymax = 1.15

ar = LinearStateSpace(A_2, C_2, G_2, mu_0=np.ones(4))

fig, ax = plt.subplots()

ensemble_mean = np.zeros(T)
for i in range(I):
    x, y = ar.simulate(ts_length=T)
    y = y.flatten()
    ax.plot(y, 'c-', lw=0.8, alpha=0.5)
    ensemble_mean = ensemble_mean + y

ensemble_mean = ensemble_mean / I
ax.plot(ensemble_mean, color='b', lw=2, alpha=0.8, label='$\\bar y_t$')
m = ar.moment_sequence()

population_means = []
for t in range(T):
    μ_x, μ_y, Σ_x, Σ_y = next(m)
    population_means.append(float(μ_y))

ax.plot(population_means, color='g', lw=2, alpha=0.8, label='$G\mu_t$')
ax.set_ylim(ymin, ymax)
ax.set_xlabel('time', fontsize=12)
ax.set_ylabel('$y_t$', fontsize=12)
ax.legend(ncol=2)
plt.show()
```

    /var/folders/zj/sz5g50055l9gj8nhh_6md1cw0000gn/T/ipykernel_31167/3206934063.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
      population_means.append(float(μ_y))

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/21.%20Linear%20State%20Space%20Models_files/21.%20Linear%20State%20Space%20Models_34_1.png)
</div>

The ensemble mean for $ x_t $ is

$$
\bar x_T := \frac{1}{I} \sum_{i=1}^I x_T^i \to \mu_T
\qquad (I \to \infty)
$$

The limit $ \mu_T $ is a “long-run average”.

(By _long-run average_ we mean the average for an infinite ($ I = \infty $) number of sample $ x_T $’s)

Another application of the law of large numbers assures us that

$$
\frac{1}{I} \sum_{i=1}^I (x_T^i - \bar x_T) (x_T^i - \bar x_T)' \to \Sigma_T
\qquad (I \to \infty)
$$

### Joint Distributions

In the preceding discussion, we examined the distributions of $ x_t $ and
$ y_t $ individually.

This provides useful information but doesn’t allow us to answer questions like

- what’s the probability that $ x_t \geq 0 $ for all $ t $?
- what’s the probability that the process $ \{y_t\} $ exceeds some value $ a $ before falling below $ b $?
- etc., etc.

Such questions concern the _joint distributions_ of these sequences.

To compute the joint distribution of $ x_0, x_1, \ldots, x_T $, recall
that joint and conditional densities are linked by the rule

$$
p(x, y) = p(y \, | \, x) p(x)
\qquad \text{(joint }=\text{ conditional }\times\text{ marginal)}
$$

From this rule, we get $ p(x_0, x_1) = p(x_1 \,|\, x_0) p(x_0) $.

The Markov property $ p(x*t \,|\, x*{t-1}, \ldots, x*0) = p(x_t \,|\, x*{t-1}) $ and repeated applications of the preceding rule lead us to

$$
p(x_0, x_1, \ldots, x_T) = p(x_0) \prod_{t=0}^{T-1} p(x_{t+1} \,|\, x_t)
$$

The marginal $ p(x_0) $ is just the primitive $ N(\mu_0, \Sigma_0) $.

In view of [(21.1)](#equation-st-space-rep), the conditional densities are

$$
p(x_{t+1} \,|\, x_t) = N(Ax_t, C C')
$$

#### Autocovariance Functions

An important object related to the joint distribution is the _autocovariance function_

$$
\Sigma_{t+j, t} := \mathbb E ( (x_{t+j} - \mu_{t+j})(x_t - \mu_t)' ) \tag{21.13}
$$

Elementary calculations show that

$$
\Sigma_{t+j,t} = A^j \Sigma_t \tag{21.14}
$$

Notice that $ \Sigma\_{t+j,t} $ generally depends on both $ j $, the gap between the two dates, and $ t $, the earlier date.

## Stationarity and Ergodicity

Stationarity and ergodicity are two properties that, when they hold, greatly aid analysis of linear state space models.

Let’s start with the intuition.

### Visualizing Stability

Let’s look at some more time series from the same model that we analyzed above.

This picture shows cross-sectional distributions for $ y $ at times
$ T, T', T'' $

```python
def cross_plot(A,
            C,
            G,
            steady_state='False',
            T0 = 10,
            T1 = 50,
            T2 = 75,
            T4 = 100):

    ar = LinearStateSpace(A, C, G, mu_0=np.ones(4))

    if steady_state == 'True':
        μ_x, μ_y, Σ_x, Σ_y, Σ_yx = ar.stationary_distributions()
        ar_state = LinearStateSpace(A, C, G, mu_0=μ_x, Sigma_0=Σ_x)

    ymin, ymax = -0.6, 0.6
    fig, ax = plt.subplots()
    ax.grid(alpha=0.4)
    ax.set_ylim(ymin, ymax)
    ax.set_ylabel('$y_t$', fontsize=12)
    ax.set_xlabel('$time$', fontsize=12)

    ax.vlines((T0, T1, T2), -1.5, 1.5)
    ax.set_xticks((T0, T1, T2))
    ax.set_xticklabels(("$T$", "$T'$", "$T''$"), fontsize=12)
    for i in range(80):
        rcolor = random.choice(('c', 'g', 'b'))

        if steady_state == 'True':
            x, y = ar_state.simulate(ts_length=T4)
        else:
            x, y = ar.simulate(ts_length=T4)

        y = y.flatten()
        ax.plot(y, color=rcolor, lw=0.8, alpha=0.5)
        ax.plot((T0, T1, T2), (y[T0], y[T1], y[T2],), 'ko', alpha=0.5)
    plt.show()
```

```python
cross_plot(A_2, C_2, G_2)
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/21.%20Linear%20State%20Space%20Models_files/21.%20Linear%20State%20Space%20Models_41_0.png)
</div>

Note how the time series “settle down” in the sense that the distributions at
$ T' $ and $ T'' $ are relatively similar to each other — but unlike
the distribution at $ T $.

Apparently, the distributions of $ y_t $ converge to a fixed long-run
distribution as $ t \to \infty $.

When such a distribution exists, it is called a _stationary distribution_.

### Stationary Distributions

In our context, a distribution $ \psi\_{\infty} $ is said to be _stationary_ for $ x_t $ if

$$
x_t \sim \psi_{\infty}
\quad \text{and} \quad
x_{t+1} = A x_t + C w_{t+1}
\quad \implies \quad
x_{t+1} \sim \psi_{\infty}
$$

Since

1. in the present case, all distributions are Gaussian
1. a Gaussian distribution is determined by its mean and variance-covariance matrix

we can restate the definition as follows: $ \psi\_{\infty} $ is stationary for $ x_t $ if

$$
\psi_{\infty}
= N(\mu_{\infty}, \Sigma_{\infty})
$$

where $ \mu*{\infty} $ and $ \Sigma*{\infty} $ are fixed points of [(21.6)](#equation-lss-mut-linear-models) and [(21.7)](#equation-eqsigmalaw-linear-models) respectively.

### Covariance Stationary Processes

Let’s observe what happens to the preceding figure if we start $ x_0 $ at the stationary distribution.

```python
cross_plot(A_2, C_2, G_2, steady_state='True')
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/21.%20Linear%20State%20Space%20Models_files/21.%20Linear%20State%20Space%20Models_45_0.png)
</div>

Now the differences in the observed distributions at $ T, T' $ and $ T'' $ come entirely from random fluctuations due to the finite sample size.

By

- our choosing $ x*0 \sim N(\mu*{\infty}, \Sigma\_{\infty}) $
- the definitions of $ \mu*{\infty} $ and $ \Sigma*{\infty} $ as fixed points of [(21.6)](#equation-lss-mut-linear-models) and [(21.7)](#equation-eqsigmalaw-linear-models) respectively

we've ensured that

$$
\mu_t = \mu_{\infty}
\quad \text{and} \quad
\Sigma_t = \Sigma_{\infty}
\quad \text{for all } t
$$

Moreover, in view of [(21.14)](#equation-eqnautocov), the autocovariance function takes the form $ \Sigma*{t+j,t} = A^j \Sigma*\infty $, which depends on $ j $ but not on $ t $.

This motivates the following definition.

A process $ \{x_t\} $ is said to be _covariance stationary_ if

- both $ \mu_t $ and $ \Sigma_t $ are constant in $ t $
- $ \Sigma\_{t+j,t} $ depends on the time gap $ j $ but not on time $ t $

In our setting, $ \{x*t\} $ will be covariance stationary if $ \mu_0, \Sigma_0, A, C $ assume values that imply that none of $ \mu_t, \Sigma_t, \Sigma*{t+j,t} $ depends on $ t $.

### Conditions for Stationarity

#### The Globally Stable Case

The difference equation $ \mu*{t+1} = A \mu_t $ is known to have *unique*
fixed point $ \mu*{\infty} = 0 $ if all eigenvalues of $ A $ have moduli strictly less than unity.

That is, if `(np.absolute(np.linalg.eigvals(A)) < 1).all() == True`.

The difference equation [(21.7)](#equation-eqsigmalaw-linear-models) also has a unique fixed point in this case, and, moreover

$$
\mu_t \to \mu_{\infty} = 0
\quad \text{and} \quad
\Sigma_t \to \Sigma_{\infty}
\quad \text{as} \quad t \to \infty
$$

regardless of the initial conditions $ \mu_0 $ and $ \Sigma_0 $.

This is the _globally stable case_.

However, global stability is more than we need for stationary solutions, and often more than we want.

To illustrate, consider [our second order difference equation example](#lss-sode).

Here the state is $ x*t = \begin{bmatrix} 1 & y_t & y*{t-1} \end{bmatrix}' $.

Because of the constant first component in the state vector, we will never have $ \mu_t \to 0 $.

How can we find stationary solutions that respect a constant state component?

#### Processes with a Constant State Component

To investigate such a process, suppose that $ A $ and $ C $ take the
form

$$
A
= \begin{bmatrix}
A_1 & a \\
0 & 1
\end{bmatrix}
\qquad
C = \begin{bmatrix}
C_1 \\
0
\end{bmatrix}
$$

where

- $ A_1 $ is an $ (n-1) \times (n-1) $ matrix
- $ a $ is an $ (n-1) \times 1 $ column vector

Let $ x*t = \begin{bmatrix} x*{1t}' & 1 \end{bmatrix}' $ where $ x\_{1t} $ is $ (n-1) \times 1 $.

It follows that

$$
\begin{aligned}
x_{1,t+1} & = A_1 x_{1t} + a + C_1 w_{t+1}
\end{aligned}
$$

Let $ \mu*{1t} = \mathbb E (x*{1t}) $ and take expectations on both sides of this expression to get

$$
\mu_{1,t+1} = A_1 \mu_{1,t} + a \tag{21.15}
$$

Assume now that the moduli of the eigenvalues of $ A_1 $ are all strictly less than one.

Then [(21.15)](#equation-eqob29) has a unique stationary solution, namely,

$$
\mu_{1\infty} = (I-A_1)^{-1} a
$$

The stationary value of $ \mu*t $ itself is then $ \mu*\infty := \begin{bmatrix}
\mu\_{1\infty}' & 1 \end{bmatrix}' $.

The stationary values of $ \Sigma*t $ and $ \Sigma*{t+j,t} $ satisfy

$$
\begin{aligned}
\Sigma_\infty & = A \Sigma_\infty A' + C C' \\
\Sigma_{t+j,t} & = A^j \Sigma_\infty \nonumber
\end{aligned} \tag{21.16}
$$

Notice that here $ \Sigma\_{t+j,t} $ depends on the time gap $ j $ but not on calendar time $ t $.

In conclusion, if

- $ x*0 \sim N(\mu*{\infty}, \Sigma\_{\infty}) $ and
- the moduli of the eigenvalues of $ A_1 $ are all strictly less than unity

then the $ \{x_t\} $ process is covariance stationary, with constant state
component.

> **Note**If the eigenvalues of $ A_1 $ are less than unity in modulus, then
> (a) starting from any initial value, the mean and variance-covariance
> matrix both converge to their stationary values; and (b)
> iterations on [(21.7)](#equation-eqsigmalaw-linear-models) converge to the fixed point of the _discrete
> Lyapunov equation_ in the first line of [(21.16)](#equation-eqnsigmainf).

### Ergodicity

Let's suppose that we're working with a covariance stationary process.

In this case, we know that the ensemble mean will converge to $ \mu\_{\infty} $ as the sample size $ I $ approaches infinity.

#### Averages over Time

Ensemble averages across simulations are interesting theoretically, but in real life, we usually observe only a _single_ realization $ \{x*t, y_t\}*{t=0}^T $.

So now let's take a single realization and form the time-series averages

$$
\bar x := \frac{1}{T} \sum_{t=1}^T x_t
\quad \text{and} \quad
\bar y := \frac{1}{T} \sum_{t=1}^T y_t
$$

Do these time series averages converge to something interpretable in terms of our basic state-space representation?

The answer depends on something called _ergodicity_.

Ergodicity is the property that time series and ensemble averages coincide.

More formally, ergodicity implies that time series sample averages converge to their
expectation under the stationary distribution.

In particular,

- $ \frac{1}{T} \sum*{t=1}^T x_t \to \mu*{\infty} $
- $ \frac{1}{T} \sum*{t=1}^T (x_t -\bar x_T) (x_t - \bar x_T)' \to \Sigma*\infty $
- $ \frac{1}{T} \sum*{t=1}^T (x*{t+j} -\bar x*T) (x_t - \bar x_T)' \to A^j \Sigma*\infty $

In our linear Gaussian setting, any covariance stationary process is also ergodic.

### Forecasting

In this section, we explore how to project future values of the state vector \( x*t \) using previous observations. The goal of forecasting is to predict future trajectories of a process based on historical behavior. We aim to determine expected values of future states, conditional on present and past observations, represented as \( \mathbb E (x*{t+h} | \Omega_t) \), where \( \Omega_t \) signifies the information set available at time \( t \).

#### Iterative Forecasting

Forecasts can be derived iteratively. This method necessitates the computation of one-step-ahead forecasts, known as \( \hat{x}_{t+1|t} \), which can then be utilized to derive multi-step-ahead forecasts. For example, to predict \( h \) steps into the future, forecasts can be derived sequentially: \( \hat{x}_{t+h|t} = f(\hat{x}\_{t+h-1|t}) \), continuing until the desired forecasting horizon is attained.

#### Multi-step Forecasting

Multi-step forecasting entails simultaneously predicting multiple periods ahead. The core challenge is the uncertainty tied to forecasts of earlier periods, as these influence subsequent predictions. A prevalent method involves employing an iterative approach where each step heavily relies on the prior forecast.

### Observational Equivalence

Observational equivalence arises when two distinct processes yield identical observable results. This indicates that, although disparate models might produce similar forecasts, they could stem from different fundamental structures. This concept is crucial in model selection and policy assessment, as seemingly equivalent models can suggest varying economic interpretations.

### Structural Estimation

Structural estimation refers to the technique of estimating model parameters based on theoretical constraints derived from the economic framework that governs the phenomena under study. This process entails specifying a model, deriving its structural equations, and comparing predicted outcomes with actual data to identify the most credible parameter estimates.

### Dynamics of Linear Models

The dynamics of linear models can be examined through their impulse response functions (IRFs), which illustrate the response of variables to shocks over time. Understanding this response is vital for economic policy, as it sheds light on the effectiveness and temporal adjustment of interventions. IRFs demonstrate both the size and the duration of shocks' impacts.

### Conclusion

In conclusion, linear state space models provide a powerful framework for comprehending dynamic systems. Their applications span numerous fields, including economics, finance, and engineering. Through a thorough understanding of forecasting, observational equivalence, and structural estimation, researchers can utilize these models to obtain valuable insights into time-dependent phenomena.

## Solution to Exercise 21.1

Suppose that every eigenvalue of $ A $ has a modulus strictly less than $ \frac{1}{\beta} $.

It then follows that $ I + \beta A + \beta^2 A^2 + \cdots = \left[I - \beta A \right]^{-1} $.

This leads to our formulas:

- Forecast of a geometric sum of future $ x $’s

$$
\mathbb E_t \left[\sum_{j=0}^\infty \beta^j x_{t+j} \right]
= [I + \beta A + \beta^2 A^2 + \cdots \ ] x_t = [I - \beta A]^{-1} x_t
$$

- Forecast of a geometric sum of future $ y $’s

$$
\mathbb E_t \left[\sum_{j=0}^\infty \beta^j y_{t+j} \right]
= G [I + \beta A + \beta^2 A^2 + \cdots \ ] x_t
= G[I - \beta A]^{-1} x_t
$$

**[1]** The eigenvalues of $ A $ are $ (1,-1, i,-i) $.

**[2]** The correct way to argue this is by induction. Suppose that
$ x*t $ is Gaussian. Then [(21.1)](#equation-st-space-rep) and
[(21.10)](#equation-lss-glig) imply that $ x*{t+1} $ is Gaussian. Since $ x_0 $
is assumed to be Gaussian, it follows that every $ x_t $ is Gaussian.
Evidently, this implies that each $ y_t $ is Gaussian.
