---
title: VARs and DMDs
draft: false
summary: test
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

# Vector Autoregressions and Dynamic Mode Decompositions

This lecture applies computational techniques from the **Singular Value Decomposition** lecture to:

- First-order vector autoregressions (VARs)
- Dynamic mode decompositions (DMDs)
- The relationships between DMDs and first-order VARs

## First-Order Vector Autoregressions

Our goal is to fit a **first-order vector autoregression**:

$$
X_{t+1} = A X_t + C \epsilon_{t+1}, \quad \epsilon_{t+1} \perp X_t \tag{6.1}
$$

Here, $\epsilon_{t+1}$ is the $t+1$ component of a sequence of independent, identically distributed $m \times 1$ random vectors with zero mean and identity covariance matrix. The $m \times 1$ vector $X_t$ is defined as:

$$
X_t = \begin{bmatrix} X_{1,t} & X_{2,t} & \cdots & X_{m,t} \end{bmatrix}^\top \tag{6.2}
$$

where $\cdot ^\top$ denotes complex transposition and $X_{i,t}$ represents variable $i$ at time $t$.

Our data is organized in an $m \times (n+1)$ matrix $\tilde X$:

$$
\tilde X = \begin{bmatrix} X_1 \mid X_2 \mid \cdots \mid X_n \mid X_{n+1} \end{bmatrix}
$$

We aim to estimate a system [(6.1)](#equation-eq-varfirstorder) consisting of $m$ least squares regressions of all variables on one lagged value of all variables.

We proceed by forming two $m \times n$ matrices from $\tilde X$:

$$
X = \begin{bmatrix} X_1 \mid X_2 \mid \cdots \mid X_{n}\end{bmatrix}
$$

and

$$
X' = \begin{bmatrix} X_2 \mid X_3 \mid \cdots \mid X_{n+1}\end{bmatrix}
$$

Here, $'$ is part of the matrix name $X'$ and doesn't indicate transposition.

We denote the rank of $X$ as $p \leq \min(m, n)$.

Two cases of interest are:

1. $n >> m$: Many more time series observations $n$ than variables $m$
2. $m >> n$: Many more variables $m$ than time series observations $n$

A common formula describes the least squares estimator $\hat A$ of $A$ in both cases:

$$
\hat A = X' X^+ \tag{6.3}
$$

where $X^+$ is the pseudo-inverse of $X$.

For more on the **Moore-Penrose pseudo-inverse**, see [Moore-Penrose pseudo-inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse)

The formulas for the pseudo-inverse differ in our two cases:

**Case 1: Short-Fat ($n >> m$)**

When $X$ has linearly independent rows, $X X^\top$ is invertible and the pseudo-inverse $X^+$ is:

$$
X^+ = X^\top (X X^\top )^{-1}
$$

This $X^+$ is a **right-inverse** satisfying $X X^+ = I_{m \times m}$.

The least-squares estimator becomes:

$$
\hat A = X' X^\top (X X^\top )^{-1} \tag{6.4}
$$

This formula is widely used in econometrics for estimating vector autoregressions.

**Case 2: Tall-Skinny ($m >> n$)**

When $X$ has linearly independent columns, $X^\top X$ is invertible and the pseudo-inverse $X^+$ is:

$$
X^+ = (X^\top X)^{-1} X^\top
$$

This $X^+$ is a **left-inverse** satisfying $X^+ X = I_{n \times n}$.

The least-squares estimator becomes:

$$
\hat A = X' (X^\top X)^{-1} X^\top \tag{6.5}
$$

In this case, the regression equation fits perfectly: $\hat A X = X'$.

This is typical in an **underdetermined least-squares** model.

To efficiently calculate the pseudo-inverse $X^+$, we use a singular value decomposition:

$$
X = U \Sigma V^\top \tag{6.8}
$$

For a **reduced** SVD, $X$ is an $m \times n$ data matrix, $U$ is $m \times p$, $\Sigma$ is $p \times p$, and $V$ is $n \times p$.

We can efficiently construct the pseudo-inverse $X^+$ as:

$$
X^{+} = V \Sigma^{-1} U^\top \tag{6.10}
$$

where $\Sigma^{-1}$ is formed by replacing each non-zero element of $\Sigma$ with $\sigma_j^{-1}$.

Our estimator $\hat A = X' X^+$ of the $m \times m$ coefficient matrix $A$ is then:

$$
\hat A = X' V \Sigma^{-1} U^\top \tag{6.11}
$$

## Dynamic Mode Decomposition (DMD)

We now focus on the $m >> n$ **tall and skinny** case associated with **Dynamic Mode Decomposition**.

Here, an $m \times n+1$ data matrix $\tilde X$ contains many more attributes (or variables) $m$ than time periods $n+1$.

Dynamic mode decomposition was introduced by [[Schmid, 2010](/courses/Introduction-to-Quantitative-Economics/References#id19)].

For more on Dynamic Mode Decomposition, see [[Kutz _et al._, 2016](/courses/Introduction-to-Quantitative-Economics/References#id42)] and [[Brunton and Kutz, 2019](/courses/Introduction-to-Quantitative-Economics/References#id258)] (section 7.2).

**Dynamic Mode Decomposition** (DMD) computes a rank $r < p$ approximation to the least squares regression coefficients $\hat A$ described by formula [(6.11)](#equation-eq-ahatsvdformula).

We'll present three alternative representations of our first-order linear dynamic system (vector autoregression).

**Note on representations:** In practice, we'll mainly use Representation 3. The first two representations provide useful intermediate steps to understand the mechanics of Representation 3.

In applications, we'll use only a small subset of **DMD modes** to approximate dynamics, constructing a reduced-rank approximation to $A$.

**For impatient readers:** Our applications will use Representation 3. You might want to skip the preliminary representations 1 and 2 on first reading.

## Representation 1

In this representation, we use a **full** SVD of $X$.

We use the $m$ **columns** of $U$, and thus the $m$ **rows** of $U^\top$, to define a $m \times 1$ vector $\tilde b_t$ as:

$$
\tilde b_t = U^\top X_t . \tag{6.12}
$$

The original data $X_t$ can be represented as:

$$
X_t = U \tilde b_t \tag{6.13}
$$

(We use $b$ to indicate we're creating a **basis** vector.)

Since we're using a **full** SVD, $U U^\top = I_{m \times m}$.

Thus, from equation [(6.12)](#equation-eq-tildexdef2), we can reconstruct $X_t$ from $\tilde b_t$.

Specifically:

- Equation [(6.12)](#equation-eq-tildexdef2) is an **encoder** that **rotates** the $m \times 1$ vector $X_t$ to become an $m \times 1$ vector $\tilde b_t$
- Equation [(6.13)](#equation-eq-xdecoder) is a **decoder** that **reconstructs** the $m \times 1$ vector $X_t$ by rotating the $m \times 1$ vector $\tilde b_t$

We define a transition matrix for an $m \times 1$ basis vector $\tilde b_t$ by:

$$
\tilde A = U^\top \hat A U \tag{6.14}
$$

We can recover $\hat A$ from:

$$
\hat A = U \tilde A U^\top
$$

The dynamics of the $m \times 1$ basis vector $\tilde b_t$ are governed by:

$$
\tilde b_{t+1} = \tilde A \tilde b_t
$$

To forecast future values of $X_t$ conditional on $X_1$, we apply decoders to both sides of this equation:

$$
\overline X_{t+1} = U \tilde A^t U^\top X_1
$$

where $\overline X_{t+1}, t \geq 1$ denotes a forecast.

## Representation 2

This representation is linked to one initially proposed by [[Schmid, 2010](/courses/Introduction-to-Quantitative-Economics/References#id19)].

It can be seen as a step towards achieving the related representation 3, which will be introduced later.

As with Representation 1, we continue to use a **full** SVD instead of a reduced SVD.

As noted in the **Singular Value Decomposition** lecture:

- (a) For a full SVD, $U U^\top = I_{m \times m}$ and $U^\top U = I_{p \times p}$ are identity matrices
- (b) For a reduced SVD of $X$, $U^\top U$ is not an identity matrix

A full SVD is too limiting for our ultimate aim of handling cases where $U^\top U$ is **not** an identity matrix due to using a reduced SVD of $X$.

For now, let's proceed assuming we're using a full SVD, satisfying conditions (a) and (b).

We perform an eigendecomposition of the $m \times m$ matrix $\tilde A = U^\top \hat A U$ defined in equation [(6.14)](#equation-eq-atilde0):

$$
\tilde A = W \Lambda W^{-1} \tag{6.15}
$$

where $\Lambda$ is a diagonal matrix of eigenvalues and $W$ is an $m \times m$ matrix whose columns are eigenvectors corresponding to rows (eigenvalues) in $\Lambda$.

When $U U^\top = I_{m \times m}$, as is true with a full SVD of $X$, it follows that:

$$
\hat A = U \tilde A U^\top = U W \Lambda W^{-1} U^\top \tag{6.16}
$$

According to equation [(6.16)](#equation-eq-eqeigahat), the diagonal matrix $\Lambda$ contains eigenvalues of $\hat A$ and corresponding eigenvectors of $\hat A$ are columns of the matrix $UW$.

The systematic (non-random) parts of the $X_t$ dynamics captured by our first-order vector autoregressions are described by:

$$
X_{t+1} = U W \Lambda W^{-1} U^\top X_t
$$

Multiplying both sides by $W^{-1} U^\top$ gives:

$$
W^{-1} U^\top X_{t+1} = \Lambda W^{-1} U^\top X_t
$$

or

$$
\hat b_{t+1} = \Lambda \hat b_t
$$

where our **encoder** is:

$$
\hat b_t = W^{-1} U^\top X_t
$$

and our **decoder** is:

$$
X_t = U W \hat b_t
$$

We can use this representation to construct a predictor $\overline X_{t+1}$ of $X_{t+1}$ conditional on $X_1$ via:

$$
\overline X_{t+1} = U W \Lambda^t W^{-1} U^\top X_1 \tag{6.17}
$$

Effectively, [[Schmid, 2010](/courses/Introduction-to-Quantitative-Economics/References#id19)] defined an $m \times m$ matrix $\Phi_s$ as:

$$
\Phi_s = UW \tag{6.18}
$$

and a generalized inverse:

$$
\Phi_s^+ = W^{-1}U^\top \tag{6.19}
$$

[[Schmid, 2010](/courses/Introduction-to-Quantitative-Economics/References#id19)] then represented equation [(6.17)](#equation-eq-dssebookrepr) as:

$$
\overline X_{t+1} = \Phi_s \Lambda^t \Phi_s^+ X_1 \tag{6.20}
$$

Components of the basis vector $\hat b_t = W^{-1} U^\top X_t \equiv \Phi_s^+ X_t$ are DMD **projected modes**.

To understand why they're called **projected modes**, note that:

$$
\Phi_s^+ = (\Phi_s^\top \Phi_s)^{-1} \Phi_s^\top
$$

so the $m \times p$ matrix:

$$
\hat b = \Phi_s^+ X
$$

is a matrix of regression coefficients of the $m \times n$ matrix $X$ on the $m \times p$ matrix $\Phi_s$.

We'll discuss this interpretation further in a related context when we talk about representation 3, which was suggested by Tu et al. [[Tu _et al._, 2014](/courses/Introduction-to-Quantitative-Economics/References#id28)].

Representation 3 is more suitable when, as is often the case in practice, we want to use a reduced SVD.

## Representation 3

Unlike the approaches used for Representations 1 and 2, which utilized a **full** SVD, we now employ a **reduced** SVD.

Let $ p \leq \textrm{min}(m,n) $ represent the rank of $ X $.

Perform a **reduced** SVD:

$$
X = \tilde U \tilde \Sigma \tilde V^\top ,
$$

where $ \tilde U $ is $ m \times p $, $ \tilde \Sigma $ is $ p \times p $, and $ \tilde V^\top $ is $ p \times n $.

Our least-squares approximator of $ A $ with minimum norm is represented as:

$$
\hat A = X' \tilde V \tilde \Sigma^{-1} \tilde U^\top \tag{6.21}
$$

**Calculating Dominant Eigenvectors of $ \hat A $**

We start by paralleling a step from Representation 1, defining a transition matrix for a rotated $ p \times 1 $ state $ \tilde b_t $ as:

$$
\tilde A =\tilde U^\top \hat A \tilde U \tag{6.22}
$$

**Interpretation as Projection Coefficients**

[[Brunton and Kutz, 2022](/courses/Introduction-to-Quantitative-Economics/References#id43)] note that $ \tilde A $ can be interpreted as a projection of $ \hat A $ onto the $ p $ modes in $ \tilde U $.

To verify this, note that since $ \tilde U^\top \tilde U = I $, it follows that:

$$
\tilde A = \tilde U^\top \hat A \tilde U = \tilde U^\top X' \tilde V \tilde \Sigma^{-1} \tilde U^\top \tilde U = \tilde U^\top X' \tilde V \tilde \Sigma^{-1} \tilde U^\top \tag{6.23}
$$

Next, we compute the regression coefficients in a projection of $ \hat A $ on $ \tilde U $ using a standard least-squares formula:

$$
(\tilde U^\top \tilde U)^{-1} \tilde U^\top \hat A = (\tilde U^\top \tilde U)^{-1} \tilde U^\top X' \tilde V \tilde \Sigma^{-1} \tilde U^\top = \tilde U^\top X' \tilde V \tilde \Sigma^{-1} \tilde U^\top = \tilde A.
$$

Thus, $ \tilde A $ is a least-squares projection of $ \hat A $ onto $ \tilde U $.

**An Inverse Challenge**

Because we are using a reduced SVD, $ \tilde U \tilde U^\top \neq I $.

Therefore,

$$
\hat A \neq \tilde U \tilde A \tilde U^\top ,
$$

so we cannot simply recover $ \hat A $ from $ \tilde A $ and $ \tilde U $.

**A Blind Alley**

We can start by hoping for the best and proceed to construct an eigendecomposition of the $ p \times p $ matrix $ \tilde A $:

$$
\tilde A = \tilde W \Lambda \tilde W^{-1} \tag{6.24}
$$

where $ \Lambda $ is a diagonal matrix of $ p $ eigenvalues and the columns of $ \tilde W $ are corresponding eigenvectors.

Mimicking our procedure in Representation 2, we compute an $ m \times p $ matrix:

$$
\tilde \Phi_s = \tilde U \tilde W \tag{6.25}
$$

At this point, where $ \hat A $ is given by formula [(6.21)](#equation-eq-ahatwithtildes), it is interesting to compute $ \hat A \tilde \Phi_s $:

$$
\begin{aligned}
\hat A \tilde \Phi_s & = (X' \tilde V \tilde \Sigma^{-1} \tilde U^\top ) (\tilde U \tilde W) \\
& = X' \tilde V \tilde \Sigma^{-1} \tilde W \\
& \neq (\tilde U \tilde W) \Lambda \\
& = \tilde \Phi_s \Lambda
\end{aligned}
$$

That $ \hat A \tilde \Phi_s \neq \tilde \Phi_s \Lambda $ means that, unlike in Representation 2, columns of $ \tilde \Phi_s = \tilde U \tilde W $ are **not** eigenvectors of $ \hat A $ corresponding to eigenvalues on the diagonal of $ \Lambda $.

**An Approach That Works**

Continuing our quest for eigenvectors of $ \hat A $ that we **can** compute with a reduced SVD, letâ€™s define an $ m \times p $ matrix $ \Phi $ as:

$$
\Phi \equiv \hat A \tilde \Phi_s = X' \tilde V \tilde \Sigma^{-1} \tilde W \tag{6.26}
$$

It turns out that columns of $ \Phi $ **are** eigenvectors of $ \hat A $.

This is due to a result established by Tu et al. [[Tu _et al._, 2014](/courses/Introduction-to-Quantitative-Economics/References#id28)].

**Proposition** The $ p $ columns of $ \Phi $ are eigenvectors of $ \hat A $.

**Proof:** From formula [(6.26)](#equation-eq-phiformula), we have:

$$
\begin{aligned}
\hat A \Phi & = (X' \tilde V \tilde \Sigma^{-1} \tilde U^\top ) (X' \tilde V \Sigma^{-1} \tilde W) \cr
& = X' \tilde V \tilde \Sigma^{-1} \tilde A \tilde W \cr
& = X' \tilde V \tilde \Sigma^{-1}\tilde W \Lambda \cr
& = \Phi \Lambda
\end{aligned}
$$

so that:

$$
\hat A \Phi = \Phi \Lambda . \tag{6.27}
$$

Let $ \phi_i $ be the $ i $th column of $ \Phi $ and $ \lambda_i $ be the corresponding $ i $ eigenvalue of $ \tilde A $ from decomposition [(6.24)](#equation-eq-tildeaeigenred).

Equating the $ m \times 1 $ vectors on both sides of equation [(6.27)](#equation-eq-aphilambda) gives:

$$
\hat A \phi_i = \lambda_i \phi_i .
$$

This confirms that $ \phi_i $ is an eigenvector of $ \hat A $ corresponding to eigenvalue $ \lambda_i $ of both $ \tilde A $ and $ \hat A $.

This concludes the proof.

Also see [[Brunton and Kutz, 2022](/courses/Introduction-to-Quantitative-Economics/References#id43)] (p. 238).

### Decoder of $ \check b $ as a Linear Projection

From the eigendecomposition [(6.27)](#equation-eq-aphilambda), we can express $ \hat A $ as:

$$
\hat A = \Phi \Lambda \Phi^+ . \tag{6.28}
$$

Using formula [(6.28)](#equation-eq-aform12), we can derive the dynamics of the $ p \times 1 $ vector $ \check b_t $:

$$
\check b_{t+1} = \Lambda \check b_t
$$

where

$$
\check b_t = \Phi^+ X_t \tag{6.29}
$$

Since the $ m \times p $ matrix $ \Phi $ has $ p $ linearly independent columns, its generalized inverse is:

$$
\Phi^{+} = (\Phi^\top \Phi)^{-1} \Phi^\top
$$

Thus,

$$
\check b = (\Phi^\top \Phi)^{-1} \Phi^\top X \tag{6.30}
$$

The $ p \times n $ matrix $ \check b $ can be recognized as a matrix of least squares regression coefficients of the $ m \times n $ matrix $ X $ on the $ m \times p $ matrix $ \Phi $. Consequently,

$$
\check X = \Phi \check b \tag{6.31}
$$

is an $ m \times n $ matrix of least squares projections of $ X $ on $ \Phi $.

**Variance Decomposition of $ X $**

According to the least-squares projection theory, we can represent $ X $ as the sum of its projection $ \check X $ on $ \Phi $ plus a matrix of errors.

To verify this, note that the least squares projection $ \check X $ relates to $ X $ by:

$$
X = \check X + \epsilon
$$

or

$$
X = \Phi \check b + \epsilon \tag{6.32}
$$

where $ \epsilon $ is an $ m \times n $ matrix of least squares errors satisfying the orthogonality conditions $ \epsilon^\top \Phi =0 $ or

$$
(X - \Phi \check b)^\top \Phi = 0_{m \times p} \tag{6.33}
$$

Rearranging the orthogonality conditions [(6.33)](#equation-eq-orthls) gives $ X^\top \Phi = \check b \Phi^\top \Phi $, which implies formula [(6.30)](#equation-eq-checkbform).

### An Approximation

We'll now describe a method to approximate the $ p \times 1 $ vector $ \check b_t $ instead of using formula [(6.29)](#equation-eq-decoder102).

The following argument, adapted from [[Brunton and Kutz, 2022](/courses/Introduction-to-Quantitative-Economics/References#id43)] (page 240), provides a computationally efficient way to approximate $ \check b_t $.

For convenience, we'll apply the method at time $ t=1 $.

At $ t=1 $, from equation [(6.32)](#equation-eq-xbcheck) we have:

$$
\check X_1 = \Phi \check b_1 \tag{6.34}
$$

where $ \check b_1 $ is a $ p \times 1 $ vector.

Recall from representation 1 that $ X_1 = U \tilde b_1 $, where $ \tilde b_1 $ is a time $ 1 $ basis vector for representation 1 and $ U $ is from the full SVD $ X = U \Sigma V^\top $.

It follows from equation [(6.32)](#equation-eq-xbcheck) that:

$$
U \tilde b_1 = X' \tilde V \tilde \Sigma^{-1} \tilde W \check b_1 + \epsilon_1
$$

where $ \epsilon_1 $ is a least-squares error vector from equation [(6.32)](#equation-eq-xbcheck).

Consequently,

$$
\tilde b_1 = U^\top X' V \tilde \Sigma^{-1} \tilde W \check b_1 + U^\top \epsilon_1
$$

Replacing the error term $ U^\top \epsilon_1 $ with zero, and substituting $ \tilde U $ from a **reduced** SVD for $ U $ from a **full** SVD of $ X $, we obtain an approximation $ \hat b_1 $ to $ \tilde b_1 $:

$$
\hat b_1 = \tilde U^\top X' \tilde V \tilde \Sigma^{-1} \tilde W \check b_1
$$

Recall from equation [(6.23)](#equation-eq-tildeaverify) that $ \tilde A = \tilde U^\top X' \tilde V \tilde \Sigma^{-1} $.

It follows that:

$$
\hat b_1 = \tilde A \tilde W \check b_1
$$

and therefore, by the eigendecomposition [(6.24)](#equation-eq-tildeaeigenred) of $ \tilde A $, we have:

$$
\hat b_1 = \tilde W \Lambda \check b_1
$$

Consequently,

$$
\hat b_1 = ( \tilde W \Lambda)^{-1} \tilde b_1
$$

or

$$
\hat b_1 = ( \tilde W \Lambda)^{-1} \tilde U^\top X_1 , \tag{6.35}
$$

which is a computationally efficient approximation to the following instance of equation [(6.29)](#equation-eq-decoder102) for the initial vector $ \check b_1 $:

$$
\check b_1= \Phi^{+} X_1 \tag{6.36}
$$

(To emphasize that [(6.35)](#equation-eq-beqnsmall) is an approximation, DMD users sometimes refer to components of basis vector $ \check b_t = \Phi^+ X_t $ as the **exact** DMD modes and components of $ \hat b_t = ( \tilde W \Lambda)^{-1} \tilde U^\top X_t $ as the **approximate** modes.)

Conditional on $ X*t $, we can compute a decoded $ \check X*{t+j}, j = 1, 2, \ldots $ from the exact modes via:

$$
\check X_{t+j} = \Phi \Lambda^j \Phi^{+} X_t \tag{6.37}
$$

or compute a decoded $ \hat X\_{t+j} $ from approximate modes via:

$$
\hat X_{t+j} = \Phi \Lambda^j (\tilde W \Lambda)^{-1} \tilde U^\top X_t . \tag{6.38}
$$

We can then use a decoded $ \check X*{t+j} $ or $ \hat X*{t+j} $ to forecast $ X\_{t+j} $.

### Using Fewer Modes

In practical applications, we typically use only a small number of modes, often three or fewer.

Some of the preceding formulas assume that we have retained all $ p $ modes associated with singular values of $ X $.

We can adjust our formulas to describe a situation where we instead retain only the $ r < p $ largest singular values.

In this case, we simply:

- Replace $ \tilde \Sigma $ with the appropriate $ r\times r $ matrix of singular values
- Replace $ \tilde U $ with the $ m \times r $ matrix whose columns correspond to the $ r $ largest singular values
- Replace $ \tilde V $ with the $ n \times r $ matrix whose columns correspond to the $ r $ largest singular values

Counterparts of all the relevant formulas above then apply.

## Source for Some Python Code

You can find a Python implementation of DMD at the following link:

[https://mathlab.sissa.it/pydmd](https://mathlab.sissa.it/pydmd)
