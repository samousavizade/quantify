---
title: Using Newton Method to Solve Economic Models
draft: false
summary: test
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

# Employing Newton's Technique for Resolving Economic Frameworks

## Synopsis

Numerous economic challenges involve identifying [fixed
points](https://en.wikipedia.org/wiki/Fixed_point_%28mathematics%29) or
[zeros](https://en.wikipedia.org/wiki/Zero_of_a_function) (occasionally termed
"roots") of functions.

For instance, in a basic supply and demand model, an equilibrium price is one
that renders excess demand zero.

In other words, an equilibrium represents a zero of the excess demand function.

Various computational methods exist for determining fixed points and
zeros.

In this lecture, we examine an important gradient-based technique known as [Newton's
method](https://en.wikipedia.org/wiki/Newton%27s_method).

Newton's method isn't universally applicable, but when it works,
it often converges faster than alternative methods.

The lecture will apply Newton's method in one-dimensional and
multi-dimensional contexts to resolve fixed-point and zero-finding problems.

- When seeking the fixed point of a function $ f $, Newton's method updates
  an existing estimate of the fixed point by solving for the fixed point of a
  linear approximation to the function $ f $.
- When seeking the zero of a function $ f $, Newton's method updates
  an existing estimate by solving for the zero of a linear approximation to
  the function $ f $.

To develop intuition,... we first consider a straightforward, one-dimensional fixed point
problem where we know the solution and solve it using both iterative
approximation and Newton's method.

Then we apply Newton's method to multi-dimensional scenarios to solve
market equilibria with multiple commodities.

At the conclusion of the lecture we harness the power of automatic
differentiation in [`autograd`](https://github.com/HIPS/autograd) to solve a very high-dimensional equilibrium problem

```python
!pip install autograd
```

    Requirement already satisfied: autograd in /Users/s.alirezamousavizade/anaconda3/lib/python3.11/site-packages (1.6.2)
    Requirement already satisfied: numpy>=1.12 in /Users/s.alirezamousavizade/anaconda3/lib/python3.11/site-packages (from autograd) (1.26.4)
    Requirement already satisfied: future>=0.15.2 in /Users/s.alirezamousavizade/anaconda3/lib/python3.11/site-packages (from autograd) (0.18.3)

We employ the following imports in this lecture

```python
import matplotlib.pyplot as plt
from collections import namedtuple
from scipy.optimize import root
from autograd import jacobian
# Thinly-wrapped numpy to enable automatic differentiation
import autograd.numpy as np

plt.rcParams["figure.figsize"] = (10, 5.7)
```

## Determining Fixed Points via Newton's Approach

In this section we determine the fixed point of the capital evolution equation in
the context of the [Solow growth
model](https://en.wikipedia.org/wiki/Solow%E2%80%93Swan_model).

We will examine the fixed point visually, solve it through iterative
approximation, and then employ Newton's method to achieve quicker convergence.

### The Solow Framework

In the Solow growth model, assuming Cobb-Douglas production technology and
zero population growth, the capital evolution equation is

$$
k_{t+1} = g(k_t) \quad \text{where} \quad
g(k) := sAk^\alpha + (1-\delta) k \tag{7.1}
$$

Here

- $ k_t $ represents capital stock per worker,
- $ A, \alpha>0 $ are production parameters, $ \alpha<1 $
- $ s>0 $ is a savings rate, and
- $ \delta \in(0,1) $ is a rate of depreciation

In this example, we aim to calculate the unique strictly positive fixed point
of $ g $, the capital evolution equation.

In other words, we seek a $ \hat{k} > 0 $ such that $ g(\hat{k})=k^\* $.

- such a $ \hat{k} $ is termed a [steady state](https://en.wikipedia.org/wiki/Steady_state),
  since $ k_t = \hat{k} $ implies $ k\_{t+1} = k^\* $.

Using manual calculations to solve $ g(k)=k $, you can verify that

$$
k^* = \left(\frac{s A}{δ}\right)^{1/(1 - α)}
$$

### Implementation

Let's store our parameters in a [`namedtuple`](https://docs.python.org/3/library/collections.html#collections.namedtuple) to maintain clean and concise code.

```python
SolowParameters = namedtuple("SolowParameters", ('A', 's', 'α', 'δ'))
```

This function generates an appropriate `namedtuple` with default parameter values.

```python
def create_solow_params(A=2.0, s=0.3, α=0.3, δ=0.4):
    "Creates a Solow model parameterization with default values."
    return SolowParameters(A=A, s=s, α=α, δ=δ)
```

The following two functions implement the capital evolution equation [(7.1)](#equation-motion-law) and record the true fixed point $ k^\* $.

```python
def g(k, params):
    A, s, α, δ = params
    return A * s * k**α + (1 - δ) * k

def exact_fixed_point(params):
    A, s, α, δ = params
    return ((s * A) / δ)**(1/(1 - α))
```

Here's a function to create a 45 degree plot of the dynamics.

```python
def plot_45(params, ax, fontsize=14):

    k_min, k_max = 0.0, 3.0
    k_grid = np.linspace(k_min, k_max, 1200)

    # Plot the functions
    lb = r"$g(k) = sAk^{\alpha} + (1 - \delta)k$"
    ax.plot(k_grid, g(k_grid, params),  lw=2, alpha=0.6, label=lb)
    ax.plot(k_grid, k_grid, "k--", lw=1, alpha=0.7, label="45")

    # Show and annotate the fixed point
    kstar = exact_fixed_point(params)
    fps = (kstar,)
    ax.plot(fps, fps, "go", ms=10, alpha=0.6)
    ax.annotate(r"$k^* = (sA / \delta)^{\frac{1}{1-\alpha}}$",
             xy=(kstar, kstar),
             xycoords="data",
             xytext=(20, -20),
             textcoords="offset points",
             fontsize=fontsize)

    ax.legend(loc="upper left", frameon=False, fontsize=fontsize)

    ax.set_yticks((0, 1, 2, 3))
    ax.set_yticklabels((0.0, 1.0, 2.0, 3.0), fontsize=fontsize)
    ax.set_ylim(0, 3)
    ax.set_xlabel("$k_t$", fontsize=fontsize)
    ax.set_ylabel("$k_{t+1}$", fontsize=fontsize)
```

Let's examine the 45 degree diagram for two parameter sets.

```python
params = create_solow_params()
fig, ax = plt.subplots(figsize=(8, 8))
plot_45(params, ax)
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/7.%20Using%20Newton%E2%80%99s%20Method%20to%20Solve%20Economic%20Models_files/7.%20Using%20Newton%E2%80%99s%20Method%20to%20Solve%20Economic%20Models_19_0.png)
</div>

```python
params = create_solow_params(α=0.05, δ=0.5)
fig, ax = plt.subplots(figsize=(8, 8))
plot_45(params, ax)
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/7.%20Using%20Newton%E2%80%99s%20Method%20to%20Solve%20Economic%20Models_files/7.%20Using%20Newton%E2%80%99s%20Method%20to%20Solve%20Economic%20Models_20_0.png)
</div>

We observe that $ k^\* $ is indeed the sole positive fixed point.

#### Iterative Approximation

First, let's compute the fixed point using iterative approximation.

In this case, iterative approximation involves repeatedly updating capital
from an initial state $ k_0 $ using the evolution equation.

Here's a time series from a specific choice of $ k_0 $.

```python
def compute_iterates(k_0, f, params, n=25):
    "Compute time series of length n generated by arbitrary function f."
    k = k_0
    k_iterates = []
    for t in range(n):
        k_iterates.append(k)
        k = f(k, params)
    return k_iterates
```

```python
params = create_solow_params()
k_0 = 0.25
k_series = compute_iterates(k_0, g, params)
k_star = exact_fixed_point(params)

fig, ax = plt.subplots()
ax.plot(k_series, 'o')
ax.plot([k_star] * len(k_series), 'k--')
ax.set_ylim(0, 3)
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/7.%20Using%20Newton%E2%80%99s%20Method%20to%20Solve%20Economic%20Models_files/7.%20Using%20Newton%E2%80%99s%20Method%20to%20Solve%20Economic%20Models_24_0.png)
</div>

Let's view the output for an extended time series.

```python
k_series = compute_iterates(k_0, g, params, n=10_000)
k_star_approx = k_series[-1]
k_star_approx
```

    1.7846741842265788

This approximates the true value closely.

```python
k_star
```

    1.7846741842265788

#### Newton's Approach

Generally, when applying Newton's fixed point method to a function $ g $,
we begin with an estimate $ x_0 $ of the fixed
point and then update by solving for the fixed point of a tangent line at
$ x_0 $.

To start, we recall that the first-order approximation of $ g $ at $ x_0 $
(i.e., the first order Taylor approximation of $ g $ at $ x_0 $) is
the function

$$
\hat g(x) \approx g(x_0)+g'(x_0)(x-x_0) \tag{7.2}
$$

We solve for the fixed point of $ \hat g $ by calculating the $ x_1 $ that satisfies

$$
x_1=\frac{g(x_0)-g'(x_0) x_0}{1-g'(x_0)}
$$

Generalizing the process above, Newton's fixed point method iterates on

$$
x_{t+1} = \frac{g(x_t) - g'(x_t) x_t}{ 1 - g'(x_t) },
\quad x_0 \text{ given} \tag{7.3}
$$

To implement Newton's method we note that the derivative of the capital evolution equation [(7.1)](#equation-motion-law) is

$$
g'(k) = \alpha s A k^{\alpha-1} + (1-\delta) \tag{7.4}
$$

Let's define this:

```python
def Dg(k, params):
    A, s, α, δ = params
    return α * A * s * k**(α-1) + (1 - δ)
```

Here's a function $ q $ representing [(7.3)](#equation-newtons-method).

```python
def q(k, params):
    return (g(k, params) - Dg(k, params) * k) / (1 - Dg(k, params))
```

Now let's plot some trajectories.

```python
def plot_trajectories(params,
                      k0_a=0.8,  # first initial condition
                      k0_b=3.1,  # second initial condition
                      n=20,      # length of time series
                      fs=14):    # fontsize

    fig, axes = plt.subplots(2, 1, figsize=(10, 6))
    ax1, ax2 = axes

    ks1 = compute_iterates(k0_a, g, params, n)
    ax1.plot(ks1, "-o", label="successive approximation")

    ks2 = compute_iterates(k0_b, g, params, n)
    ax2.plot(ks2, "-o", label="successive approximation")

    ks3 = compute_iterates(k0_a, q, params, n)
    ax1.plot(ks3, "-o", label="newton steps")

    ks4 = compute_iterates(k0_b, q, params, n)
    ax2.plot(ks4, "-o", label="newton steps")

    for ax in axes:
        ax.plot(k_star * np.ones(n), "k--")
        ax.legend(fontsize=fs, frameon=False)
        ax.set_ylim(0.6, 3.2)
        ax.set_yticks((k_star,))
        ax.set_yticklabels(("$k^*$",), fontsize=fs)
        ax.set_xticks(np.linspace(0, 19, 20))

    plt.show()
```

```python
params = create_solow_params()
plot_trajectories(params)
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/7.%20Using%20Newton%E2%80%99s%20Method%20to%20Solve%20Economic%20Models_files/7.%20Using%20Newton%E2%80%99s%20Method%20to%20Solve%20Economic%20Models_35_0.png)
</div>

We can observe that Newton's method converges more rapidly than iterative approximation.

## Locating Roots in Single Dimension

In the previous section we computed fixed points.

In fact, Newton's method is more commonly associated with the task of
finding zeros of functions.

Let's discuss this "root-finding" problem and then show how it relates to
the task of finding fixed points.

### Newton's Method for Zeros

Let's assume we want to find an $ x $ such that $ f(x)=0 $ for some smooth
function $ f $ mapping real numbers to real numbers.

Suppose we have an estimate $ x_0 $ and we want to update it to a new point $ x_1 $.

As a first step, we take the first-order approximation of $ f $ around $ x_0 $:

$$
\hat f(x) \approx f\left(x_0\right)+f^{\prime}\left(x_0\right)\left(x-x_0\right)
$$

Now we solve for the zero of $ \hat f $.

Specifically, we set $ \hat{f}(x_1) = 0 $ and solve for $ x_1 $ to get

$$
x_1 = x_0 - \frac{ f(x_0) }{ f'(x_0) },
\quad x_0 \text{ given}
$$

Generalizing the formula above, for one-dimensional zero-finding problems, Newton's method iterates on

$$
x_{t+1} = x_t - \frac{ f(x_t) }{ f'(x_t) },
\quad x_0 \text{ given} \tag{7.5}
$$

The following code implements the iteration [(7.5)](#equation-oned-newton)

```python
def newton(f, Df, x_0, tol=1e-7, max_iter=100_000):
    x = x_0

    # Implement the zero-finding formula
    def q(x):
        return x - f(x) / Df(x)

    error = tol + 1
    n = 0
    while error > tol:
        n += 1
        if(n > max_iter):
            raise Exception('Max iteration reached without convergence')
        y = q(x)
        error = np.abs(x - y)
        x = y
        print(f'iteration {n}, error = {error:.5f}')
    return x
```

Many libraries implement Newton's method in one dimension, including
SciPy, so this code is for illustrative purposes only.

(That said, when we want to apply Newton's method using techniques such as
automatic differentiation or GPU acceleration, it will be beneficial to know how
to implement Newton's method ourselves.)

### Application to Determining Fixed Points

Now consider again the Solow fixed-point calculation, where we solve for $ k $
satisfying $ g(k) = k $.

We can convert this to a zero-finding problem by setting $ f(x) := g(x)-x $.

Any zero of $ f $ is clearly a fixed point of $ g $.

Let's apply this concept to the Solow problem

```python
params = create_solow_params()
k_star_approx_newton = newton(f=lambda x: g(x, params) - x,
                              Df=lambda x: Dg(x, params) - 1,
                              x_0=0.8)
```

    iteration 1, error = 1.27209
    iteration 2, error = 0.28180
    iteration 3, error = 0.00561
    iteration 4, error = 0.00000
    iteration 5, error = 0.00000

```python
k_star_approx_newton
```

    1.7846741842265788

The outcome confirms the convergence we observed in the graphs above:... a highly accurate result is achieved with only 5 iterations.

## Newton's Method for Multiple Variables

In this section, we introduce a two-good problem, present a
visualization of the problem, and determine the equilibrium of the two-good market
using both a zero finder in `SciPy` and Newton's method.

We then extend the idea to a larger market with 5,000 goods and compare the
performance of the two methods again.

We will observe a significant performance gain when using Newton's method.

### A Two Goods Market Equilibrium

Let's begin by computing the market equilibrium of a two-good problem.

We consider a market for two related products, good 0 and good 1, with
price vector $ p = (p_0, p_1) $

Supply of good $ i $ at price $ p $,

$$
q^s_i (p) = b_i \sqrt{p_i}
$$

Demand of good $ i $ at price $ p $ is,

$$
q^d_i (p) = \exp(-(a_{i0} p_0 + a_{i1} p_1)) + c_i
$$

Here $ c*i $, $ b_i $ and $ a*{ij} $ are parameters.

For instance, the two goods might be computer components that are typically used together, making them complements. Thus, demand depends on the price of both components.

The excess demand function is,

$$
e_i(p) = q^d_i(p) - q^s_i(p), \quad i = 0, 1
$$

An equilibrium price vector $ \hat{p} $ satisfies $ e_i(\hat{p}) = 0 $.

We set

$$
A = \begin{pmatrix}
a_{00} & a_{01} \\
a_{10} & a_{11}
\end{pmatrix},
\qquad
b = \begin{pmatrix}
b_0 \\
b_1
\end{pmatrix}
\qquad \text{and} \qquad
c = \begin{pmatrix}
c_0 \\
c_1
\end{pmatrix}
$$

for this specific question.

#### A Graphical Exploration

Since our problem is only two-dimensional, we can use graphical analysis to visualize and help understand the problem.

Our first step is to define the excess demand function

$$
e(p) =
\begin{pmatrix}
e_0(p) \\
e_1(p)
\end{pmatrix}
$$

The function below calculates the excess demand for given parameters

```python
def e(p, A, b, c):
    return np.exp(- A @ p) + c - b * np.sqrt(p)
```

Our default parameter values will be

$$
A = \begin{pmatrix}
0.5 & 0.4 \\
0.8 & 0.2
\end{pmatrix},
\qquad
b = \begin{pmatrix}
1 \\
1
\end{pmatrix}
\qquad \text{and} \qquad
c = \begin{pmatrix}
1 \\
1
\end{pmatrix}
$$

```python
A = np.array([
    [0.5, 0.4],
    [0.8, 0.2]
])
b = np.ones(2)
c = np.ones(2)
```

At a price level of $ p = (1, 0.5) $, the excess demand is

```python
ex_demand = e((1.0, 0.5), A, b, c)

print(f'The excess demand for good 0 is {ex_demand[0]:.3f} \n'
      f'The excess demand for good 1 is {ex_demand[1]:.3f}')
```

    The excess demand for good 0 is 0.497
    The excess demand for good 1 is 0.699

Next we plot the two functions $ e_0 $ and $ e_1 $ on a grid of $ (p_0, p_1) $ values, using contour surfaces and lines.

We will use the following function to create the contour plots

```python
def plot_excess_demand(ax, good=0, grid_size=100, grid_max=4, surface=True):

    # Create a 100x100 grid
    p_grid = np.linspace(0, grid_max, grid_size)
    z = np.empty((100, 100))

    for i, p_1 in enumerate(p_grid):
        for j, p_2 in enumerate(p_grid):
            z[i, j] = e((p_1, p_2), A, b, c)[good]

    if surface:
        cs1 = ax.contourf(p_grid, p_grid, z.T, alpha=0.5)
        plt.colorbar(cs1, ax=ax, format="%.6f")

    ctr1 = ax.contour(p_grid, p_grid, z.T, levels=[0.0])
    ax.set_xlabel("$p_0$")
    ax.set_ylabel("$p_1$")
    ax.set_title(f'Excess Demand for Good {good}')
    plt.clabel(ctr1, inline=1, fontsize=13)
```

Here's our plot of $ e_0 $:

```python
fig, ax = plt.subplots()
plot_excess_demand(ax, good=0)
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/7.%20Using%20Newton%E2%80%99s%20Method%20to%20Solve%20Economic%20Models_files/7.%20Using%20Newton%E2%80%99s%20Method%20to%20Solve%20Economic%20Models_56_0.png)
</div>

Here's our plot of $ e_1 $:

```python
fig, ax = plt.subplots()
plot_excess_demand(ax, good=1)
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/7.%20Using%20Newton%E2%80%99s%20Method%20to%20Solve%20Economic%20Models_files/7.%20Using%20Newton%E2%80%99s%20Method%20to%20Solve%20Economic%20Models_58_0.png)
</div>

We observe the black contour line of zero, which indicates when $ e_i(p)=0 $.

For a price vector $ p $ such that $ e_i(p)=0 $ we know that good $ i $ is in equilibrium (demand equals supply).

If these two contour lines intersect at some price vector $ \hat{p} $, then $ \hat{p} $ is an equilibrium price vector.

```python
fig, ax = plt.subplots(figsize=(10, 5.7))
for good in (0, 1):
    plot_excess_demand(ax, good=good, surface=False)
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/7.%20Using%20Newton%E2%80%99s%20Method%20to%20Solve%20Economic%20Models_files/7.%20Using%20Newton%E2%80%99s%20Method%20to%20Solve%20Economic%20Models_60_0.png)
</div>

It appears there is an equilibrium near $ p = (1.6, 1.5) $.

#### Employing a Multidimensional Root Finder

To determine $ p^\* $ more accurately, we utilize a zero-finding algorithm from `scipy.optimize`.

We provide $ p = (1, 1) $ as our initial estimate.

```python
init_p = np.ones(2)
```

This employs the [modified Powell method](https://docs.scipy.org/doc/scipy/reference/optimize.root-hybr.html#optimize-root-hybr) to locate the zero

```python
%%time
solution = root(lambda p: e(p, A, b, c), init_p, method='hybr')
```

    CPU times: user 1.13 ms, sys: 81 μs, total: 1.21 ms
    Wall time: 822 μs

Here's the resulting value:

```python
p = solution.x
p
```

    array([1.57080182, 1.46928838])

This appears close to our estimate from examining the figure. We can insert it back into $ e $ to verify that $ e(p) \approx 0 $:

```python
np.max(np.abs(e(p, A, b, c)))
```

    2.0383694732117874e-13

This is indeed a very minor error.

#### Incorporating Gradient Information

In many instances, for zero-finding algorithms applied to smooth functions, providing the [Jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) of the function results in improved convergence properties.

Here we manually compute the elements of the Jacobian

$$
J(p) =
\begin{pmatrix}
\frac{\partial e_0}{\partial p_0}(p) & \frac{\partial e_0}{\partial p_1}(p) \\
\frac{\partial e_1}{\partial p_0}(p) & \frac{\partial e_1}{\partial p_1}(p)
\end{pmatrix}
$$

```python
def jacobian_e(p, A, b, c):
    p_0, p_1 = p
    a_00, a_01 = A[0, :]
    a_10, a_11 = A[1, :]
    j_00 = -a_00 * np.exp(-a_00 * p_0) - (b[0]/2) * p_0**(-1/2)
    j_01 = -a_01 * np.exp(-a_01 * p_1)
    j_10 = -a_10 * np.exp(-a_10 * p_0)
    j_11 = -a_11 * np.exp(-a_11 * p_1) - (b[1]/2) * p_1**(-1/2)
    J = [[j_00, j_01],
         [j_10, j_11]]
    return np.array(J)
```

```python
%%time
solution = root(lambda p: e(p, A, b, c),
                init_p,
                jac=lambda p: jacobian_e(p, A, b, c),
                method='hybr')
```

    CPU times: user 872 μs, sys: 17 μs, total: 889 μs
    Wall time: 122 μs

Now the solution is even more precise (although, in this low-dimensional problem, the difference is quite minimal):

```python
p = solution.x
np.max(np.abs(e(p, A, b, c)))
```

    1.1102230246251565e-15

#### Applying Newton's Method

Now let's employ Newton's method to calculate the equilibrium price using the multivariate version of Newton's method

$$
p_{n+1} = p_n - J_e(p_n)^{-1} e(p_n) \tag{7.6}
$$

This is a multivariate version of [(7.5)](#equation-oned-newton)

(Here $ J_e(p_n) $ is the Jacobian of $ e $ evaluated at $ p_n $.)

The iteration begins from some initial guess of the price vector $ p_0 $.

Here, instead of coding the Jacobian manually, We utilize the `jacobian()` function in the `autograd` library to auto-differentiate and compute the Jacobian.

With minor modifications, we can extend [our previous attempt](#first-newton-attempt) to multi-dimensional problems

```python
def newton(f, x_0, tol=1e-5, max_iter=10):
    x = x_0
    q = lambda x: x - np.linalg.solve(jacobian(f)(x), f(x))
    error = tol + 1
    n = 0
    while error > tol:
        n+=1
        if(n > max_iter):
            raise Exception('Max iteration reached without convergence')
        y = q(x)
        if(any(np.isnan(y))):
            raise Exception('Solution not found with NaN generated')
        error = np.linalg.norm(x - y)
        x = y
        print(f'iteration {n}, error = {error:.5f}')
    print('\n' + f'Result = {x} \n')
    return x
```

```python
def e(p, A, b, c):
    return np.exp(- np.dot(A, p)) + c - b * np.sqrt(p)
```

We discover that the algorithm terminates in 4 steps

```python
%%time
p = newton(lambda p: e(p, A, b, c), init_p)
```

    iteration 1, error = 0.62515
    iteration 2, error = 0.11152
    iteration 3, error = 0.00258
    iteration 4, error = 0.00000

    Result = [1.57080182 1.46928838]

    CPU times: user 3.55 ms, sys: 182 μs, total: 3.73 ms
    Wall time: 1.08 ms

```python
np.max(np.abs(e(p, A, b, c)))
```

    1.4632739464559563e-13

The outcome is highly accurate.

With the increased overhead, the speed is not superior to the optimized `scipy` function.

### A High-Dimensional Challenge

Our next task is to explore a large market with 3,000 goods.

The excess demand function remains essentially the same, but now the matrix $ A $ is $ 3000 \times 3000 $ and the parameter vectors $ b $ and $ c $ are $ 3000 \times 1 $.

```python
dim = 3000
np.random.seed(123)

# Create a random matrix A and normalize the rows to sum to one
A = np.random.rand(dim, dim)
A = np.asarray(A)
s = np.sum(A, axis=0)
A = A / s

# Set up b and c
b = np.ones(dim)
c = np.ones(dim)
```

Here's our starting condition

```python
init_p = np.ones(dim)
```

```python
%%time
p = newton(lambda p: e(p, A, b, c), init_p)
```

    iteration 1, error = 23.22267


    iteration 2, error = 3.94538


    iteration 3, error = 0.08500


    iteration 4, error = 0.00004


    iteration 5, error = 0.00000

    Result = [1.50185286 1.49865815 1.50028285 ... 1.50875149 1.48724784 1.48577532]

    CPU times: user 11min 24s, sys: 26.5 s, total: 11min 51s
    Wall time: 2min 12s

```python
np.max(np.abs(e(p, A, b, c)))
```

    6.661338147750939e-16

Using the same tolerance, we compare the execution time and precision of Newton's method to SciPy's `root` function

```python
%%time
solution = root(lambda p: e(p, A, b, c),
                init_p,
                jac=lambda p: jacobian(e)(p, A, b, c),
                method='hybr',
                tol=1e-5)
```

    CPU times: user 4min 38s, sys: 17.6 s, total: 4min 55s
    Wall time: 1min 11s

```python
p = solution.x
np.max(np.abs(e(p, A, b, c)))
```

    8.295585955941931e-07

## Exercises

## Exercise 7.1

Consider a three-dimensional extension of the Solow fixed point problem with

$$
A = \begin{pmatrix}
2 & 3 & 3 \\
2 & 4 & 2 \\
1 & 5 & 1 \\
\end{pmatrix},
\quad
s = 0.2, \quad α = 0.5, \quad δ = 0.8
$$

As before, the law of motion is

$$
k_{t+1} = g(k_t) \quad \text{where} \quad
g(k) := sAk^\alpha + (1-\delta) k
$$

However $ k_t $ is now a $ 3 \times 1 $ vector.

Determine the fixed point using Newton's method with the following initial values:

$$
\begin{aligned}
k1_{0} &= (1, 1, 1) \\
k2_{0} &= (3, 5, 5) \\
k3_{0} &= (50, 50, 50)
\end{aligned}
$$

- The calculation of the fixed point is equivalent to finding $ \hat{k} $ such that $ f(\hat{k}) - k^\* = 0 $.
- If you are uncertain about your solution, you can begin with the solved example:...

$$
A = \begin{pmatrix}
2 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 2 \\
\end{pmatrix}
$$

with $ s = 0.3 $, $ α = 0.3 $, and $ δ = 0.4 $ and initial value:

$$
k_0 = (1, 1, 1)
$$

The result should converge to the [analytical solution](#solved-k).

## Solution to Exercise 7.1

Let's first define the parameters for this problem

```python
A = np.array([[2.0, 3.0, 3.0],
              [2.0, 4.0, 2.0],
              [1.0, 5.0, 1.0]])

s = 0.2
α = 0.5
δ = 0.8

initLs = [np.ones(3),
          np.array([3.0, 5.0, 5.0]),
          np.repeat(50.0, 3)]
```

Then define the multivariate version of the formula for the [(7.1)](#equation-motion-law)

```python
def multivariate_solow(k, A=A, s=s, α=α, δ=δ):
    return (s * np.dot(A, k**α) + (1 - δ) * k)
```

Let's iterate through each starting value and observe the output

```python
attempt = 1
for init in initLs:
    print(f'Attempt {attempt}: Starting value is {init} \n')
    %time k = newton(lambda k: multivariate_solow(k) - k, \
                    init)
    print('-'*64)
    attempt += 1
```

    Attempt 1: Starting value is [1. 1. 1.]

    iteration 1, error = 50.49630
    iteration 2, error = 41.10937
    iteration 3, error = 4.29413
    iteration 4, error = 0.38543
    iteration 5, error = 0.00544
    iteration 6, error = 0.00000

    Result = [3.84058108 3.87071771 3.41091933]

    CPU times: user 20.7 ms, sys: 77.9 ms, total: 98.6 ms
    Wall time: 18.4 ms
    ----------------------------------------------------------------
    Attempt 2: Starting value is [3. 5. 5.]

    iteration 1, error = 2.07011
    iteration 2, error = 0.12642
    iteration 3, error = 0.00060
    iteration 4, error = 0.00000

    Result = [3.84058108 3.87071771 3.41091933]

    CPU times: user 3.55 ms, sys: 10.9 ms, total: 14.5 ms
    Wall time: 1.82 ms
    ----------------------------------------------------------------
    Attempt 3: Starting value is [50. 50. 50.]

    iteration 1, error = 73.00943
    iteration 2, error = 6.49379
    iteration 3, error = 0.68070
    iteration 4, error = 0.01620
    iteration 5, error = 0.00001
    iteration 6, error = 0.00000

    Result = [3.84058108 3.87071771 3.41091933]

    CPU times: user 5.06 ms, sys: 15.7 ms, total: 20.8 ms
    Wall time: 2.6 ms
    ----------------------------------------------------------------

We find that the results are consistent regardless of the starting values given the well-defined nature of this problem.

However, the number of iterations required for convergence depends on the initial values.

Let's substitute the output back into the formula to verify our last result

```python
multivariate_solow(k) - k
```

    array([ 0.0000000e+00, -4.4408921e-16,  4.4408921e-16])

Note that the error is very small.

We can also test our results against the known solution

```python
A = np.array([[2.0, 0.0, 0.0],
               [0.0, 2.0, 0.0],
               [0.0, 0.0, 2.0]])

s = 0.3
α = 0.3
δ = 0.4

init = np.repeat(1.0, 3)


%time k = newton(lambda k: multivariate_solow(k, A=A, s=s, α=α, δ=δ) - k, \
                 init)
```

    iteration 1, error = 1.57459
    iteration 2, error = 0.21345
    iteration 3, error = 0.00205
    iteration 4, error = 0.00000

    Result = [1.78467418 1.78467418 1.78467418]

    CPU times: user 4.15 ms, sys: 13.8 ms, total: 18 ms
    Wall time: 2.25 ms

The result is very close to the ground truth but still slightly different.

```python
%time k = newton(lambda k: multivariate_solow(k, A=A, s=s, α=α, δ=δ) - k, \
                 init,\
                 tol=1e-7)
```

    iteration 1, error = 1.57459
    iteration 2, error = 0.21345
    iteration 3, error = 0.00205
    iteration 4, error = 0.00000
    iteration 5, error = 0.00000

    Result = [1.78467418 1.78467418 1.78467418]

    CPU times: user 5.09 ms, sys: 16.4 ms, total: 21.5 ms
    Wall time: 2.69 ms

We can see it progresses towards a more accurate solution.

## Exercise 7.2

In this exercise, let's experiment with different initial values and observe how Newton's method responds to various starting points.

Let's define a three-good problem with the following default values:

$$
A = \begin{pmatrix}
0.2 & 0.1 & 0.7 \\
0.3 & 0.2 & 0.5 \\
0.1 & 0.8 & 0.1 \\
\end{pmatrix},
\qquad
b = \begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix}
\qquad \text{and} \qquad
c = \begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix}
$$

For this exercise, use the following extreme price vectors as initial values:

$$
\begin{aligned}
p1_{0} &= (5, 5, 5) \\
p2_{0} &= (1, 1, 1) \\
p3_{0} &= (4.5, 0.1, 4)
\end{aligned}
$$

Set the tolerance to $ 0.0 $ for more precise output.

## Solution to Exercise 7.2

Define parameters and initial values

```python
A = np.array([
    [0.2, 0.1, 0.7],
    [0.3, 0.2, 0.5],
    [0.1, 0.8, 0.1]
])

b = np.array([1.0, 1.0, 1.0])
c = np.array([1.0, 1.0, 1.0])

initLs = [np.repeat(5.0, 3),
          np.ones(3),
          np.array([4.5, 0.1, 4.0])]
```

Let's iterate through each initial guess and examine the output

```python
attempt = 1
for init in initLs:
    print(f'Attempt {attempt}: Starting value is {init} \n')
    %time p = newton(lambda p: e(p, A, b, c), \
                init, \
                tol=1e-15, \
                max_iter=15)
    print('-'*64)
    attempt += 1
```

We can observe that Newton's method may fail for certain starting values.

Occasionally, it may require several initial guesses to achieve convergence.

Substitute the result back into the formula to verify our result

```python
e(p, A, b, c)
```

    array([ 2.22044605e-16,  0.00000000e+00, -2.22044605e-16])

We can see the result is highly accurate.
