---
title: Multivariate Normal Distribution
draft: false
summary: test
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

# Multivariate Normal Distribution

## Introduction

This lecture explores a fundamental concept in probability theory, statistics, and economics: the **multivariate normal distribution**.

In this session, you will discover formulas for:

- the joint distribution of a random vector $ x $ of size $ N $
- marginal distributions for all subsets of $ x $
- conditional distributions for subsets of $ x $ given other subsets of $ x $

We will employ the multivariate normal distribution to develop some useful models:

- a factor analysis model for intelligence quotient, or IQ
- a factor analysis model for two independent inherent abilities, such as mathematical and verbal skills.
- a more comprehensive factor analysis model
- Principal Components Analysis (PCA) as an approximation to a factor analysis model
- time series generated by linear stochastic difference equations
- optimal linear filtering theory

## Understanding the Multivariate Normal Distribution

This lecture introduces a Python class `MultivariateNormal` designed to create **marginal** and **conditional** distributions related to a multivariate normal distribution.

For a multivariate normal distribution, it is beneficial that:

- conditional expectations are equivalent to linear least squares projections
- conditional distributions are defined by multivariate linear regressions

We will apply our Python class to some examples.

We use the following imports:

```python
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (11, 5)  #set default figure size
import numpy as np
from numba import njit
import statsmodels.api as sm
```

Assume an $ N \times 1 $ random vector $ z $ has a multivariate normal probability density.

This implies that the probability density is given by

$$
f\left(z;\mu,\Sigma\right)=\left(2\pi\right)^{-\left(\frac{N}{2}\right)}\det\left(\Sigma\right)^{-\frac{1}{2}}\exp\left(-.5\left(z-\mu\right)^{\prime}\Sigma^{-1}\left(z-\mu\right)\right)
$$

where $ \mu=Ez $ is the mean of the random vector $ z $ and $ \Sigma=E\left(z-\mu\right)\left(z-\mu\right)^\prime $ is the covariance matrix of $ z $.

The covariance matrix $ \Sigma $ is symmetric and positive definite.

```python
@njit
def f(z, Œº, Œ£):
    """
    The density function of multivariate normal distribution.

    Parameters
    ---------------
    z: ndarray(float, dim=2)
        random vector, N by 1
    Œº: ndarray(float, dim=1 or 2)
        the mean of z, N by 1
    Œ£: ndarray(float, dim=2)
        the covarianece matrix of z, N by 1
    """

    z = np.atleast_2d(z)
    Œº = np.atleast_2d(Œº)
    Œ£ = np.atleast_2d(Œ£)

    N = z.size

    temp1 = np.linalg.det(Œ£) ** (-1/2)
    temp2 = np.exp(-.5 * (z - Œº).T @ np.linalg.inv(Œ£) @ (z - Œº))

    return (2 * np.pi) ** (-N/2) * temp1 * temp2
```

For an integer $ k\in \{1,\dots, N-1\} $, partition $ z $ as

$$
z=\left[\begin{array}{c} z_{1}\\ z_{2} \end{array}\right],
$$

where $ z_1 $ is an $ \left(N-k\right)\times1 $ vector and $ z_2 $ is a $ k\times1 $ vector.

Let

$$
\mu=\left[\begin{array}{c}
\mu_{1}\\
\mu_{2}
\end{array}\right],\quad\Sigma=\left[\begin{array}{cc}
\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{array}\right]
$$

be corresponding partitions of $ \mu $ and $ \Sigma $.

The **marginal** distribution of $ z_1 $ is

- multivariate normal with mean $ \mu*1 $ and covariance matrix $ \Sigma*{11} $.

The **marginal** distribution of $ z_2 $ is

- multivariate normal with mean $ \mu*2 $ and covariance matrix $ \Sigma*{22} $.

The distribution of $ z_1 $ **conditional** on $ z_2 $ is

- multivariate normal with mean

$$
\hat{\mu}_1 = \mu_1 + \beta \left(z_2 -\mu_2\right)
$$

and covariance matrix

$$
\hat{\Sigma}_{11}=\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}=\Sigma_{11}-\beta\Sigma_{22}\beta^{\prime}
$$

where

$$
\beta = \Sigma_{12}\Sigma_{22}^{-1}
$$

is an $ \left(N-k\right) \times k $ matrix of **population regression coefficients** of the $ (N -k) \times 1 $ random vector $ z_1 - \mu_1 $ on the $ k \times 1 $ random vector $ z_2 - \mu_2 $.

The following class constructs a multivariate normal distribution instance with two methods.

- a method `partition` computes $ \beta $, taking $ k $ as an input
- a method `cond_dist` computes either the distribution of $ z_1 $ conditional on $ z_2 $ or the distribution of $ z_2 $ conditional on $ z_1 $

```python
class MultivariateNormal:
    """
    Class of multivariate normal distribution.

    Parameters
    ----------
    Œº: ndarray(float, dim=1)
        the mean of z, N by 1
    Œ£: ndarray(float, dim=2)
        the covarianece matrix of z, N by 1

    Arguments
    ---------
    Œº, Œ£:
        see parameters
    Œºs: list(ndarray(float, dim=1))
        list of mean vectors Œº1 and Œº2 in order
    Œ£s: list(list(ndarray(float, dim=2)))
        2 dimensional list of covariance matrices
        Œ£11, Œ£12, Œ£21, Œ£22 in order
    Œ≤s: list(ndarray(float, dim=1))
        list of regression coefficients Œ≤1 and Œ≤2 in order
    """

    def __init__(self, Œº, Œ£):
        "initialization"
        self.Œº = np.array(Œº)
        self.Œ£ = np.atleast_2d(Œ£)

    def partition(self, k):
        """
        Given k, partition the random vector z into a size k vector z1
        and a size N-k vector z2. Partition the mean vector Œº into
        Œº1 and Œº2, and the covariance matrix Œ£ into Œ£11, Œ£12, Œ£21, Œ£22
        correspondingly. Compute the regression coefficients Œ≤1 and Œ≤2
        using the partitioned arrays.
        """
        Œº = self.Œº
        Œ£ = self.Œ£

        self.Œºs = [Œº[:k], Œº[k:]]
        self.Œ£s = [[Œ£[:k, :k], Œ£[:k, k:]],
                   [Œ£[k:, :k], Œ£[k:, k:]]]

        self.Œ≤s = [self.Œ£s[0][1] @ np.linalg.inv(self.Œ£s[1][1]),
                   self.Œ£s[1][0] @ np.linalg.inv(self.Œ£s[0][0])]

    def cond_dist(self, ind, z):
        """
        Compute the conditional distribution of z1 given z2, or reversely.
        Argument ind determines whether we compute the conditional
        distribution of z1 (ind=0) or z2 (ind=1).

        Returns
        ---------
        Œº_hat: ndarray(float, ndim=1)
            The conditional mean of z1 or z2.
        Œ£_hat: ndarray(float, ndim=2)
            The conditional covariance matrix of z1 or z2.
        """
        Œ≤ = self.Œ≤s[ind]
        Œºs = self.Œºs
        Œ£s = self.Œ£s

        Œº_hat = Œºs[ind] + Œ≤ @ (z - Œºs[1-ind])
        Œ£_hat = Œ£s[ind][ind] - Œ≤ @ Œ£s[1-ind][1-ind] @ Œ≤.T

        return Œº_hat, Œ£_hat
```

Let‚Äôs apply this code to a series of examples.

We begin with a straightforward bivariate example; then we‚Äôll proceed to a trivariate example.

We‚Äôll calculate population moments of some conditional distributions using our `MultivariateNormal` class.

For fun, we‚Äôll also calculate sample analogs of the related population regressions by generating simulations and then computing linear least squares regressions.

We‚Äôll compare those linear least squares regressions for the simulated data to their population counterparts.

## Example with Two Variables

We start with a bivariate normal distribution characterized by

$$
\mu=\left[\begin{array}{c}
.5 \\
1.0
\end{array}\right],\quad\Sigma=\left[\begin{array}{cc}
1 & .5\\
.5 & 1
\end{array}\right]
$$

```python
Œº = np.array([.5, 1.])
Œ£ = np.array([[1., .5], [.5 ,1.]])

# construction of the multivariate normal instance
multi_normal = MultivariateNormal(Œº, Œ£)
```

```python
k = 1 # choose partition

# partition and compute regression coefficients
multi_normal.partition(k)
multi_normal.Œ≤s[0],multi_normal.Œ≤s[1]
```

    (array([[0.5]]), array([[0.5]]))

Let‚Äôs demonstrate that you _can regress anything on anything else_.

We have calculated everything needed to compute two regression lines, one of $ z_2 $ on $ z_1 $, the other of $ z_1 $ on $ z_2 $.

We‚Äôll represent these regressions as

$$
z_1 = a_1 + b_1 z_2 + \epsilon_1
$$

and

$$
z_2 = a_2 + b_2 z_1 + \epsilon_2
$$

where we have the population least squares orthogonality conditions

$$
E \epsilon_1 z_2 = 0
$$

and

$$
E \epsilon_2 z_1 = 0
$$

Let‚Äôs compute $ a_1, a_2, b_1, b_2 $.

```python
beta = multi_normal.Œ≤s

a1 = Œº[0] - beta[0]*Œº[1]
b1 = beta[0]

a2 = Œº[1] - beta[1]*Œº[0]
b2 = beta[1]
```

Let‚Äôs print out the intercepts and slopes.

For the regression of $ z_1 $ on $ z_2 $ we have

```python
print ("a1 = ", a1)
print ("b1 = ", b1)
```

    a1 =  [[0.]]
    b1 =  [[0.5]]

For the regression of $ z_2 $ on $ z_1 $ we have

```python
print ("a2 = ", a2)
print ("b2 = ", b2)
```

    a2 =  [[0.75]]
    b2 =  [[0.5]]

Now let‚Äôs plot the two regression lines and examine them.

```python
z2 = np.linspace(-4,4,100)


a1 = np.squeeze(a1)
b1 = np.squeeze(b1)

a2 = np.squeeze(a2)
b2 = np.squeeze(b2)

z1  = b1*z2 + a1


z1h = z2/b2 - a2/b2


fig = plt.figure(figsize=(12,12))
ax = fig.add_subplot(1, 1, 1)
ax.set(xlim=(-4, 4), ylim=(-4, 4))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position('zero')
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')
plt.ylabel('$z_1$', loc = 'top')
plt.xlabel('$z_2$,', loc = 'right')
plt.title('two regressions')
plt.plot(z2,z1, 'r', label = "$z_1$ on $z_2$")
plt.plot(z2,z1h, 'b', label = "$z_2$ on $z_1$")
plt.legend()
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/12.%20Multivariate%20Normal%20Distribution_files/12.%20Multivariate%20Normal%20Distribution_21_0.png)
</div>

The red line is the expectation of $ z_1 $ given $ z_2 $.

The intercept and slope of the red line are

```python
print("a1 = ", a1)
print("b1 = ", b1)
```

    a1 =  0.0
    b1 =  0.5

The blue line is the expectation of $ z_2 $ given $ z_1 $.

The intercept and slope of the blue line are

```python
print("-a2/b2 = ", - a2/b2)
print("1/b2 = ", 1/b2)
```

    -a2/b2 =  -1.5
    1/b2 =  2.0

We can use these regression lines or our code to compute conditional expectations.

Let‚Äôs calculate the mean and variance of the distribution of $ z_2 $ given $ z_1=5 $.

After that, we‚Äôll switch what are on the left and right sides of the regression.

```python
# compute the cond. dist. of z1
ind = 1
z1 = np.array([5.]) # given z1

Œº2_hat, Œ£2_hat = multi_normal.cond_dist(ind, z1)
print('Œº2_hat, Œ£2_hat = ', Œº2_hat, Œ£2_hat)
```

    Œº2_hat, Œ£2_hat =  [3.25] [[0.75]]

Now let‚Äôs compute the mean and variance of the distribution of $ z_1 $ given $ z_2=5 $.

```python
# compute the cond. dist. of z1
ind = 0
z2 = np.array([5.]) # given z2

Œº1_hat, Œ£1_hat = multi_normal.cond_dist(ind, z2)
print('Œº1_hat, Œ£1_hat = ', Œº1_hat, Œ£1_hat)
```

    Œº1_hat, Œ£1_hat =  [2.5] [[0.75]]

Let‚Äôs compare the previous population mean and variance with results from drawing a large sample and then regressing $ z_1 - \mu_1 $ on $ z_2 - \mu_2 $.

We know that

$$
E z_1 | z_2 = \left(\mu_1 - \beta \mu_2 \right) + \beta z_2
$$

which can be rearranged to

$$
z_1 - \mu_1 = \beta \left( z_2 - \mu_2 \right) + \epsilon,
$$

We expect that for larger and larger sample sizes, estimated OLS coefficients will converge to $ \beta $ and the estimated variance of $ \epsilon $ will converge to $ \hat{\Sigma}\_1 $.

```python
n = 1_000_000 # sample size

# simulate multivariate normal random vectors
data = np.random.multivariate_normal(Œº, Œ£, size=n)
z1_data = data[:, 0]
z2_data = data[:, 1]

# OLS regression
Œº1, Œº2 = multi_normal.Œºs
results = sm.OLS(z1_data - Œº1, z2_data - Œº2).fit()
```

Let‚Äôs compare the previous population $ \beta $ with the OLS sample estimate on $ z_2 - \mu_2 $

```python
multi_normal.Œ≤s[0], results.params
```

    (array([[0.5]]), array([0.49936689]))

Let‚Äôs compare our population $ \hat{\Sigma}\_1 $ with the degrees-of-freedom adjusted estimate of the variance of $ \epsilon $

```python
Œ£1_hat, results.resid @ results.resid.T / (n - 1)
```

    (array([[0.75]]), 0.7506267045956279)

Lastly, let‚Äôs compute the estimate of $ \hat{E z_1 | z_2} $ and compare it with $ \hat{\mu}\_1 $

```python
Œº1_hat, results.predict(z2 - Œº2) + Œº1
```

    (array([2.5]), array([2.49746758]))

Thus, in each case, for our very large sample size, the sample analogues closely approximate their population counterparts.

A Law of Large Numbers explains why sample analogues approximate population objects.

## Example with Three Variables

Let‚Äôs apply our code to a trivariate example.

We‚Äôll specify the mean vector and the covariance matrix as follows.

```python
Œº = np.random.random(3)
C = np.random.random((3, 3))
Œ£ = C @ C.T # positive semi-definite

multi_normal = MultivariateNormal(Œº, Œ£)
```

```python
Œº, Œ£
```

    (array([0.19532509, 0.06772962, 0.7417879 ]),
     array([[0.42415081, 0.64271754, 0.44411893],
            [0.64271754, 2.00060339, 1.64752868],
            [0.44411893, 1.64752868, 1.45350207]]))

```python
k = 1
multi_normal.partition(k)
```

Let‚Äôs compute the distribution of $ z*1 $ conditional on $ z*{2}=\left[\begin{array}{c} 2\\ 5 \end{array}\right] $.

```python
ind = 0
z2 = np.array([2., 5.])

Œº1_hat, Œ£1_hat = multi_normal.cond_dist(ind, z2)
```

```python
n = 1_000_000
data = np.random.multivariate_normal(Œº, Œ£, size=n)
z1_data = data[:, :k]
z2_data = data[:, k:]
```

```python
Œº1, Œº2 = multi_normal.Œºs
results = sm.OLS(z1_data - Œº1, z2_data - Œº2).fit()
```

As above, we compare population and sample regression coefficients, the conditional covariance matrix, and the conditional mean vector in that order.

```python
multi_normal.Œ≤s[0], results.params
```

    (array([[ 1.04630891, -0.88042875]]), array([ 1.04766693, -0.88183286]))

```python
Œ£1_hat, results.resid @ results.resid.T / (n - 1)
```

    (array([[0.14268479]]), 0.14251853938650302)

```python
Œº1_hat, results.predict(z2 - Œº2) + Œº1
```

    (array([-1.53197553]), array([-1.53533047]))

Once again, sample analogues do a good job of approximating their populations counterparts.

## Single Dimensional Intelligence (IQ)

Let‚Äôs move closer to a real-life example, namely,... inferring a one-dimensional measure of intelligence called IQ from a list of test scores.

The $ i $th test score $ y*i $ equals the sum of an unknown scalar IQ $ \theta $ and a random variable $ w*{i} $.

$$
y_{i} = \theta + \sigma_y w_i, \quad i=1,\dots, n
$$

The distribution of IQ‚Äôs for a cross-section of people is a normal random variable described by

$$
\theta = \mu_{\theta} + \sigma_{\theta} w_{n+1}.
$$

We assume that the noises $ \{w*i\}*{i=1}^N $ in the test scores are IID and not correlated with IQ.

We also assume that $ \{w*i\}*{i=1}^{n+1} $ are i.i.d. standard normal:

$$
\boldsymbol{w}=
\left[\begin{array}{c}
w_{1}\\
w_{2}\\
\vdots\\
w_{n}\\
w_{n+1}
\end{array}\right]\sim N\left(0,I_{n+1}\right)
$$

The following system describes the $ (n+1) \times 1 $ random vector $ X $ that interests us:

$$
X=\left[\begin{array}{c}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}\\
\theta
\end{array}\right]=\left[\begin{array}{c}
\mu_{\theta}\\
\mu_{\theta}\\
\vdots\\
\mu_{\theta}\\
\mu_{\theta}
\end{array}\right]+\left[\begin{array}{ccccc}
\sigma_{y} & 0 & \cdots & 0 & \sigma_{\theta}\\
0 & \sigma_{y} & \cdots & 0 & \sigma_{\theta}\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & \cdots & \sigma_{y} & \sigma_{\theta}\\
0 & 0 & \cdots & 0 & \sigma_{\theta}
\end{array}\right]\left[\begin{array}{c}
w_{1}\\
w_{2}\\
\vdots\\
w_{n}\\
w_{n+1}
\end{array}\right],
$$

or equivalently,

$$
X=\mu_{\theta}\boldsymbol{1}_{n+1}+D\boldsymbol{w}
$$

where $ X = \begin{bmatrix} y \cr \theta \end{bmatrix} $,
$ \boldsymbol{1}\_{n+1} $ is a vector of $ 1 $s of size
$ n+1 $, and $ D $ is an $ n+1 $ by $ n+1 $ matrix.

Let‚Äôs define a Python function that constructs the mean $ \mu $ and covariance matrix $ \Sigma $ of the random vector $ X $ that we know is governed by a multivariate normal distribution.

As arguments, the function takes the number of tests $ n $, the mean $ \mu*{\theta} $ and the standard deviation $ \sigma*\theta $ of the IQ distribution, and the standard deviation of the randomness in test scores $ \sigma\_{y} $.

```python
def construct_moments_IQ(n, ŒºŒ∏, œÉŒ∏, œÉy):

    Œº_IQ = np.full(n+1, ŒºŒ∏)

    D_IQ = np.zeros((n+1, n+1))
    D_IQ[range(n), range(n)] = œÉy
    D_IQ[:, n] = œÉŒ∏

    Œ£_IQ = D_IQ @ D_IQ.T

    return Œº_IQ, Œ£_IQ, D_IQ
```

Now let‚Äôs consider a specific instance of this model.

Assume we have recorded $ 50 $ test scores and we know that $ \mu*{\theta}=100 $, $ \sigma*{\theta}=10 $, and $ \sigma\_{y}=10 $.

We can compute the mean vector and covariance matrix of $ X $ easily with our `construct_moments_IQ` function as follows.

```python
n = 50
ŒºŒ∏, œÉŒ∏, œÉy = 100., 10., 10.

Œº_IQ, Œ£_IQ, D_IQ = construct_moments_IQ(n, ŒºŒ∏, œÉŒ∏, œÉy)
Œº_IQ, Œ£_IQ, D_IQ
```

    (array([100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,
            100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,
            100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,
            100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,
            100., 100., 100., 100., 100., 100., 100.]),
     array([[200., 100., 100., ..., 100., 100., 100.],
            [100., 200., 100., ..., 100., 100., 100.],
            [100., 100., 200., ..., 100., 100., 100.],
            ...,
            [100., 100., 100., ..., 200., 100., 100.],
            [100., 100., 100., ..., 100., 200., 100.],
            [100., 100., 100., ..., 100., 100., 100.]]),
     array([[10.,  0.,  0., ...,  0.,  0., 10.],
            [ 0., 10.,  0., ...,  0.,  0., 10.],
            [ 0.,  0., 10., ...,  0.,  0., 10.],
            ...,
            [ 0.,  0.,  0., ..., 10.,  0., 10.],
            [ 0.,  0.,  0., ...,  0., 10., 10.],
            [ 0.,  0.,  0., ...,  0.,  0., 10.]]))

We can now use our `MultivariateNormal` class to construct an instance, then partition the mean vector and covariance matrix as we wish.

We want to regress IQ, the random variable $ \theta $ (_what we don‚Äôt know_), on the vector $ y $ of test scores (_what we do know_).

We choose `k=n` so that $ z*{1} = y $ and $ z*{2} = \theta $.

```python
multi_normal_IQ = MultivariateNormal(Œº_IQ, Œ£_IQ)

k = n
multi_normal_IQ.partition(k)
```

Using the generator `multivariate_normal`, we can make one draw of the random vector from our distribution and then compute the distribution of $ \theta $ conditional on our test scores.

Let‚Äôs do that and then print out some pertinent quantities.

```python
x = np.random.multivariate_normal(Œº_IQ, Œ£_IQ)
y = x[:-1] # test scores
Œ∏ = x[-1]  # IQ
```

```python
# the true value
Œ∏
```

    98.69789731490053

The method `cond_dist` takes test scores $ y $ as input and returns the conditional normal distribution of the IQ $ \theta $.

In the following code, `ind` sets the variables on the right side of the regression.

Given the way we have defined the vector $ X $, we want to set `ind=1` in order to make $ \theta $ the left side variable in the population regression.

```python
ind = 1
multi_normal_IQ.cond_dist(ind, y)
```

    (array([97.13085029]), array([[1.96078431]]))

The first number is the conditional mean $ \hat{\mu}_{\theta} $ and the second is the conditional variance $ \hat{\Sigma}_{\theta} $.

How do additional test scores affect our inferences?

To shed light on this, we compute a sequence of conditional distributions of $ \theta $ by varying the number of test scores in the conditioning set from $ 1 $ to $ n $.

We‚Äôll make a pretty graph showing how our judgment of the person‚Äôs IQ change as more test results come in.

```python
# array for containing moments
ŒºŒ∏_hat_arr = np.empty(n)
Œ£Œ∏_hat_arr = np.empty(n)

# loop over number of test scores
for i in range(1, n+1):
    # construction of multivariate normal distribution instance
    Œº_IQ_i, Œ£_IQ_i, D_IQ_i = construct_moments_IQ(i, ŒºŒ∏, œÉŒ∏, œÉy)
    multi_normal_IQ_i = MultivariateNormal(Œº_IQ_i, Œ£_IQ_i)

    # partition and compute conditional distribution
    multi_normal_IQ_i.partition(i)
    scores_i = y[:i]
    ŒºŒ∏_hat_i, Œ£Œ∏_hat_i = multi_normal_IQ_i.cond_dist(1, scores_i)

    # store the results
    ŒºŒ∏_hat_arr[i-1] = ŒºŒ∏_hat_i[0]
    Œ£Œ∏_hat_arr[i-1] = Œ£Œ∏_hat_i[0, 0]

# transform variance to standard deviation
œÉŒ∏_hat_arr = np.sqrt(Œ£Œ∏_hat_arr)
```

```python
ŒºŒ∏_hat_lower = ŒºŒ∏_hat_arr - 1.96 * œÉŒ∏_hat_arr
ŒºŒ∏_hat_higher = ŒºŒ∏_hat_arr + 1.96 * œÉŒ∏_hat_arr

plt.hlines(Œ∏, 1, n+1, ls='--', label='true $Œ∏$')
plt.plot(range(1, n+1), ŒºŒ∏_hat_arr, color='b', label='$\hat{Œº}_{Œ∏}$')
plt.plot(range(1, n+1), ŒºŒ∏_hat_lower, color='b', ls='--')
plt.plot(range(1, n+1), ŒºŒ∏_hat_higher, color='b', ls='--')
plt.fill_between(range(1, n+1), ŒºŒ∏_hat_lower, ŒºŒ∏_hat_higher,
                 color='b', alpha=0.2, label='95%')

plt.xlabel('number of test scores')
plt.ylabel('$\hat{Œ∏}$')
plt.legend()

plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/12.%20Multivariate%20Normal%20Distribution_files/12.%20Multivariate%20Normal%20Distribution_65_0.png)
</div>

The solid blue line in the plot above illustrates $ \hat{\mu}_{\theta} $ as a function of the number of test scores that have been recorded and considered. The blue area represents the range obtained by adding or subtracting $ 1.96 \hat{\sigma}_{\theta} $ from $ \hat{\mu}_{\theta} $. This indicates that $ 95\% $ of the probability mass of the conditional distribution is within this range. The black dotted line shows the value of the random $ \theta $ that was drawn. As more test scores are collected, our estimate of the person‚Äôs $ \theta $ becomes increasingly accurate. Observing the shifts in the conditional distributions, we see that adding more test scores causes $ \hat{\theta} $ to stabilize and approach $ \theta $. Each $ y_{i} $ provides additional information about $ \theta $. If the number of tests $ n \rightarrow + \infty $, the conditional standard deviation $ \hat{\sigma}\_{\theta} $ would converge to $ 0 $ at a rate of $ \frac{1}{n^{.5}} $.

## Information as Surprise

Using a different framework, let‚Äôs examine things from another angle. We can represent the random vector $ X $ as

$$
X = \mu_{\theta} \boldsymbol{1}_{n+1} + C \epsilon, \quad \epsilon \sim N\left(0, I\right)
$$

where $ C $ is a lower triangular **Cholesky factor** of $ \Sigma $, so that

$$
\Sigma \equiv DD^{\prime} = C C^\prime
$$

and

$$
E \epsilon \epsilon' = I .
$$

It follows that

$$
\epsilon \sim N(0, I) .
$$

Let $ G=C^{-1} $. $ G $ is also lower triangular.

We can compute $ \epsilon $ using the formula

$$
\epsilon = G \left( X - \mu_{\theta} \boldsymbol{1}_{n+1} \right)
$$

This formula confirms that the orthonormal vector $ \epsilon $ contains the same information as the non-orthogonal vector $ \left( X - \mu*{\theta} \boldsymbol{1}*{n+1} \right) $. We can say that $ \epsilon $ is an orthogonal basis for $ \left( X - \mu*{\theta} \boldsymbol{1}*{n+1} \right) $. Let $ c\_{i} $ be the $ i $th element in the last row of $ C $. Then we can write

$$
\theta = \mu_{\theta} + c_1 \epsilon_1 + c_2 \epsilon_2 + \dots + c_n \epsilon_n + c_{n+1} \epsilon_{n+1} \tag{12.1}
$$

The mutual orthogonality of the $ \epsilon_i $‚Äôs provides us with an informative way to interpret them in light of equation [(12.1)](#equation-mnv-1). Thus, relative to what is known from tests $ i=1, \ldots, n-1 $, $ c_i \epsilon_i $ is the amount of **new information** about $ \theta $ brought by the test number $ i $. Here **new information** means **surprise** or what could not be predicted from earlier information. Formula [(12.1)](#equation-mnv-1) also provides us with an enlightening way to express conditional means and conditional variances that we computed earlier. In particular,

$$
E\left[\theta \mid y_1, \dots, y_k\right] = \mu_{\theta} + c_1 \epsilon_1 + \dots + c_k \epsilon_k
$$

and

$$
Var\left(\theta \mid y_1, \dots, y_k\right) = c^2_{k+1} + c^2_{k+2} + \dots + c^2_{n+1}.
$$

```python
C = np.linalg.cholesky(Œ£_IQ)
G = np.linalg.inv(C)

Œµ = G @ (x - ŒºŒ∏)
```

```python
cŒµ = C[n, :] * Œµ

# compute the sequence of ŒºŒ∏ and Œ£Œ∏ conditional on y1, y2, ..., yk
ŒºŒ∏_hat_arr_C = np.array([np.sum(cŒµ[:k+1]) for k in range(n)]) + ŒºŒ∏
Œ£Œ∏_hat_arr_C = np.array([np.sum(C[n, i+1:n+1] ** 2) for i in range(n)])
```

To verify that these formulas yield the same results as our earlier calculations, we can compare the means and variances of $ \theta $ conditional on $ \{y*i\}*{i=1}^k $ with what we obtained previously using the formulas implemented in the class `MultivariateNormal` based on our original representation of conditional distributions for multivariate normal distributions.

```python
# conditional mean
np.max(np.abs(ŒºŒ∏_hat_arr - ŒºŒ∏_hat_arr_C)) < 1e-10
```

    True

```python
# conditional variance
np.max(np.abs(Œ£Œ∏_hat_arr - Œ£Œ∏_hat_arr_C)) < 1e-10
```

    True

## Cholesky Factor Magic

Evidently, the Cholesky factorizations automatically compute the population **regression coefficients** and associated statistics that are produced by our `MultivariateNormal` class.

The Cholesky factorization computes these things **recursively**.

Indeed, in formula [(12.1)](#equation-mnv-1),

- the random variable $ c*i \epsilon_i $ represents information about $ \theta $ that is not contained in the information from $ \epsilon_1, \epsilon_2, \ldots, \epsilon*{i-1} $
- the coefficient $ c*i $ is the simple population regression coefficient of $ \theta - \mu*\theta $ on $ \epsilon_i $

## Math and Verbal Intelligence

We can modify the previous example to be more realistic.

There is substantial evidence that IQ is not a scalar.

Some individuals excel in mathematical skills but struggle with language skills.

Others excel in language skills but struggle with mathematical skills.

So now we'll assume that there are two dimensions of IQ, $ \theta $ and $ \eta $.

These determine average performances in math and language tests, respectively.

We observe math scores $ \{y*i\}*{i=1}^{n} $ and language scores $ \{y*i\}*{i=n+1}^{2n} $.

When $ n=2 $, we assume that outcomes are draws from a multivariate normal distribution with representation

$$
X=\left[\begin{array}{c}
y_{1}\\
y_{2}\\
y_{3}\\
y_{4}\\
\theta\\
\eta
\end{array}\right]=\left[\begin{array}{c}
\mu_{\theta}\\
\mu_{\theta}\\
\mu_{\eta}\\
\mu_{\eta}\\
\mu_{\theta}\\
\mu_{\eta}
\end{array}\right]+\left[\begin{array}{cccccc}
\sigma_{y} & 0 & 0 & 0 & \sigma_{\theta} & 0\\
0 & \sigma_{y} & 0 & 0 & \sigma_{\theta} & 0\\
0 & 0 & \sigma_{y} & 0 & 0 & \sigma_{\eta}\\
0 & 0 & 0 & \sigma_{y} & 0 & \sigma_{\eta}\\
0 & 0 & 0 & 0 & \sigma_{\theta} & 0\\
0 & 0 & 0 & 0 & 0 & \sigma_{\eta}
\end{array}\right]\left[\begin{array}{c}
w_{1}\\
w_{2}\\
w_{3}\\
w_{4}\\
w_{5}\\
w_{6}
\end{array}\right]
$$

where $ w \begin{bmatrix} w_1 \cr w_2 \cr \vdots \cr w_6 \end{bmatrix} $ is a standard normal random vector.

We construct a Python function `construct_moments_IQ2d` to construct the mean vector and covariance matrix of the joint normal distribution.

```python
def construct_moments_IQ2d(n, ŒºŒ∏, œÉŒ∏, ŒºŒ∑, œÉŒ∑, œÉy):

    Œº_IQ2d = np.empty(2*(n+1))
    Œº_IQ2d[:n] = ŒºŒ∏
    Œº_IQ2d[2*n] = ŒºŒ∏
    Œº_IQ2d[n:2*n] = ŒºŒ∑
    Œº_IQ2d[2*n+1] = ŒºŒ∑


    D_IQ2d = np.zeros((2*(n+1), 2*(n+1)))
    D_IQ2d[range(2*n), range(2*n)] = œÉy
    D_IQ2d[:n, 2*n] = œÉŒ∏
    D_IQ2d[2*n, 2*n] = œÉŒ∏
    D_IQ2d[n:2*n, 2*n+1] = œÉŒ∑
    D_IQ2d[2*n+1, 2*n+1] = œÉŒ∑

    Œ£_IQ2d = D_IQ2d @ D_IQ2d.T

    return Œº_IQ2d, Œ£_IQ2d, D_IQ2d
```

Let's put the function to work.

```python
n = 2
# mean and variance of Œ∏, Œ∑, and y
ŒºŒ∏, œÉŒ∏, ŒºŒ∑, œÉŒ∑, œÉy = 100., 10., 100., 10, 10

Œº_IQ2d, Œ£_IQ2d, D_IQ2d = construct_moments_IQ2d(n, ŒºŒ∏, œÉŒ∏, ŒºŒ∑, œÉŒ∑, œÉy)
Œº_IQ2d, Œ£_IQ2d, D_IQ2d
```

    (array([100., 100., 100., 100., 100., 100.]),
     array([[200., 100.,   0.,   0., 100.,   0.],
            [100., 200.,   0.,   0., 100.,   0.],
            [  0.,   0., 200., 100.,   0., 100.],
            [  0.,   0., 100., 200.,   0., 100.],
            [100., 100.,   0.,   0., 100.,   0.],
            [  0.,   0., 100., 100.,   0., 100.]]),
     array([[10.,  0.,  0.,  0., 10.,  0.],
            [ 0., 10.,  0.,  0., 10.,  0.],
            [ 0.,  0., 10.,  0.,  0., 10.],
            [ 0.,  0.,  0., 10.,  0., 10.],
            [ 0.,  0.,  0.,  0., 10.,  0.],
            [ 0.,  0.,  0.,  0.,  0., 10.]]))

```python
# take one draw
x = np.random.multivariate_normal(Œº_IQ2d, Œ£_IQ2d)
y1 = x[:n]
y2 = x[n:2*n]
Œ∏ = x[2*n]
Œ∑ = x[2*n+1]

# the true values
Œ∏, Œ∑
```

    (106.38184976589969, 91.3683091254041)

We first compute the joint normal distribution of $ \left(\theta, \eta\right) $.

```python
multi_normal_IQ2d = MultivariateNormal(Œº_IQ2d, Œ£_IQ2d)

k = 2*n # the length of data vector
multi_normal_IQ2d.partition(k)

multi_normal_IQ2d.cond_dist(1, [*y1, *y2])
```

    (array([110.51071053, 100.07240215]),
     array([[33.33333333,  0.        ],
            [ 0.        , 33.33333333]]))

Now let's compute distributions of $ \theta $ and $ \mu $ separately conditional on various subsets of test scores.

It will be interesting to compare outcomes with the help of an auxiliary function `cond_dist_IQ2d` that we now construct.

```python
def cond_dist_IQ2d(Œº, Œ£, data):

    n = len(Œº)

    multi_normal = MultivariateNormal(Œº, Œ£)
    multi_normal.partition(n-1)
    Œº_hat, Œ£_hat = multi_normal.cond_dist(1, data)

    return Œº_hat, Œ£_hat
```

Let's see how things work for an example.

```python
for indices, IQ, conditions in [([*range(2*n), 2*n], 'Œ∏', 'y1, y2, y3, y4'),
                                ([*range(n), 2*n], 'Œ∏', 'y1, y2'),
                                ([*range(n, 2*n), 2*n], 'Œ∏', 'y3, y4'),
                                ([*range(2*n), 2*n+1], 'Œ∑', 'y1, y2, y3, y4'),
                                ([*range(n), 2*n+1], 'Œ∑', 'y1, y2'),
                                ([*range(n, 2*n), 2*n+1], 'Œ∑', 'y3, y4')]:

    Œº_hat, Œ£_hat = cond_dist_IQ2d(Œº_IQ2d[indices], Œ£_IQ2d[indices][:, indices], x[indices[:-1]])
    print(f'The mean and variance of {IQ} conditional on {conditions: <15} are ' +
          f'{Œº_hat[0]:1.2f} and {Œ£_hat[0, 0]:1.2f} respectively')
```

    The mean and variance of Œ∏ conditional on y1, y2, y3, y4  are 110.51 and 33.33 respectively
    The mean and variance of Œ∏ conditional on y1, y2          are 110.51 and 33.33 respectively
    The mean and variance of Œ∏ conditional on y3, y4          are 100.00 and 100.00 respectively
    The mean and variance of Œ∑ conditional on y1, y2, y3, y4  are 100.07 and 33.33 respectively
    The mean and variance of Œ∑ conditional on y1, y2          are 100.00 and 100.00 respectively
    The mean and variance of Œ∑ conditional on y3, y4          are 100.07 and 33.33 respectively

Evidently, math tests provide no information about $ \mu $ and language tests provide no information about $ \eta $.

## Univariate Time Series Analysis

We can utilize the multivariate normal distribution along with some matrix algebra to lay the groundwork for univariate linear time series analysis.

Let $ x*t, y_t, v_t, w*{t+1} $ each be scalars for $ t \geq 0 $.

Consider the following model:

$$
\begin{aligned}
x_0 & \sim N\left(0, \sigma_0^2\right) \\
x_{t+1} & = a x_{t} + b w_{t+1}, \quad w_{t+1} \sim N\left(0, 1\right), t \geq 0 \\
y_{t} & = c x_{t} + d v_{t}, \quad v_{t} \sim N\left(0, 1\right), t \geq 0
\end{aligned}
$$

We can compute the moments of $ x\_{t} $:

1. $ E x*{t+1}^2 = a^2 E x*{t}^2 + b^2, t \geq 0 $, where $ E x*{0}^2 = \sigma*{0}^2 $
2. $ E x*{t+j} x*{t} = a^{j} E x\_{t}^2, \forall t \ \forall j $

Given some $ T $, we can formulate the sequence $ \{x*{t}\}*{t=0}^T $ as a random vector

$$
X=\left[\begin{array}{c}
x_{0}\\
x_{1}\\
\vdots\\
x_{T}
\end{array}\right]
$$

and the covariance matrix $ \Sigma\_{x} $ can be constructed using the moments we have computed above.

Similarly, we can define

$$
Y=\left[\begin{array}{c}
y_{0}\\
y_{1}\\
\vdots\\
y_{T}
\end{array}\right], \quad
v=\left[\begin{array}{c}
v_{0}\\
v_{1}\\
\vdots\\
v_{T}
\end{array}\right]
$$

and therefore

$$
Y = C X + D V
$$

where $ C $ and $ D $ are both diagonal matrices with constant $ c $ and $ d $ as diagonal respectively.

Consequently, the covariance matrix of $ Y $ is

$$
\Sigma_{y} = E Y Y^{\prime} = C \Sigma_{x} C^{\prime} + D D^{\prime}
$$

By stacking $ X $ and $ Y $, we can write

$$
Z=\left[\begin{array}{c}
X\\
Y
\end{array}\right]
$$

and

$$
\Sigma_{z} = EZZ^{\prime}=\left[\begin{array}{cc}
\Sigma_{x} & \Sigma_{x}C^{\prime}\\
C\Sigma_{x} & \Sigma_{y}
\end{array}\right]
$$

Thus, the stacked sequences $ \{x*{t}\}*{t=0}^T $ and $ \{y*{t}\}*{t=0}^T $ jointly follow the multivariate normal distribution $ N\left(0, \Sigma\_{z}\right) $.

```python
# as an example, consider the case where T = 3
T = 3
```

```python
# variance of the initial distribution x_0
œÉ0 = 1.

# parameters of the equation system
a = .9
b = 1.
c = 1.0
d = .05
```

```python
# construct the covariance matrix of X
Œ£x = np.empty((T+1, T+1))

Œ£x[0, 0] = œÉ0 ** 2
for i in range(T):
    Œ£x[i, i+1:] = Œ£x[i, i] * a ** np.arange(1, T+1-i)
    Œ£x[i+1:, i] = Œ£x[i, i+1:]

    Œ£x[i+1, i+1] = a ** 2 * Œ£x[i, i] + b ** 2
```

```python
Œ£x
```

    array([[1.      , 0.9     , 0.81    , 0.729   ],
           [0.9     , 1.81    , 1.629   , 1.4661  ],
           [0.81    , 1.629   , 2.4661  , 2.21949 ],
           [0.729   , 1.4661  , 2.21949 , 2.997541]])

```python
# construct the covariance matrix of Y
C = np.eye(T+1) * c
D = np.eye(T+1) * d

Œ£y = C @ Œ£x @ C.T + D @ D.T
```

```python
# construct the covariance matrix of Z
Œ£z = np.empty((2*(T+1), 2*(T+1)))

Œ£z[:T+1, :T+1] = Œ£x
Œ£z[:T+1, T+1:] = Œ£x @ C.T
Œ£z[T+1:, :T+1] = C @ Œ£x
Œ£z[T+1:, T+1:] = Œ£y
```

```python
Œ£z
```

    array([[1.      , 0.9     , 0.81    , 0.729   , 1.      , 0.9     ,
            0.81    , 0.729   ],
           [0.9     , 1.81    , 1.629   , 1.4661  , 0.9     , 1.81    ,
            1.629   , 1.4661  ],
           [0.81    , 1.629   , 2.4661  , 2.21949 , 0.81    , 1.629   ,
            2.4661  , 2.21949 ],
           [0.729   , 1.4661  , 2.21949 , 2.997541, 0.729   , 1.4661  ,
            2.21949 , 2.997541],
           [1.      , 0.9     , 0.81    , 0.729   , 1.0025  , 0.9     ,
            0.81    , 0.729   ],
           [0.9     , 1.81    , 1.629   , 1.4661  , 0.9     , 1.8125  ,
            1.629   , 1.4661  ],
           [0.81    , 1.629   , 2.4661  , 2.21949 , 0.81    , 1.629   ,
            2.4686  , 2.21949 ],
           [0.729   , 1.4661  , 2.21949 , 2.997541, 0.729   , 1.4661  ,
            2.21949 , 3.000041]])

```python
# construct the mean vector of Z
Œºz = np.zeros(2*(T+1))
```

The following Python code allows us to sample random vectors $ X $ and $ Y $. This will be quite useful for performing the conditioning required in the exercises below.

```python
z = np.random.multivariate_normal(Œºz, Œ£z)

x = z[:T+1]
y = z[T+1:]
```

### Smoothing Example

This is an instance of a classic `smoothing` calculation aimed at computing $ E X \mid Y $.

An interpretation of this example is:

- $ X $ represents a random sequence of hidden Markov state variables $ x_t $
- $ Y $ is a sequence of observed signals $ y_t $ that provide information about the hidden state

```python
# construct a MultivariateNormal instance
multi_normal_ex1 = MultivariateNormal(Œºz, Œ£z)
x = z[:T+1]
y = z[T+1:]
```

```python
# partition Z into X and Y
multi_normal_ex1.partition(T+1)
```

```python
# compute the conditional mean and covariance matrix of X given Y=y

print("X = ", x)
print("Y = ", y)
print(" E [ X | Y] = ", )

multi_normal_ex1.cond_dist(0, y)
```

    X =  [ 1.60655859 -0.68558551 -0.09687098 -0.65675348]
    Y =  [ 1.5831203  -0.6999392  -0.16124751 -0.64514194]
     E [ X | Y] =





    (array([ 1.57443532, -0.69362597, -0.16351703, -0.64390011]),
     array([[2.48875094e-03, 5.57449314e-06, 1.24861718e-08, 2.80231394e-11],
            [5.57449314e-06, 2.48876343e-03, 5.57452116e-06, 1.25113937e-08],
            [1.24861718e-08, 5.57452116e-06, 2.48876346e-03, 5.58575339e-06],
            [2.80232504e-11, 1.25113935e-08, 5.58575339e-06, 2.49377812e-03]]))

### Filtering Exercise

Compute $ E\left[x_{t} \mid y_{t-1}, y_{t-2}, \dots, y_{0}\right] $.

To do this, we first need to construct the mean vector and the covariance matrix of the subvector $ \left[x_{t}, y_{0}, \dots, y_{t-2}, y_{t-1}\right] $.

For example, let‚Äôs say that we want the conditional distribution of $ x\_{3} $.

```python
t = 3
```

```python
# mean of the subvector
sub_Œºz = np.zeros(t+1)

# covariance matrix of the subvector
sub_Œ£z = np.empty((t+1, t+1))

sub_Œ£z[0, 0] = Œ£z[t, t] # x_t
sub_Œ£z[0, 1:] = Œ£z[t, T+1:T+t+1]
sub_Œ£z[1:, 0] = Œ£z[T+1:T+t+1, t]
sub_Œ£z[1:, 1:] = Œ£z[T+1:T+t+1, T+1:T+t+1]
```

```python
sub_Œ£z
```

    array([[2.997541, 0.729   , 1.4661  , 2.21949 ],
           [0.729   , 1.0025  , 0.9     , 0.81    ],
           [1.4661  , 0.9     , 1.8125  , 1.629   ],
           [2.21949 , 0.81    , 1.629   , 2.4686  ]])

```python
multi_normal_ex2 = MultivariateNormal(sub_Œºz, sub_Œ£z)
multi_normal_ex2.partition(1)
```

```python
sub_y = y[:t]

multi_normal_ex2.cond_dist(0, sub_y)
```

    (array([-0.14616194]), array([[1.00201996]]))

### Prediction Exercise

Compute $ E\left[y_{t} \mid y_{t-j}, \dots, y_{0} \right] $.

As in exercise 2, we will construct the mean vector and covariance matrix of the subvector $ \left[y_{t}, y_{0}, \dots, y_{t-j-1}, y_{t-j} \right] $.

For instance, consider a scenario where $ t=3 $ and $ j=2 $.

```python
t = 3
j = 2
```

```python
sub_Œºz = np.zeros(t-j+2)
sub_Œ£z = np.empty((t-j+2, t-j+2))

sub_Œ£z[0, 0] = Œ£z[T+t+1, T+t+1]
sub_Œ£z[0, 1:] = Œ£z[T+t+1, T+1:T+t-j+2]
sub_Œ£z[1:, 0] = Œ£z[T+1:T+t-j+2, T+t+1]
sub_Œ£z[1:, 1:] = Œ£z[T+1:T+t-j+2, T+1:T+t-j+2]
```

```python
sub_Œ£z
```

    array([[3.000041, 0.729   , 1.4661  ],
           [0.729   , 1.0025  , 0.9     ],
           [1.4661  , 0.9     , 1.8125  ]])

```python
multi_normal_ex3 = MultivariateNormal(sub_Œºz, sub_Œ£z)
multi_normal_ex3.partition(1)
```

```python
sub_y = y[:t-j+1]

multi_normal_ex3.cond_dist(0, sub_y)
```

    (array([-0.56267466]), array([[1.81413617]]))

### Constructing a Wold Representation

Now we‚Äôll apply Cholesky decomposition to decompose $ \Sigma\_{y}=H H^{\prime} $ and form

$$
\epsilon = H^{-1} Y.
$$

Then we can represent $ y\_{t} $ as

$$
y_{t} = h_{t,t} \epsilon_{t} + h_{t,t-1} \epsilon_{t-1} + \dots + h_{t,0} \epsilon_{0}.
$$

```python
H = np.linalg.cholesky(Œ£y)

H
```

    array([[1.00124922, 0.        , 0.        , 0.        ],
           [0.8988771 , 1.00225743, 0.        , 0.        ],
           [0.80898939, 0.89978675, 1.00225743, 0.        ],
           [0.72809046, 0.80980808, 0.89978676, 1.00225743]])

```python
Œµ = np.linalg.inv(H) @ y

Œµ
```

    array([ 1.5811451 , -2.11641667,  0.46290159, -0.49785613])

```python
y
```

    array([ 1.5831203 , -0.6999392 , -0.16124751, -0.64514194])

This example is an instance of what is known as a **Wold representation** in time series analysis.

## Stochastic Difference Equation

Consider the stochastic second-order linear difference equation

$$
y_{t} = \alpha_{0} + \alpha_{1} y_{t-1} + \alpha_{2} y_{t-2} + u_{t}
$$

where $ u*{t} \sim N \left(0, \sigma*{u}^{2}\right) $ and

$$
\left[\begin{array}{c}
y_{-1}\\
y_{0}
\end{array}\right]\sim N\left(\mu_{\tilde{y}},\Sigma_{\tilde{y}}\right)
$$

It can be expressed as a stacked system

$$
\underset{\equiv A}{\underbrace{\left[\begin{array}{cccccccc}
1 & 0 & 0 & 0 & \cdots & 0 & 0 & 0\\
-\alpha_{1} & 1 & 0 & 0 & \cdots & 0 & 0 & 0\\
-\alpha_{2} & -\alpha_{1} & 1 & 0 & \cdots & 0 & 0 & 0\\
0 & -\alpha_{2} & -\alpha_{1} & 1 & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \cdots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & 0 & \cdots & -\alpha_{2} & -\alpha_{1} & 1
\end{array}\right]}}\left[\begin{array}{c}
y_{1}\\
y_{2}\\
y_{3}\\
y_{4}\\
\vdots\\
y_{T}
\end{array}\right]=\underset{\equiv b}{\underbrace{\left[\begin{array}{c}
\alpha_{0}+\alpha_{1}y_{0}+\alpha_{2}y_{-1}\\
\alpha_{0}+\alpha_{2}y_{0}\\
\alpha_{0}\\
\alpha_{0}\\
\vdots\\
\alpha_{0}
\end{array}\right]}} +\underset{\equiv u}{\underbrace{\left[\begin{array}{c}
u_{1} \\
u_2 \\
u_3\\
u_4\\
\vdots\\
u_T
\end{array}\right]}}
$$

We can compute $ y $ by solving the system

$$
y = A^{-1} \left(b + u\right)
$$

We have

$$
\begin{aligned}
\mu_{y} = A^{-1} \mu_{b} \\
\Sigma_{y} &= A^{-1} E \left[\left(b - \mu_{b} + u \right) \left(b - \mu_{b} + u \right)^{\prime}\right] \left(A^{-1}\right)^{\prime} \\
&= A^{-1} \left(\Sigma_{b} + \Sigma_{u} \right) \left(A^{-1}\right)^{\prime}
\end{aligned}
$$

where

$$
\mu_{b}=\left[\begin{array}{c}
\alpha_{0}+\alpha_{1}\mu_{y_{0}}+\alpha_{2}\mu_{y_{-1}}\\
\alpha_{0}+\alpha_{2}\mu_{y_{0}}\\
\alpha_{0}\\
\vdots\\
\alpha_{0}
\end{array}\right]
$$

$$
\Sigma_{b}=\left[\begin{array}{cc}
C\Sigma_{\tilde{y}}C^{\prime} & \boldsymbol{0}_{N-2\times N-2}\\
\boldsymbol{0}_{N-2\times2} & \boldsymbol{0}_{N-2\times N-2}
\end{array}\right]
$$

```python
# set parameters
T = 80
T = 160
# coefficients of the second order difference equation
ùõº0 = 10
ùõº1 = 1.53
ùõº2 = -.9

# variance of u
œÉu = 1.
œÉu = 10.

# distribution of y_{-1} and y_{0}
Œºy_tilde = np.array([1., 0.5])
Œ£y_tilde = np.array([[2., 1.], [1., 0.5]])
```

```python
# construct A and A^{\prime}
A = np.zeros((T, T))

for i in range(T):
    A[i, i] = 1

    if i-1 >= 0:
        A[i, i-1] = -ùõº1

    if i-2 >= 0:
        A[i, i-2] = -ùõº2

A_inv = np.linalg.inv(A)
```

```python
# compute the mean vectors of b and y
Œºb = np.full(T, ùõº0)
Œºb[0] += ùõº1 * Œºy_tilde[1] + ùõº2 * Œºy_tilde[0]
Œºb[1] += ùõº2 * Œºy_tilde[1]

Œºy = A_inv @ Œºb
```

```python
# compute the covariance matrices of b and y
Œ£u = np.eye(T) * œÉu ** 2

Œ£b = np.zeros((T, T))

C = np.array([[ùõº2, ùõº1], [0, ùõº2]])
Œ£b[:2, :2] = C @ Œ£y_tilde @ C.T

Œ£y = A_inv @ (Œ£b + Œ£u) @ A_inv.T
```

## Application to Stock Price Model

Let

$$
p_{t} = \sum_{j=0}^{T-t} \beta^{j} y_{t+j}
$$

Form

$$
\underset{\equiv p}{\underbrace{\left[\begin{array}{c}
p_{1}\\
p_{2}\\
p_{3}\\
\vdots\\
p_{T}
\end{array}\right]}}=\underset{\equiv B}{\underbrace{\left[\begin{array}{ccccc}
1 & \beta & \beta^{2} & \cdots & \beta^{T-1}\\
0 & 1 & \beta & \cdots & \beta^{T-2}\\
0 & 0 & 1 & \cdots & \beta^{T-3}\\
\vdots & \vdots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & 1
\end{array}\right]}}\left[\begin{array}{c}
y_{1}\\
y_{2}\\
y_{3}\\
\vdots\\
y_{T}
\end{array}\right]
$$

we have

$$
\begin{aligned}
\mu_{p} = B \mu_{y} \\
\Sigma_{p} = B \Sigma_{y} B^{\prime}
\end{aligned}
$$

```python
Œ≤ = .96
```

```python
# construct B
B = np.zeros((T, T))

for i in range(T):
    B[i, i:] = Œ≤ ** np.arange(0, T-i)
```

Denote

$$
z=\left[\begin{array}{c}
y\\
p
\end{array}\right]=\underset{\equiv D}{\underbrace{\left[\begin{array}{c}
I\\
B
\end{array}\right]}} y
$$

Thus, $ \{y*t\}*{t=1}^{T} $ and $ \{p*t\}*{t=1}^{T} $ jointly follow the multivariate normal distribution $ N \left(\mu*{z}, \Sigma*{z}\right) $, where

$$
\mu_{z}=D\mu_{y}
$$

$$
\Sigma_{z}=D\Sigma_{y}D^{\prime}
$$

```python
D = np.vstack([np.eye(T), B])
```

```python
Œºz = D @ Œºy
Œ£z = D @ Œ£y @ D.T
```

We can simulate paths of $ y*{t} $ and $ p*{t} $ and compute the conditional mean $ E \left[p_{t} \mid y_{t-1}, y_{t}\right] $ using the `MultivariateNormal` class.

```python
z = np.random.multivariate_normal(Œºz, Œ£z)
y, p = z[:T], z[T:]
```

```python
cond_Ep = np.empty(T-1)

sub_Œº = np.empty(3)
sub_Œ£ = np.empty((3, 3))
for t in range(2, T+1):
    sub_Œº[:] = Œºz[[t-2, t-1, T-1+t]]
    sub_Œ£[:, :] = Œ£z[[t-2, t-1, T-1+t], :][:, [t-2, t-1, T-1+t]]

    multi_normal = MultivariateNormal(sub_Œº, sub_Œ£)
    multi_normal.partition(2)

    cond_Ep[t-2] = multi_normal.cond_dist(1, y[t-2:t])[0][0]
```

```python
plt.plot(range(1, T), y[1:], label='$y_{t}$')
plt.plot(range(1, T), y[:-1], label='$y_{t-1}$')
plt.plot(range(1, T), p[1:], label='$p_{t}$')
plt.plot(range(1, T), cond_Ep, label='$Ep_{t}|y_{t}, y_{t-1}$')

plt.xlabel('t')
plt.legend(loc=1)
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/12.%20Multivariate%20Normal%20Distribution_files/12.%20Multivariate%20Normal%20Distribution_132_0.png)
</div>

In the above graph, the green line represents what the stock price would be if people had perfect foresight about the path of dividends. The green line is the conditional expectation $ E p*t | y_t, y*{t-1} $, which represents the price if people optimally predict future dividends based on the information $ y*t, y*{t-1} $ at time $ t $.

## Filtering Foundations

Assume that $ x*0 $ is an $ n \times 1 $ random vector and that $ y_0 $ is a $ p \times 1 $ random vector determined by the \_observation equation*

$$
y_0 = G x_0 + v_0 , \quad x_0 \sim {\mathcal N}(\hat x_0, \Sigma_0), \quad v_0 \sim {\mathcal N}(0, R)
$$

where $ v_0 $ is orthogonal to $ x_0 $, $ G $ is a $ p \times n $ matrix, and $ R $ is a $ p \times p $ positive definite matrix.

We consider the problem of someone who

- _observes_ $ y_0 $
- does not observe $ x_0 $,
- knows $ \hat x_0, \Sigma_0, G, R $ and therefore the joint probability distribution of the vector $ \begin{bmatrix} x_0 \cr y_0 \end{bmatrix} $
- wants to infer $ x_0 $ from $ y_0 $ in light of what he knows about that joint probability distribution.

Therefore, the person wants to construct the probability distribution of $ x_0 $ conditional on the random vector $ y_0 $.

The joint distribution of $ \begin{bmatrix} x_0 \cr y_0 \end{bmatrix} $ is multivariate normal $ {\mathcal N}(\mu, \Sigma) $ with

$$
\mu = \begin{bmatrix} \hat x_0 \cr G \hat x_0 \end{bmatrix} , \quad
\Sigma = \begin{bmatrix} \Sigma_0 & \Sigma_0 G' \cr
G \Sigma_0 & G \Sigma_0 G' + R \end{bmatrix}
$$

By applying an appropriate instance of the above formulas for the mean vector $ \hat \mu*1 $ and covariance matrix $ \hat \Sigma*{11} $ of $ z_1 $ conditional on $ z_2 $, we find that the probability distribution of $ x_0 $ conditional on $ y_0 $ is $ {\mathcal N}(\tilde x_0, \tilde \Sigma_0) $ where

$$
\begin{aligned} \beta_0 & = \Sigma_0 G' (G \Sigma_0 G' + R)^{-1} \cr
\tilde x_0 & = \hat x_0 + \beta_0 ( y_0 - G \hat x_0) \cr
\tilde \Sigma_0 & = \Sigma_0 - \Sigma_0 G' (G \Sigma_0 G' + R)^{-1} G \Sigma_0
\end{aligned}
$$

We can express our finding that the probability distribution of $ x_0 $ conditional on $ y_0 $ is $ {\mathcal N}(\tilde x_0, \tilde \Sigma_0) $ by representing $ x_0 $ as

$$
x_0 = \tilde x_0 + \zeta_0 \tag{12.2}
$$

where $ \zeta_0 $ is a Gaussian random vector that is orthogonal to $ \tilde x_0 $ and $ y_0 $ and that has mean vector $ 0 $ and conditional covariance matrix $ E [\zeta_0 \zeta_0' | y_0] = \tilde \Sigma_0 $.

### Step toward dynamics

Now assume we are in a time series context with a one-step state transition equation

$$
x_1 = A x_0 + C w_1 , \quad w_1 \sim {\mathcal N}(0, I )
$$

where $ A $ is an $ n \times n $ matrix and $ C $ is an $ n \times m $ matrix.

Using equation [(12.2)](#equation-eq-x0rep2), we can represent $ x_1 $ as

$$
x_1 = A (\tilde x_0 + \zeta_0) + C w_1
$$

Consequently,

$$
E x_1 | y_0 = A \tilde x_0
$$

and the corresponding conditional covariance matrix $ E (x_1 - E x_1| y_0) (x_1 - E x_1| y_0)' \equiv \Sigma_1 $ is

$$
\Sigma_1 = A \tilde \Sigma_0 A' + C C'
$$

or

$$
\Sigma_1 = A \Sigma_0 A' - A \Sigma_0 G' (G \Sigma_0 G' + R)^{-1} G \Sigma_0 A'
$$

We can express the mean of $ x_1 $ conditional on $ y_0 $ as

$$
\hat x_1 = A \hat x_0 + A \Sigma_0 G' (G \Sigma_0 G' + R)^{-1} (y_0 - G \hat x_0)
$$

or

$$
\hat x_1 = A \hat x_0 + K_0 (y_0 - G \hat x_0)
$$

where

$$
K_0 = A \Sigma_0 G' (G \Sigma_0 G' + R)^{-1}
$$

### Dynamic version

Suppose now that for $ t \geq 0 $, $ \{x*{t+1}, y_t\}*{t=0}^\infty $ are governed by the equations

$$
\begin{aligned}
x_{t+1} & = A x_t + C w_{t+1} \cr
y_t & = G x_t + v_t
\end{aligned}
$$

where as before $ x*0 \sim {\mathcal N}(\hat x_0, \Sigma_0) $, $ w*{t+1} $ is the $ t+1 $th component of an i.i.d. stochastic process distributed as $ w*{t+1} \sim {\mathcal N}(0, I) $, and $ v_t $ is the $ t $th component of an i.i.d. process distributed as $ v_t \sim {\mathcal N}(0, R) $ and the $ \{w*{t+1}\}_{t=0}^\infty $ and $ \{v_t\}_{t=0}^\infty $ processes are orthogonal at all pairs of dates.

The logic and formulas we applied earlier imply that the probability distribution of $ x*t $ conditional on $ y_0, y_1, \ldots , y*{t-1} = y^{t-1} $ is

$$
x_t | y^{t-1} \sim {\mathcal N}(A \tilde x_t , A \tilde \Sigma_t A' + C C' )
$$

where $ \{\tilde x*t, \tilde \Sigma_t\}*{t=1}^\infty $ can be computed by iterating on the following equations starting from $ t=1 $ and initial conditions for $ \tilde x_0, \tilde \Sigma_0 $ computed as we have above:

$$
\begin{aligned} \Sigma_t & = A \tilde \Sigma_{t-1} A' + C C' \cr
\hat x_t & = A \tilde x_{t-1} \cr
\beta_t & = \Sigma_t G' (G \Sigma_t G' + R)^{-1} \cr
\tilde x_t & = \hat x_t + \beta_t ( y_t - G \hat x_t) \cr
\tilde \Sigma_t & = \Sigma_t - \Sigma_t G' (G \Sigma_t G' + R)^{-1} G \Sigma_t
\end{aligned}
$$

If we shift the first equation forward one period and then substitute the expression for $ \tilde \Sigma_t $ on the right side of the fifth equation into it we obtain

$$
\Sigma_{t+1}= C C' + A \Sigma_t A' - A \Sigma_t G' (G \Sigma_t G' +R)^{-1} G \Sigma_t A' .
$$

This is a matrix Riccati difference equation that is closely related to another matrix Riccati difference equation that appears in a quantecon lecture on the basics of linear quadratic control theory.

That equation has the form

$$
P_{t-1} =R + A' P_t A - A' P_t B (B' P_t B + Q)^{-1} B' P_t A .
$$

Examine the two preceding equations for a moment. The first is a matrix difference equation for a conditional covariance matrix, while the second is a matrix difference equation in the matrix appearing in a quadratic form for an intertemporal cost of value function.

Although not identical, these equations display striking family resemblances.

- The first equation describes dynamics that work **forward** in time
- The second equation describes dynamics that work **backward** in time
- While many terms are similar, one equation seems to apply matrix transformations to some matrices that play similar roles in the other equation

The family resemblances of these two equations reflect a profound **duality** that exists between control theory and filtering theory.

### An example

We can use the Python class _MultivariateNormal_ to construct examples.

Here is an example for a single period problem at time $ 0 $

```python
G = np.array([[1., 3.]])
R = np.array([[1.]])

x0_hat = np.array([0., 1.])
Œ£0 = np.array([[1., .5], [.3, 2.]])

Œº = np.hstack([x0_hat, G @ x0_hat])
Œ£ = np.block([[Œ£0, Œ£0 @ G.T], [G @ Œ£0, G @ Œ£0 @ G.T + R]])
```

```python
# construction of the multivariate normal instance
multi_normal = MultivariateNormal(Œº, Œ£)
```

```python
multi_normal.partition(2)
```

```python
# the observation of y
y0 = 2.3

# conditional distribution of x0
Œº1_hat, Œ£11 = multi_normal.cond_dist(0, y0)
Œº1_hat, Œ£11
```

    (array([-0.078125,  0.803125]),
     array([[ 0.72098214, -0.203125  ],
            [-0.403125  ,  0.228125  ]]))

```python
A = np.array([[0.5, 0.2], [-0.1, 0.3]])
C = np.array([[2.], [1.]])

# conditional distribution of x1
x1_cond = A @ Œº1_hat
Œ£1_cond = C @ C.T + A @ Œ£11 @ A.T
x1_cond, Œ£1_cond
```

    (array([0.1215625, 0.24875  ]),
     array([[4.12874554, 1.95523214],
            [1.92123214, 1.04592857]]))

### Code for Iterating

Below is the code for solving a dynamic filtering problem by iterating on our equations, followed by an example.

```python
def iterate(x0_hat, Œ£0, A, C, G, R, y_seq):

    p, n = G.shape

    T = len(y_seq)
    x_hat_seq = np.empty((T+1, n))
    Œ£_hat_seq = np.empty((T+1, n, n))

    x_hat_seq[0] = x0_hat
    Œ£_hat_seq[0] = Œ£0

    for t in range(T):
        xt_hat = x_hat_seq[t]
        Œ£t = Œ£_hat_seq[t]
        Œº = np.hstack([xt_hat, G @ xt_hat])
        Œ£ = np.block([[Œ£t, Œ£t @ G.T], [G @ Œ£t, G @ Œ£t @ G.T + R]])

        # filtering
        multi_normal = MultivariateNormal(Œº, Œ£)
        multi_normal.partition(n)
        x_tilde, Œ£_tilde = multi_normal.cond_dist(0, y_seq[t])

        # forecasting
        x_hat_seq[t+1] = A @ x_tilde
        Œ£_hat_seq[t+1] = C @ C.T + A @ Œ£_tilde @ A.T

    return x_hat_seq, Œ£_hat_seq
```

```python
iterate(x0_hat, Œ£0, A, C, G, R, [2.3, 1.2, 3.2])
```

    (array([[0.        , 1.        ],
            [0.1215625 , 0.24875   ],
            [0.18680212, 0.06904689],
            [0.75576875, 0.05558463]]),
     array([[[1.        , 0.5       ],
             [0.3       , 2.        ]],

            [[4.12874554, 1.95523214],
             [1.92123214, 1.04592857]],

            [[4.08198663, 1.99218488],
             [1.98640488, 1.00886423]],

            [[4.06457628, 2.00041999],
             [1.99943739, 1.00275526]]]))

The iterative algorithm described here is a version of the well-known **Kalman filter**.

We discuss the Kalman filter and some of its applications in **A First Look at the Kalman Filter**.

## Classic Factor Analysis Model

The factor analysis model, commonly used in psychology and other fields, can be represented as

$$
Y = \Lambda f + U
$$

where

1. $ Y $ is an $ n \times 1 $ random vector, and $ E U U^{\prime} = D $ is a diagonal matrix.
2. $ \Lambda $ is an $ n \times k $ coefficient matrix.
3. $ f $ is a $ k \times 1 $ random vector, with $ E f f^{\prime} = I $.
4. $ U $ is an $ n \times 1 $ random vector, and $ U \perp f $ (i.e., $ E U f' = 0 $).
5. It is assumed that $ k $ is small relative to $ n $; often $ k $ is only $ 1 $ or $ 2 $, as in our IQ examples.

This implies that

$$
\begin{aligned}
\Sigma_y = E Y Y^{\prime} = \Lambda \Lambda^{\prime} + D \\
E Y f^{\prime} = \Lambda \\
E f Y^{\prime} = \Lambda^{\prime}
\end{aligned}
$$

Thus, the covariance matrix $ \Sigma_Y $ is the sum of a diagonal matrix $ D $ and a positive semi-definite matrix $ \Lambda \Lambda^{\prime} $ of rank $ k $.

This means that all covariances among the $ n $ components of the $ Y $ vector are intermediated by their common dependencies on the $ k $ factors.

Form

$$
Z=\left(\begin{array}{c}
f\\
Y
\end{array}\right)
$$

the covariance matrix of the expanded random vector $ Z $ can be computed as

$$
\Sigma_{z} = EZZ^{\prime}=\left(\begin{array}{cc}
I & \Lambda^{\prime}\\
\Lambda & \Lambda\Lambda^{\prime}+D
\end{array}\right)
$$

In the following, we first construct the mean vector and the covariance matrix for the case where $ N=10 $ and $ k=2 $.

```python
N = 10
k = 2
```

We set the coefficient matrix $ \Lambda $ and the covariance matrix of $ U $ to be

$$
\Lambda=\left(\begin{array}{cc}
1 & 0\\
\vdots & \vdots\\
1 & 0\\
0 & 1\\
\vdots & \vdots\\
0 & 1
\end{array}\right),\quad D=\left(\begin{array}{cccc}
\sigma_{u}^{2} & 0 & \cdots & 0\\
0 & \sigma_{u}^{2} & \cdots & 0\\
\vdots & \vdots & \vdots & \vdots\\
0 & 0 & \cdots & \sigma_{u}^{2}
\end{array}\right)
$$

where the first half of the first column of $ \Lambda $ is filled with $ 1 $s and $ 0 $s for the rest half, and symmetrically for the second column.

$ D $ is a diagonal matrix with parameter $ \sigma\_{u}^{2} $ on the diagonal.

```python
Œõ = np.zeros((N, k))
Œõ[:N//2, 0] = 1
Œõ[N//2:, 1] = 1

œÉu = .5
D = np.eye(N) * œÉu ** 2
```

```python
# compute Œ£y
Œ£y = Œõ @ Œõ.T + D
```

We can now construct the mean vector and the covariance matrix for $ Z $.

```python
Œºz = np.zeros(k+N)

Œ£z = np.empty((k+N, k+N))

Œ£z[:k, :k] = np.eye(k)
Œ£z[:k, k:] = Œõ.T
Œ£z[k:, :k] = Œõ
Œ£z[k:, k:] = Œ£y
```

```python
z = np.random.multivariate_normal(Œºz, Œ£z)

f = z[:k]
y = z[k:]
```

```python
multi_normal_factor = MultivariateNormal(Œºz, Œ£z)
multi_normal_factor.partition(k)
```

Let‚Äôs compute the conditional distribution of the hidden factor $ f $ on the observations $ Y $, namely, $ f \mid Y=y $.

```python
multi_normal_factor.cond_dist(0, y)
```

    (array([-0.07548373,  1.28809687]),
     array([[0.04761905, 0.        ],
            [0.        , 0.04761905]]))

We can verify that the conditional mean $ E \left[f \mid Y=y\right] = B Y $ where $ B = \Lambda^{\prime} \Sigma\_{y}^{-1} $.

```python
B = Œõ.T @ np.linalg.inv(Œ£y)

B @ y
```

    array([-0.07548373,  1.28809687])

Similarly, we can compute the conditional distribution $ Y \mid f $.

```python
multi_normal_factor.cond_dist(1, f)
```

    (array([-0.04941602, -0.04941602, -0.04941602, -0.04941602, -0.04941602,
             0.99372257,  0.99372257,  0.99372257,  0.99372257,  0.99372257]),
     array([[0.25, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
            [0.  , 0.25, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
            [0.  , 0.  , 0.25, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
            [0.  , 0.  , 0.  , 0.25, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
            [0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.  , 0.  , 0.  , 0.  ],
            [0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.  , 0.  , 0.  ],
            [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.  , 0.  ],
            [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.  ],
            [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  ],
            [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.25]]))

It can be verified that the mean is $ \Lambda I^{-1} f = \Lambda f $.

```python
Œõ @ f
```

    array([-0.04941602, -0.04941602, -0.04941602, -0.04941602, -0.04941602,
            0.99372257,  0.99372257,  0.99372257,  0.99372257,  0.99372257])

## PCA and Factor Analysis

To learn about Principal Components Analysis (PCA), please see this lecture **Singular Value Decompositions**.

For fun, let‚Äôs apply a PCA decomposition to a covariance matrix $ \Sigma_y $ that in fact is governed by our factor-analytic model.

Technically, this means that the PCA model is misspecified. (Can you explain why?)

Nevertheless, this exercise will let us study how well the first two principal components from a PCA can approximate the conditional expectations $ E f_i | Y $ for our two factors $ f_i $, $ i=1,2 $ for the factor analytic model that we have assumed truly governs the data on $ Y $ we have generated.

So we compute the PCA decomposition

$$
\Sigma_{y} = P \tilde{\Lambda} P^{\prime}
$$

where $ \tilde{\Lambda} $ is a diagonal matrix.

We have

$$
Y = P \epsilon
$$

and

$$
\epsilon = P^\prime Y
$$

Note that we will arrange the eigenvectors in $ P $ in the _descending_ order of eigenvalues.

```python
ùúÜ_tilde, P = np.linalg.eigh(Œ£y)

# arrange the eigenvectors by eigenvalues
ind = sorted(range(N), key=lambda x: ùúÜ_tilde[x], reverse=True)

P = P[:, ind]
ùúÜ_tilde = ùúÜ_tilde[ind]
Œõ_tilde = np.diag(ùúÜ_tilde)

print('ùúÜ_tilde =', ùúÜ_tilde)
```

    ùúÜ_tilde = [5.25 5.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25]

```python
# verify the orthogonality of eigenvectors
np.abs(P @ P.T - np.eye(N)).max()
```

    2.220446049250313e-16

```python
# verify the eigenvalue decomposition is correct
P @ Œõ_tilde @ P.T
```

    array([[1.25, 1.  , 1.  , 1.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
           [1.  , 1.25, 1.  , 1.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
           [1.  , 1.  , 1.25, 1.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
           [1.  , 1.  , 1.  , 1.25, 1.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
           [1.  , 1.  , 1.  , 1.  , 1.25, 0.  , 0.  , 0.  , 0.  , 0.  ],
           [0.  , 0.  , 0.  , 0.  , 0.  , 1.25, 1.  , 1.  , 1.  , 1.  ],
           [0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.25, 1.  , 1.  , 1.  ],
           [0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.  , 1.25, 1.  , 1.  ],
           [0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.  , 1.  , 1.25, 1.  ],
           [0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.  , 1.  , 1.  , 1.25]])

```python
Œµ = P.T @ y

print("Œµ = ", Œµ)
```

    Œµ =  [-0.17722608  3.02428576  0.2526106  -0.15669567 -0.15821437 -0.0035276
      0.11252632  0.36515158  1.1670303  -0.01240238]

```python
# print the values of the two factors

print('f = ', f)
```

    f =  [-0.04941602  0.99372257]

Below we‚Äôll plot several things

- the $ N $ values of $ y $
- the $ N $ values of the principal components $ \epsilon $
- the value of the first factor $ f_1 $ plotted only for the first $ N/2 $ observations of $ y $ for which it receives a non-zero loading in $ \Lambda $
- the value of the second factor $ f_2 $ plotted only for the final $ N/2 $ observations for which it receives a non-zero loading in $ \Lambda $

```python
plt.scatter(range(N), y, label='y')
plt.scatter(range(N), Œµ, label='$\epsilon$')
plt.hlines(f[0], 0, N//2-1, ls='--', label='$f_{1}$')
plt.hlines(f[1], N//2, N-1, ls='-.', label='$f_{2}$')
plt.legend()

plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/12.%20Multivariate%20Normal%20Distribution_files/12.%20Multivariate%20Normal%20Distribution_171_0.png)
</div>

Consequently, the first two $ \epsilon\_{j} $ correspond to the largest two eigenvalues.

Let‚Äôs look at them, after which we‚Äôll look at $ E f | y = B y $

```python
Œµ[:2]
```

    array([-0.17722608,  3.02428576])

```python
# compare with Ef|y
B @ y
```

    array([-0.07548373,  1.28809687])

The fraction of variance in $ y\_{t} $ explained by the first two principal components can be computed as below.

```python
ùúÜ_tilde[:2].sum() / ùúÜ_tilde.sum()
```

    0.8400000000000002

Compute

$$
\hat{Y} = P_{j} \epsilon_{j} + P_{k} \epsilon_{k}
$$

where $ P*{j} $ and $ P*{k} $ correspond to the largest two eigenvalues.

```python
y_hat = P[:, :2] @ Œµ[:2]
```

In this example, it turns out that the projection $ \hat{Y} $ of $ Y $ on the first two principal components does a good job of approximating $ Ef \mid y $.

We confirm this in the following plot of $ f $, $ E y \mid f $, $ E f \mid y $, and $ \hat{y} $ on the coordinate axis versus $ y $ on the ordinate axis.

```python
plt.scatter(range(N), Œõ @ f, label='$Ey|f$')
plt.scatter(range(N), y_hat, label='$\hat{y}$')
plt.hlines(f[0], 0, N//2-1, ls='--', label='$f_{1}$')
plt.hlines(f[1], N//2, N-1, ls='-.', label='$f_{2}$')

Efy = B @ y
plt.hlines(Efy[0], 0, N//2-1, ls='--', color='b', label='$Ef_{1}|y$')
plt.hlines(Efy[1], N//2, N-1, ls='-.', color='b', label='$Ef_{2}|y$')
plt.legend()

plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/12.%20Multivariate%20Normal%20Distribution_files/12.%20Multivariate%20Normal%20Distribution_180_0.png)
</div>

The covariance matrix of $ \hat{Y} $ can be computed by first constructing the covariance matrix of $ \epsilon $ and then using the upper left block for $ \epsilon*{1} $ and $ \epsilon*{2} $.

```python
Œ£Œµjk = (P.T @ Œ£y @ P)[:2, :2]

Pjk = P[:, :2]

Œ£y_hat = Pjk @ Œ£Œµjk @ Pjk.T
print('Œ£y_hat = \n', Œ£y_hat)
```

    Œ£y_hat =
     [[1.05 1.05 1.05 1.05 1.05 0.   0.   0.   0.   0.  ]
     [1.05 1.05 1.05 1.05 1.05 0.   0.   0.   0.   0.  ]
     [1.05 1.05 1.05 1.05 1.05 0.   0.   0.   0.   0.  ]
     [1.05 1.05 1.05 1.05 1.05 0.   0.   0.   0.   0.  ]
     [1.05 1.05 1.05 1.05 1.05 0.   0.   0.   0.   0.  ]
     [0.   0.   0.   0.   0.   1.05 1.05 1.05 1.05 1.05]
     [0.   0.   0.   0.   0.   1.05 1.05 1.05 1.05 1.05]
     [0.   0.   0.   0.   0.   1.05 1.05 1.05 1.05 1.05]
     [0.   0.   0.   0.   0.   1.05 1.05 1.05 1.05 1.05]
     [0.   0.   0.   0.   0.   1.05 1.05 1.05 1.05 1.05]]
