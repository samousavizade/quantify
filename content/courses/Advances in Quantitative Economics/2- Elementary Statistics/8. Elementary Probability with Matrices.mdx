---
title: Elementary Probability with Matrices
draft: false
summary: test
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

# Fundamental Probability Concepts Using Matrices

This lecture employs matrix algebra to demonstrate key principles of probability theory.

After providing informal definitions of underlying concepts, we'll utilize matrices and vectors to represent probability distributions.

Topics we'll explore include:

- Joint probability distributions
- Marginal distributions derived from joint distributions
- Conditional probability distributions
- Statistical independence between random variables
- Joint distributions associated with given marginal distributions
- Couplings
- Copulas
- Probability distribution of the sum of two independent random variables
- Convolution of marginal distributions
- Parameters defining probability distributions
- Sufficient statistics for data summarization

We'll employ matrices to represent bivariate probability distributions and vectors for univariate probability distributions

In addition to Anaconda's standard libraries, this lecture requires the following packages:

```python
!pip install prettytable
```

    Collecting prettytable


      Downloading prettytable-3.11.0-py3-none-any.whl.metadata (30 kB)


    Requirement already satisfied: wcwidth in /Users/s.alirezamousavizade/anaconda3/lib/python3.11/site-packages (from prettytable) (0.2.13)


    Downloading prettytable-3.11.0-py3-none-any.whl (28 kB)


    Installing collected packages: prettytable
    Successfully installed prettytable-3.11.0

We'll begin by importing the necessary libraries

```python
import numpy as np
import matplotlib.pyplot as plt
import prettytable as pt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib_inline.backend_inline import set_matplotlib_formats
set_matplotlib_formats('retina')
```

## Overview of Core Concepts

We'll briefly define a **probability space**, a **probability measure**, and a **random variable**.

For most of this lecture, these concepts will remain in the background, but they underpin the other elements we'll focus on.

Let $ \Omega $ be a set of possible outcomes and $ \omega \in \Omega $ be a specific outcome.

Let $ \mathcal{G} \subset \Omega $ be a subset of $ \Omega $.

Let $ \mathcal{F} $ be a collection of such subsets $ \mathcal{G} \subset \Omega $.

The pair $ \Omega,\mathcal{F} $ forms our **probability space** on which we'll define a probability measure.

A **probability measure** $ \mu $ maps a set of possible outcomes $ \mathcal{G} \in \mathcal{F} $ to a number between $ 0 $ and $ 1 $

- this represents the "probability" that $ X $ belongs to $ A $, denoted by $ \textrm{Prob}\{X\in A\} $.

A **random variable** $ X(\omega) $ is a function of the outcome $ \omega \in \Omega $.

The random variable $ X(\omega) $ has a **probability distribution** induced by the probability measure $ \mu $ and the function
$ X(\omega) $:

$$
\textrm{Prob} (X \in A ) = \int_{\mathcal{G}} \mu(\omega) d \omega \tag{8.1}
$$

where $ {\mathcal G} $ is the subset of $ \Omega $ for which $ X(\omega) \in A $.

This is called the induced probability distribution of random variable $ X $.

## Interpreting Probability

Before delving deeper, let's discuss what probability theory means and its connection to statistics.

Much of this lecture will focus on fixed "population" probabilities.

These are purely mathematical constructs.

To understand how statisticians link probabilities to data, it's crucial to grasp these concepts:

- A single draw from a probability distribution
- Repeated independently and identically distributed (i.i.d.) draws of "samples" or "realizations" from the same probability distribution
- A **statistic** defined as a function of a sample sequence
- An **empirical distribution** or **histogram** (a binned empirical distribution) that records observed **relative frequencies**
- The idea that a population probability distribution represents anticipated **relative frequencies** in a long sequence of i.i.d. draws.... The following mathematical tools precisely define **anticipated relative frequencies**
- **Law of Large Numbers (LLN)**
- **Central Limit Theorem (CLT)**

**Scalar example**

Let $ X $ be a scalar random variable taking $ I $ possible values
$ 0, 1, 2, \ldots, I-1 $ with probabilities

$$
{\rm Prob}(X = i) = f_i, \quad
$$

where

$$
f_i \geqslant 0, \quad \sum_i f_i = 1 .
$$

We sometimes write

$$
X \sim \{{f_i}\}_{i=0}^{I-1}
$$

as shorthand for saying the random variable $ X $ follows the probability distribution $ \{{f*i}\}*{i=0}^{I-1} $.

Consider drawing a sample $ x*0, x_1, \dots , x*{N-1} $ of $ N $ independent and identically distributed draws of $ X $.

What do "identical" and "independent" mean in IID or iid ("identically and independently distributed)?

- "identical" means each draw comes from the same distribution.
- "independent" means joint distributions equal products of marginal distributions, i.e.,

$$
\begin{aligned}
\textrm{Prob}\{x_0 = i_0, x_1 = i_1, \dots , x_{N-1} = i_{N-1}\} &= \textrm{Prob}\{x_0 = i_0\} \cdot \dots \cdot \textrm{Prob}\{x_{I-1} = i_{I-1}\}\\
&= f_{i_0} f_{i_1} \cdot \dots \cdot f_{i_{N-1}}\\
\end{aligned}
$$

We define an **empirical distribution** as follows.

For each $ i = 0,\dots,I-1 $, let

$$
\begin{aligned}
N_i & = \text{number of times} \ X = i,\\
N & = \sum^{I-1}_{i=0} N_i \quad \text{total number of draws},\\
\tilde {f_i} & = \frac{N_i}{N} \sim \ \text{frequency of draws where}\ X=i
\end{aligned}
$$

Key concepts justifying the connection between probability theory and statistics are laws of large numbers and central limit theorems

**LLN:**

- A Law of Large Numbers (LLN) states that $ \tilde {f_i} \to f_i \text{ as } N \to \infty $

**CLT:**

- A Central Limit Theorem (CLT) describes the **rate** at which $ \tilde {f_i} \to f_i $

**Remarks**

- For "frequentist" statisticians, **anticipated relative frequency** is **all** that a probability distribution signifies.
- However, for a Bayesian, it carries additional or different meaning.

## Depicting Probability Distributions

A probability distribution $ \textrm{Prob} (X \in A) $ can be characterized by its **cumulative distribution function (CDF)**

$$
F_{X}(x) = \textrm{Prob}\{X\leq x\}.
$$

In some cases, a random variable can also be described by a **density function** $ f(x) $
related to its CDF by

$$
\textrm{Prob} \{X\in B\} = \int_{t\in B}f(t)dt
$$

$$
F(x) = \int_{-\infty}^{x}f(t)dt
$$

Here $ B $ is a set of possible $ X $'s whose probability we want to determine.

When a probability density exists, a probability distribution can be defined by either its CDF or its density.

For a **discrete-valued** random variable

- the number of possible values of $ X $ is finite or countably infinite
- we substitute a **probability mass function** for a **density**, which is a non-negative sequence summing to one
- we replace integration with summation in formulas like [(8.1)](#equation-eq-cdffromdensity) that link a CDF to a probability mass function

This lecture primarily focuses on discrete random variables.

This approach allows us to mainly use linear algebra tools.

Later, we'll briefly discuss approximating a continuous random variable with a discrete one.

## Single-Variable Probability Distributions

We'll dedicate most of this lecture to discrete-valued random variables, but we'll also touch on continuous-valued random variables.

### Discrete random variable

Let $ X $ be a discrete random variable with possible values: $ i=0,1,\ldots,I-1 = \bar{X} $.

We choose the maximum index $ I-1 $ to align with Python's indexing convention.

Define $ f_i \equiv \textrm{Prob}\{X=i\} $
and construct the non-negative vector

$$
f=\left[\begin{array}{c}
f_{0}\\
f_{1}\\
\vdots\\
f_{I-1}
\end{array}\right] \tag{8.2}
$$

where $ f*{i} \in [0,1] $ for each $ i $ and $ \sum*{i=0}^{I-1}f_i=1 $.

This vector defines a **probability mass function**.

The distribution [(8.2)](#equation-eq-discretedist)
has **parameters** $ \{f*{i}\}*{i=0,1,... \cdots ,I-2} $ since $ f*{I-1} = 1-\sum*{i=0}^{I-2}f\_{i} $.

These parameters determine the distribution's shape.

(Sometimes $ I = \infty $.)

Such a "non-parametric" distribution has as many "parameters" as possible values of the random variable.

We often work with special distributions characterized by a small number of parameters.

In these special parametric distributions,

$$
f_i = g(i; \theta)
$$

where $ \theta $ is a parameter vector with much smaller dimension than $ I $.

**Remarks:**

- The concept of **parameter** is closely related to the notion of **sufficient statistic**.
- Sufficient statistics are nonlinear functions of a dataset.
- Sufficient statistics are designed to summarize all **information** about parameters contained in a dataset.
- They are important tools that AI uses to summarize **big data** sets
- R. A. Fisher provided a rigorous definition of **information** – see [https://en.wikipedia.org/wiki/Fisher_information](https://en.wikipedia.org/wiki/Fisher_information)

An example of a parametric probability distribution is a **geometric distribution**.

It is described by

$$
f_{i} = \textrm{Prob}\{X=i\} = (1-\lambda)\lambda^{i},\quad \lambda \in [0,1], \quad i = 0, 1, 2, \ldots
$$

Clearly, $ \sum\_{i=0}^{\infty}f_i=1 $.

Let $ \theta $ be a parameter vector of the distribution described by $ f $, then

$$
f_i( \theta)\ge0, \sum_{i=0}^{\infty}f_i(\theta)=1
$$

### Continuous random variable

Let $ X $ be a continuous random variable taking values $ X \in \tilde{X}\equiv[X_U,X_L] $ with distributions having parameters $ \theta $.

$$
\textrm{Prob}\{X\in A\} = \int_{x\in A} f(x;\theta)\,dx; \quad f(x;\theta)\ge0
$$

where $ A $ is a subset of $ \tilde{X} $ and

$$
\textrm{Prob}\{X\in \tilde{X}\} =1
$$

## Two-Variable Probability Distributions

We'll now examine a bivariate **joint distribution**.

To start, we'll focus on two discrete random variables.

Let $ X,Y $ be two discrete random variables taking values:

$$
X\in\{0,\ldots,I-1\}
$$

$$
Y\in\{0,\ldots,J-1\}
$$

Their **joint distribution** is represented by a matrix

$$
F_{I\times J}=[f_{ij}]_{i\in\{0,\ldots,I-1\},... j\in\{0,\ldots,J-1\}}
$$

where elements are

$$
f_{ij}=\textrm{Prob}\{X=i,Y=j\} \geq 0
$$

and

$$
\sum_{i}\sum_{j}f_{ij}=1
$$

## Marginal Probability Distributions

The joint distribution gives rise to marginal distributions

$$
\textrm{Prob}\{X=i\}= \sum_{j=0}^{J-1}f_{ij} = \mu_i, \quad i=0,\ldots,I-1
$$

$$
\textrm{Prob}\{Y=j\}= \sum_{i=0}^{I-1}f_{ij} = \nu_j, \quad j=0,\ldots,J-1
$$

For instance, consider a joint distribution over $ (X,Y) $ given by

$$
F = \left[
\begin{matrix}
.25 & .1\\
.15 & .5
\end{matrix}
\right] \tag{8.3}
$$

The resulting marginal distributions are:

$$
\begin{aligned}
\textrm{Prob} \{X=0\}&=.25+.1=.35\\
\textrm{Prob}\{X=1\}& =.15+.5=.65\\
\textrm{Prob}\{Y=0\}&=.25+.15=.4\\
\textrm{Prob}\{Y=1\}&=.1+.5=.6
\end{aligned}
$$

**Note:** For continuous random variables, we replace summation with integration to compute marginal distributions.

## Conditional Probability Distributions

Conditional probabilities are defined as

$$
\textrm{Prob}\{A | B\}=\frac{\textrm{Prob}\{A \cap B\}}{\textrm{Prob}\{B\}}
$$

where $ A, B $ are two events.

For a pair of discrete random variables, we have the **conditional distribution**

$$
\textrm{Prob}\{X=i|Y=j\}=\frac{f_{ij}}{\sum_{i}f_{ij}}
=\frac{\textrm{Prob} \{X=i, Y=j\} }{\textrm{Prob} \{Y=j\} }
$$

where $ i=0, \ldots,I-1,... \quad j=0,\ldots,J-1 $.

Note that

$$
\sum_{i}\textrm{Prob}\{X=i|Y=j\}
=\frac{ \sum_{i}f_{ij} }{ \sum_{i}f_{ij}}=1
$$

**Remark:** The mathematics of conditional probability leads to **Bayes' Law**:

$$
\textrm{Prob}\{X=i|Y=j\}    =\frac{\textrm{Prob}\{X=i,Y=j\}}{\textrm{Prob}\{Y=j\}}=\frac{\textrm{Prob}\{Y=j|X=i\}\textrm{Prob}\{X=i\}}{\textrm{Prob}\{Y=j\}}
$$

For the joint distribution [(8.3)](#equation-eq-example101discrete)

$$
\textrm{Prob}\{X=0|Y=1\} =\frac{ .1}{.1+.5}=\frac{.1}{.6}
$$

## Statistical Independence

Random variables X and Y are statistically **independent** if

$$
\textrm{Prob}\{X=i,Y=j\}={f_ig_j}
$$

where

$$
\begin{aligned}
\textrm{Prob}\{X=i\} &=f_i\ge0， \sum{f_i}=1 \cr
\textrm{Prob}\{Y=j\} & =g_j\ge0， \sum{g_j}=1
\end{aligned}
$$

Conditional distributions are

$$
\begin{aligned}
\textrm{Prob}\{X=i|Y=j\} & =\frac{f_ig_j}{\sum_{i}f_ig_j}=\frac{f_ig_j}{g_j}=f_i \\
\textrm{Prob}\{Y=j|X=i\} & =\frac{f_ig_j}{\sum_{j}f_ig_j}=\frac{f_ig_j}{f_i}=g_j
\end{aligned}
$$

## Means and Variances

The mean and variance of a discrete random variable $ X $ are

$$
\begin{aligned}
\mu_{X} & \equiv\mathbb{E}\left[X\right]
=\sum_{k}k \textrm{Prob}\{X=k\} \\
\sigma_{X}^{2} & \equiv\mathbb{D}\left[X\right]=\sum_{k}\left(k-\mathbb{E}\left[X\right]\right)^{2}\textrm{Prob}\{X=k\}
\end{aligned}
$$

A continuous random variable with density $ f\_{X}(x) $ has mean and variance

$$
\begin{aligned}
\mu_{X} & \equiv\mathbb{E}\left[X\right]=\int_{-\infty}^{\infty}xf_{X}(x)dx \\
\sigma_{X}^{2}\equiv\mathbb{D}\left[X\right] & =\mathrm{E}\left[\left(X-\mu_{X}\right)^{2}\right]=\int_{-\infty}^{\infty}\left(x-\mu_{X}\right)^{2}f_{X}(x)dx
\end{aligned}
$$

## Generating Random Numbers

Suppose we have access to a pseudo random number generator that produces a uniform random variable, i.e., one with probability distribution

$$
\textrm{Prob}\{\tilde{X}=i\}=\frac{1}{I},\quad i=0,\ldots,I-1
$$

How can we transform $ \tilde{X} $ to obtain a random variable $ X $ for which $ \textrm{Prob}\{X=i\}=f_i,\quad i=0,\ldots,I-1 $,
where $ f_i $ is an arbitrary discrete probability distribution on $ i=0,1,\dots,I-1 $?

The key tool is the inverse of a cumulative distribution function (CDF).

Observe that the CDF of a distribution is monotone and non-decreasing, taking values between $ 0 $ and $ 1 $.

We can generate a sample of a random variable $ X $ with a known CDF as follows:

- draw a random variable $ u $ from a uniform distribution on $ [0,1] $
- input the sample value of $ u $ into the **"inverse"** target CDF for $ X $
- $ X $ follows the target CDF

Thus, knowing the **"inverse"** CDF of a distribution is sufficient to simulate from this distribution.

> **Note**The "inverse" CDF must exist for this method to work.

The inverse CDF is

$$
F^{-1}(u)\equiv\inf \{x\in \mathbb{R}: F(x) \geq u\} \quad(0<u<1)
$$

```python
n, λ = 1_000_000, 0.3

# draw uniform numbers
u = np.random.rand(n)

# transform
x = -np.log(1-u)/λ

# draw geometric distributions
x_g = np.random.exponential(1 / λ, n)

# plot and compare
plt.hist(x, bins=100, density=True)
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/8.%20Elementary%20Probability%20with%20Matrices_files/8.%20Elementary%20Probability%20with%20Matrices_16_0.png)
</div>

```python
plt.hist(x_g, bins=100, density=True, alpha=0.6)
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/8.%20Elementary%20Probability%20with%20Matrices_files/8.%20Elementary%20Probability%20with%20Matrices_17_0.png)
</div>

**Geometric distribution**

Let $ X $ be distributed geometrically, that is

$$
\begin{aligned}
\textrm{Prob}(X=i) & =(1-\lambda)\lambda^i,\quad\lambda\in(0,1), \quad i=0,1,\dots \\
& \sum_{i=0}^{\infty}\textrm{Prob}(X=i)=1\longleftrightarrow(1- \lambda)\sum_{i=0}^{\infty}\lambda^i=\frac{1-\lambda}{1-\lambda}=1
\end{aligned}
$$

Its CDF is given by

$$
\begin{aligned}
\textrm{Prob}(X\le i)& =(1-\lambda)\sum_{j=0}^{i}\lambda^i\\
& =(1-\lambda)[\frac{1-\lambda^{i+1}}{1-\lambda}]\\
& =1-\lambda^{i+1}\\
& =F(X)=F_i \quad
\end{aligned}
$$

Again, let $ \tilde{U} $ follow a uniform distribution and we want to find $ X $ such that $ F(X)=\tilde{U} $.

Let's deduce the distribution of $ X $ from

$$
\begin{aligned}
\tilde{U} & =F(X)=1-\lambda^{x+1}\\
1-\tilde{U} & =\lambda^{x+1}\\
\log(1-\tilde{U})& =(x+1)\log\lambda\\
\frac{\log(1-\tilde{U})}{\log\lambda}& =x+1\\
\frac{\log(1-\tilde{U})}{\log\lambda}-1 &=x
\end{aligned}
$$

However, $ \tilde{U}=F^{-1}(X) $ may not be an integer for any $ x\geq0 $.

So let

$$
x=\lceil\frac{\log(1-\tilde{U})}{\log\lambda}-1\rceil
$$

where $ \lceil . \rceil $ is the ceiling function.

Thus $ x $ is the smallest integer such that the discrete geometric CDF is greater than or equal to $ \tilde{U} $.

We can verify that $ x $ is indeed geometrically distributed by the following `numpy` program.

> **Note**The exponential distribution is the continuous analog of geometric distribution.

```python
n, λ = 1_000_000, 0.8

# draw uniform numbers
u = np.random.rand(n)

# transform
x = np.ceil(np.log(1-u)/np.log(λ) - 1)

# draw geometric distributions
x_g = np.random.geometric(1-λ, n)

# plot and compare
plt.hist(x, bins=150, density=True)
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/8.%20Elementary%20Probability%20with%20Matrices_files/8.%20Elementary%20Probability%20with%20Matrices_19_0.png)
</div>

```python
np.random.geometric(1-λ, n).max()
```

    67

```python
np.log(0.4)/np.log(0.3)
```

    0.7610560044063083

```python
plt.hist(x_g, bins=150, density=True, alpha=0.6)
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/8.%20Elementary%20Probability%20with%20Matrices_files/8.%20Elementary%20Probability%20with%20Matrices_22_0.png)
</div>

## Some Discrete Probability Distributions

Let's write some Python code to compute means and variances of some univariate random variables.

We'll use our code to

- compute population means and variances from the probability distribution
- generate a sample of $ N $ independently and identically distributed draws and compute sample means and variances
- compare population and sample means and variances

## Geometric distribution

$$
\textrm{Prob}(X=k)=(1-p)^{k-1}p,k=1,2, \ldots
$$

$ \implies $

$$
\begin{aligned}
\mathbb{E}(X) & =\frac{1}{p}\\\mathbb{D}(X) & =\frac{1-p}{p^2}
\end{aligned}
$$

We draw observations from the distribution and compare the sample mean and variance with the theoretical results.

```python
# specify parameters
p, n = 0.3, 1_000_000

# draw observations from the distribution
x = np.random.geometric(p, n)

# compute sample mean and variance
μ_hat = np.mean(x)
σ2_hat = np.var(x)

print("The sample mean is: ", μ_hat, "\nThe sample variance is: ", σ2_hat)

# compare with theoretical results
print("\nThe population mean is: ", 1/p)
print("The population variance is: ", (1-p)/(p**2))
```

    The sample mean is:  3.329537
    The sample variance is:  7.729802365630999

    The population mean is:  3.3333333333333335
    The population variance is:  7.777777777777778

### Newcomb–Benford distribution

The **Newcomb–Benford law** fits many data sets, e.g., reports of incomes to tax authorities, in which
the leading digit is more likely to be small than large.

See [https://en.wikipedia.org/wiki/Benford's_law](https://en.wikipedia.org/wiki/Benford%27s_law)

A Benford probability distribution is

$$
\textrm{Prob}\{X=d\}=\log _{10}(d+1)-\log _{10}(d)=\log _{10}\left(1+\frac{1}{d}\right)
$$

where $ d\in\{1,2,\cdots,9\} $ can be thought of as a **first digit** in a sequence of digits.

This is a well defined discrete distribution since we can verify that probabilities are nonnegative and sum to $ 1 $.

$$
\log_{10}\left(1+\frac{1}{d}\right)\geq0,\quad\sum_{d=1}^{9}\log_{10}\left(1+\frac{1}{d}\right)=1
$$

The mean and variance of a Benford distribution are

$$
\begin{aligned}
\mathbb{E}\left[X\right]     &=\sum_{d=1}^{9}d\log_{10}\left(1+\frac{1}{d}\right)\simeq3.4402 \\
\mathbb{V}\left[X\right]     & =\sum_{d=1}^{9}\left(d-\mathbb{E}\left[X\right]\right)^{2}\log_{10}\left(1+\frac{1}{d}\right)\simeq6.0565
\end{aligned}
$$

We verify the above and compute the mean and variance using `numpy`.

```python
Benford_pmf = np.array([np.log10(1+1/d) for d in range(1,10)])
k = np.array(range(1,10))

# mean
mean = np.sum(Benford_pmf * k)

# variance
var = np.sum([(k-mean)**2 * Benford_pmf])

# verify sum to 1
print(np.sum(Benford_pmf))
print(mean)
print(var)
```

    0.9999999999999999
    3.4402369671232065
    6.056512631375665

```python
# plot distribution
plt.plot(range(1,10), Benford_pmf, 'o')
plt.title('Benford\'s distribution')
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/8.%20Elementary%20Probability%20with%20Matrices_files/8.%20Elementary%20Probability%20with%20Matrices_28_0.png)
</div>

### Pascal (negative binomial) distribution

Consider a sequence of independent Bernoulli trials.

Let $ p $ be the probability of success.

Let $ X $ be a random variable that represents the number of failures before we get $ r $ success.

Its distribution is

$$
\begin{aligned}
X & \sim NB(r,p) \\
\textrm{Prob}(X=k;r,p) & = \begin{pmatrix}k+r-1 \\ r-1 \end{pmatrix}p^r(1-p)^{k}
\end{aligned}
$$

Here, we choose from among $ k+r-1 $ possible outcomes because the last draw is by definition a success.

We compute the mean and variance to be

$$
\begin{aligned}
\mathbb{E}(X) & = \frac{k(1-p)}{p} \\
\mathbb{V}(X) & = \frac{k(1-p)}{p^2}
\end{aligned}
$$

```python
# specify parameters
r, p, n = 10, 0.3, 1_000_000

# draw observations from the distribution
x = np.random.negative_binomial(r, p, n)

# compute sample mean and variance
μ_hat = np.mean(x)
σ2_hat = np.var(x)

print("The sample mean is: ", μ_hat, "\nThe sample variance is: ", σ2_hat)
print("\nThe population mean is: ", r*(1-p)/p)
print("The population variance is: ", r*(1-p)/p**2)
```

    The sample mean is:  23.333184
    The sample variance is:  77.75593842214398

    The population mean is:  23.333333333333336
    The population variance is:  77.77777777777779

## Continuous Random Variables

### Univariate Gaussian distribution

We denote

$$
X \sim N(\mu,\sigma^2)
$$

to represent the probability distribution

$$
f(x|u,\sigma^2)=\frac{1}{\sqrt{2\pi \sigma^2}}e^{[-\frac{1}{2\sigma^2}(x-u)^2]}
$$

In the following example, we set $ \mu = 0, \sigma = 0.1 $.

```python
# specify parameters
μ, σ = 0, 0.1

# specify number of draws
n = 1_000_000

# draw observations from the distribution
x = np.random.normal(μ, σ, n)

# compute sample mean and variance
μ_hat = np.mean(x)
σ_hat = np.std(x)

print("The sample mean is: ", μ_hat)
print("The sample standard deviation is: ", σ_hat)
```

    The sample mean is:  0.00010803631554784542
    The sample standard deviation is:  0.10003549137457107

```python
# compare
print(μ-μ_hat < 1e-3)
print(σ-σ_hat < 1e-3)
```

    True
    True

### Uniform Distribution

$$
\begin{aligned}
X & \sim U[a,b] \\
f(x)& = \begin{cases} \frac{1}{b-a}, & a \leq x \leq b \\ \quad0, & \text{otherwise} \end{cases}
\end{aligned}
$$

The population mean and variance are

$$
\begin{aligned}
\mathbb{E}(X) & = \frac{a+b}{2} \\
\mathbb{V}(X) & = \frac{(b-a)^2}{12}
\end{aligned}
$$

```python
# specify parameters
a, b = 10, 20

# specify number of draws
n = 1_000_000

# draw observations from the distribution
x = a + (b-a)*np.random.rand(n)

# compute sample mean and variance
μ_hat = np.mean(x)
σ2_hat = np.var(x)

print("The sample mean is: ", μ_hat, "\nThe sample variance is: ", σ2_hat)
print("\nThe population mean is: ", (a+b)/2)
print("The population variance is: ", (b-a)**2/12)
```

    The sample mean is:  15.000488519984822
    The sample variance is:  8.33198587911684

    The population mean is:  15.0
    The population variance is:  8.333333333333334

## A Mixed Discrete-Continuous Distribution

Let's illustrate this concept with a brief scenario.

Imagine you're applying for a job that requires an interview. You have a 5% chance of passing the interview. If you pass, your daily salary will be uniformly distributed between 300 and 400.

We can describe your daily salary as a discrete-continuous variable with the following probabilities:

$$
P(X=0)=0.95
$$

$$
P(300\le X \le 400)=\int_{300}^{400} f(x)\,... dx=0.05
$$

$$
f(x) = 0.0005
$$

Let's begin by generating a random sample and computing sample moments.

```python
x = np.random.rand(1_000_000)
# x[x > 0.95] = 100*x[x > 0.95]+300
x[x > 0.95] = 100*np.random.rand(len(x[x > 0.95]))+300
x[x <= 0.95] = 0

μ_hat = np.mean(x)
σ2_hat = np.var(x)

print("The sample mean is: ", μ_hat, "\nThe sample variance is: ", σ2_hat)
```

    The sample mean is:  17.575503518967157
    The sample variance is:  5883.142466774653

We can calculate the analytical mean and variance as follows:

$$
\begin{aligned}
\mu &= \int_{300}^{400}xf(x)dx \\
&= 0.0005\int_{300}^{400}xdx \\
&= 0.0005 \times \frac{1}{2}x^2\bigg|_{300}^{400}
\end{aligned}
$$

$$
\begin{aligned}
\sigma^2 &= 0.95\times(0-17.5)^2+\int_{300}^{400}(x-17.5)^2f(x)dx \\
&= 0.95\times17.5^2+0.0005\int_{300}^{400}(x-17.5)^2dx \\
&= 0.95\times17.5^2+0.0005 \times \frac{1}{3}(x-17.5)^3 \bigg|_{300}^{400}
\end{aligned}
$$

```python
mean = 0.0005*0.5*(400**2 - 300**2)
var = 0.95*17.5**2+0.0005/3*((400-17.5)**3-(300-17.5)**3)
print("mean: ", mean)
print("variance: ", var)
```

    mean:  17.5
    variance:  5860.416666666666

## Matrix Representation of Some Bivariate Distributions

We'll use matrices to represent a joint distribution, conditional distribution, marginal distribution, and the mean and variance of a bivariate random variable.

The following table illustrates a probability distribution for a bivariate random variable.

$$
F=[f_{ij}]=\left[\begin{array}{cc}
0.3 & 0.2\\
0.1 & 0.4
\end{array}\right]
$$

Marginal distributions are computed as:

$$
\textrm{Prob}(X=i)=\sum_j{f_{ij}}=u_i
$$

$$
\textrm{Prob}(Y=j)=\sum_i{f_{ij}}=v_j
$$

We'll now draw some samples to confirm that the "sampling" distribution aligns well with the "population" distribution.

**Sample results:**

```python
# specify parameters
xs = np.array([0, 1])
ys = np.array([10, 20])
f = np.array([[0.3, 0.2], [0.1, 0.4]])
f_cum = np.cumsum(f)

# draw random numbers
p = np.random.rand(1_000_000)
x = np.vstack([xs[1]*np.ones(p.shape), ys[1]*np.ones(p.shape)])
# map to the bivariate distribution

x[0, p < f_cum[2]] = xs[1]
x[1, p < f_cum[2]] = ys[0]

x[0, p < f_cum[1]] = xs[0]
x[1, p < f_cum[1]] = ys[1]

x[0, p < f_cum[0]] = xs[0]
x[1, p < f_cum[0]] = ys[0]
print(x)
```

    [[ 1.  1.  1. ...  1.  1.  0.]
     [20. 20. 20. ... 20. 20. 20.]]

Here, we employ the inverse CDF technique to generate samples from the joint distribution $ F $.

```python
# marginal distribution
xp = np.sum(x[0, :] == xs[0])/1_000_000
yp = np.sum(x[1, :] == ys[0])/1_000_000

# print output
print("marginal distribution for x")
xmtb = pt.PrettyTable()
xmtb.field_names = ['x_value', 'x_prob']
xmtb.add_row([xs[0], xp])
xmtb.add_row([xs[1], 1-xp])
print(xmtb)

print("\nmarginal distribution for y")
ymtb = pt.PrettyTable()
ymtb.field_names = ['y_value', 'y_prob']
ymtb.add_row([ys[0], yp])
ymtb.add_row([ys[1], 1-yp])
print(ymtb)
```

    marginal distribution for x
    +---------+----------+
    | x_value |  x_prob  |
    +---------+----------+
    |    0    | 0.499831 |
    |    1    | 0.500169 |
    +---------+----------+

    marginal distribution for y
    +---------+--------------------+
    | y_value |       y_prob       |
    +---------+--------------------+
    |    10   |      0.39921       |
    |    20   | 0.6007899999999999 |
    +---------+--------------------+

```python
# conditional distributions
xc1 = x[0, x[1, :] == ys[0]]
xc2 = x[0, x[1, :] == ys[1]]
yc1 = x[1, x[0, :] == xs[0]]
yc2 = x[1, x[0, :] == xs[1]]

xc1p = np.sum(xc1 == xs[0])/len(xc1)
xc2p = np.sum(xc2 == xs[0])/len(xc2)
yc1p = np.sum(yc1 == ys[0])/len(yc1)
yc2p = np.sum(yc2 == ys[0])/len(yc2)

# print output
print("conditional distribution for x")
xctb = pt.PrettyTable()
xctb.field_names = ['y_value', 'prob(x=0)', 'prob(x=1)']
xctb.add_row([ys[0], xc1p, 1-xc1p])
xctb.add_row([ys[1], xc2p, 1-xc2p])
print(xctb)

print("\nconditional distribution for y")
yctb = pt.PrettyTable()
yctb.field_names = ['x_value',  'prob(y=10)', 'prob(y=20)']
yctb.add_row([xs[0], yc1p, 1-yc1p])
yctb.add_row([xs[1], yc2p, 1-yc2p])
print(yctb)
```

    conditional distribution for x
    +---------+---------------------+---------------------+
    | y_value |      prob(x=0)      |      prob(x=1)      |
    +---------+---------------------+---------------------+
    |    10   |  0.7505373111895994 | 0.24946268881040057 |
    |    20   | 0.33324289685247754 |  0.6667571031475225 |
    +---------+---------------------+---------------------+

    conditional distribution for y
    +---------+---------------------+---------------------+
    | x_value |      prob(y=10)     |      prob(y=20)     |
    +---------+---------------------+---------------------+
    |    0    |  0.5994466129551789 | 0.40055338704482113 |
    |    1    | 0.19910870125897448 |  0.8008912987410255 |
    +---------+---------------------+---------------------+

Let's compute population marginal and conditional probabilities using matrix algebra.

$$
\left[\begin{array}{cccccc}
\ & \vdots & y_{1} & y_{2} & \vdots & x\\
\cdots & \vdots & \cdots & \cdots & \vdots & \cdots\\
x_{1} & \vdots & 0.3 & 0.2 & \vdots & 0.5\\
x_{2} & \vdots & 0.1 & 0.4 & \vdots & 0.5\\
\cdots & \vdots & \cdots & \cdots & \vdots & \cdots\\
y & \vdots & 0.4 & 0.6 & \vdots & 1
\end{array}\right]
$$

$ \implies $

(1) Marginal distribution:

$$
\left[\begin{array}{cccccc}
var & \vdots & var_1 & var_2 \\
\cdots & \vdots & \cdots & \cdots \\
x & \vdots & 0.5 & 0.5 \\
\cdots & \vdots & \cdots & \cdots \\
y & \vdots & 0.4 & 0.6 \\
\end{array}\right]
$$

(2) Conditional distribution:

$$
\left[\begin{array}{cccccc}
\quad x & \vdots & \quad x_1 & \quad x_2 \\
\cdots\cdots\cdots & \vdots & \cdots\cdots\cdots & \cdots\cdots\cdots \\
y=y_1 & \vdots & \frac{0.3}{0.4}=0.75 & \frac{0.1}{0.4}=0.25 \\
\cdots\cdots\cdots & \vdots & \cdots\cdots\cdots & \cdots\cdots\cdots \\
y=y_2 & \vdots & \frac{0.2}{0.6}\approx 0.33 & \frac{0.4}{0.6}\approx0.67 \\
\end{array}\right]
$$

$$
\left[\begin{array}{cccccc}
\quad y & \vdots & \quad y_1 & \quad y_2 \\
\cdots\cdots\cdots & \vdots & \cdots\cdots\cdots & \cdots\cdots\cdots \\
x=x_1 & \vdots & \frac{0.3}{0.5}=0.6 & \frac{0.2}{0.5}=0.4 \\
\cdots\cdots\cdots & \vdots & \cdots\cdots\cdots & \cdots\cdots\cdots \\
x=x_2 & \vdots & \frac{0.1}{0.5}=0.2 & \frac{0.4}{0.5}=0.8 \\
\end{array}\right]
$$

These population objects closely resemble the sample counterparts computed earlier.

Let's encapsulate some of the functions we've used in a Python class for a general discrete bivariate joint distribution.

```python
class discrete_bijoint:

    def __init__(self, f, xs, ys):
        '''initialization
        -----------------
        parameters:
        f: the bivariate joint probability matrix
        xs: values of x vector
        ys: values of y vector
        '''
        self.f, self.xs, self.ys = f, xs, ys

    def joint_tb(self):
        '''print the joint distribution table'''
        xs = self.xs
        ys = self.ys
        f = self.f
        jtb = pt.PrettyTable()
        jtb.field_names = ['x_value/y_value', *ys, 'marginal sum for x']
        for i in range(len(xs)):
            jtb.add_row([xs[i], *f[i, :], np.sum(f[i, :])])
        jtb.add_row(['marginal_sum for y', *np.sum(f, 0), np.sum(f)])
        print("\nThe joint probability distribution for x and y\n", jtb)
        self.jtb = jtb

    def draw(self, n):
        '''draw random numbers
        ----------------------
        parameters:
        n: number of random numbers to draw
        '''
        xs = self.xs
        ys = self.ys
        f_cum = np.cumsum(self.f)
        p = np.random.rand(n)
        x = np.empty([2, p.shape[0]])
        lf = len(f_cum)
        lx = len(xs)-1
        ly = len(ys)-1
        for i in range(lf):
            x[0, p < f_cum[lf-1-i]] = xs[lx]
            x[1, p < f_cum[lf-1-i]] = ys[ly]
            if ly == 0:
                lx -= 1
                ly = len(ys)-1
            else:
                ly -= 1
        self.x = x
        self.n = n

    def marg_dist(self):
        '''marginal distribution'''
        x = self.x
        xs = self.xs
        ys = self.ys
        n = self.n
        xmp = [np.sum(x[0, :] == xs[i])/n for i in range(len(xs))]
        ymp = [np.sum(x[1, :] == ys[i])/n for i in range(len(ys))]

        # print output
        xmtb = pt.PrettyTable()
        ymtb = pt.PrettyTable()
        xmtb.field_names = ['x_value', 'x_prob']
        ymtb.field_names = ['y_value', 'y_prob']
        for i in range(max(len(xs), len(ys))):
            if i < len(xs):
                xmtb.add_row([xs[i], xmp[i]])
            if i < len(ys):
                ymtb.add_row([ys[i], ymp[i]])
        xmtb.add_row(['sum', np.sum(xmp)])
        ymtb.add_row(['sum', np.sum(ymp)])
        print("\nmarginal distribution for x\n", xmtb)
        print("\nmarginal distribution for y\n", ymtb)

        self.xmp = xmp
        self.ymp = ymp

    def cond_dist(self):
        '''conditional distribution'''
        x = self.x
        xs = self.xs
        ys = self.ys
        n = self.n
        xcp = np.empty([len(ys), len(xs)])
        ycp = np.empty([len(xs), len(ys)])
        for i in range(max(len(ys), len(xs))):
            if i < len(ys):
                xi = x[0, x[1, :] == ys[i]]
                idx = xi.reshape(len(xi), 1) == xs.reshape(1, len(xs))
                xcp[i, :] = np.sum(idx, 0)/len(xi)
            if i < len(xs):
                yi = x[1, x[0, :] == xs[i]]
                idy = yi.reshape(len(yi), 1) == ys.reshape(1, len(ys))
                ycp[i, :] = np.sum(idy, 0)/len(yi)

        # print output
        xctb = pt.PrettyTable()
        yctb = pt.PrettyTable()
        xctb.field_names = ['x_value', *xs, 'sum']
        yctb.field_names = ['y_value', *ys, 'sum']
        for i in range(max(len(xs), len(ys))):
            if i < len(ys):
                xctb.add_row([ys[i], *xcp[i], np.sum(xcp[i])])
            if i < len(xs):
                yctb.add_row([xs[i], *ycp[i], np.sum(ycp[i])])
        print("\nconditional distribution for x\n", xctb)
        print("\nconditional distribution for y\n", yctb)

        self.xcp = xcp
        self.xyp = ycp
```

Let's apply our code to some examples.

**Example 1**

```python
# joint
d = discrete_bijoint(f, xs, ys)
d.joint_tb()
```

    The joint probability distribution for x and y
     +--------------------+-----+--------------------+--------------------+
    |  x_value/y_value   |  10 |         20         | marginal sum for x |
    +--------------------+-----+--------------------+--------------------+
    |         0          | 0.3 |        0.2         |        0.5         |
    |         1          | 0.1 |        0.4         |        0.5         |
    | marginal_sum for y | 0.4 | 0.6000000000000001 |        1.0         |
    +--------------------+-----+--------------------+--------------------+

```python
# sample marginal
d.draw(1_000_000)
d.marg_dist()
```

    marginal distribution for x
     +---------+----------+
    | x_value |  x_prob  |
    +---------+----------+
    |    0    | 0.499712 |
    |    1    | 0.500288 |
    |   sum   |   1.0    |
    +---------+----------+

    marginal distribution for y
     +---------+----------+
    | y_value |  y_prob  |
    +---------+----------+
    |    10   | 0.399738 |
    |    20   | 0.600262 |
    |   sum   |   1.0    |
    +---------+----------+

```python
# sample conditional
d.cond_dist()
```

    conditional distribution for x
     +---------+--------------------+---------------------+-----+
    | x_value |         0          |          1          | sum |
    +---------+--------------------+---------------------+-----+
    |    10   | 0.7504415392081814 | 0.24955846079181865 | 1.0 |
    |    20   | 0.3327413696019405 |  0.6672586303980595 | 1.0 |
    +---------+--------------------+---------------------+-----+

    conditional distribution for y
     +---------+---------------------+---------------------+-----+
    | y_value |          10         |          20         | sum |
    +---------+---------------------+---------------------+-----+
    |    0    |  0.6003057761270492 | 0.39969422387295084 | 1.0 |
    |    1    | 0.19940114494051425 |  0.8005988550594857 | 1.0 |
    +---------+---------------------+---------------------+-----+

**Example 2**

```python
xs_new = np.array([10, 20, 30])
ys_new = np.array([1, 2])
f_new = np.array([[0.2, 0.1], [0.1, 0.3], [0.15, 0.15]])
d_new = discrete_bijoint(f_new, xs_new, ys_new)
d_new.joint_tb()
```

    The joint probability distribution for x and y
     +--------------------+---------------------+------+---------------------+
    |  x_value/y_value   |          1          |  2   |  marginal sum for x |
    +--------------------+---------------------+------+---------------------+
    |         10         |         0.2         | 0.1  | 0.30000000000000004 |
    |         20         |         0.1         | 0.3  |         0.4         |
    |         30         |         0.15        | 0.15 |         0.3         |
    | marginal_sum for y | 0.45000000000000007 | 0.55 |         1.0         |
    +--------------------+---------------------+------+---------------------+

```python
d_new.draw(1_000_000)
d_new.marg_dist()
```

    marginal distribution for x
     +---------+----------+
    | x_value |  x_prob  |
    +---------+----------+
    |    10   | 0.299808 |
    |    20   | 0.400905 |
    |    30   | 0.299287 |
    |   sum   |   1.0    |
    +---------+----------+

    marginal distribution for y
     +---------+----------+
    | y_value |  y_prob  |
    +---------+----------+
    |    1    | 0.449377 |
    |    2    | 0.550623 |
    |   sum   |   1.0    |
    +---------+----------+

```python
d_new.cond_dist()
```

    conditional distribution for x
     +---------+---------------------+--------------------+---------------------+-----+
    | x_value |          10         |         20         |          30         | sum |
    +---------+---------------------+--------------------+---------------------+-----+
    |    1    | 0.44443307067339893 | 0.222089693063953  |  0.3334772362626481 | 1.0 |
    |    2    | 0.18177591564464252 | 0.5468405787626016 | 0.27138350559275587 | 1.0 |
    +---------+---------------------+--------------------+---------------------+-----+

    conditional distribution for y
     +---------+---------------------+--------------------+-----+
    | y_value |          1          |         2          | sum |
    +---------+---------------------+--------------------+-----+
    |    10   |  0.666153004589604  | 0.333846995410396  | 1.0 |
    |    20   | 0.24894176924707848 | 0.7510582307529216 | 1.0 |
    |    30   |  0.5007133620905686 | 0.4992866379094314 | 1.0 |
    +---------+---------------------+--------------------+-----+

## A Continuous Bivariate Random Vector

A two-dimensional Gaussian distribution has a joint density given by

$$
f(x,y) =(2\pi\sigma_1\sigma_2\sqrt{1-\rho^2})^{-1}\exp\left[-\frac{1}{2(1-\rho^2)}\left(\frac{(x-\mu_1)^2}{\sigma_1^2}-\frac{2\rho(x-\mu_1)(y-\mu_2)}{\sigma_1\sigma_2}+\frac{(y-\mu_2)^2}{\sigma_2^2}\right)\right]
$$

$$
\frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}\exp\left[-\frac{1}{2(1-\rho^2)}\left(\frac{(x-\mu_1)^2}{\sigma_1^2}-\frac{2\rho(x-\mu_1)(y-\mu_2)}{\sigma_1\sigma_2}+\frac{(y-\mu_2)^2}{\sigma_2^2}\right)\right]
$$

We'll start with a bivariate normal distribution characterized by

$$
\mu=\left[\begin{array}{c}
0\\
5
\end{array}\right],\quad\Sigma=\left[\begin{array}{cc}
5 & .2\\
.2 & 1
\end{array}\right]
$$

```python
# define the joint probability density function
def func(x, y, μ1=0, μ2=5, σ1=np.sqrt(5), σ2=np.sqrt(1), ρ=.2/np.sqrt(5*1)):
    A = (2 * np.pi * σ1 * σ2 * np.sqrt(1 - ρ**2))**(-1)
    B = -1 / 2 / (1 - ρ**2)
    C1 = (x - μ1)**2 / σ1**2
    C2 = 2 * ρ * (x - μ1) * (y - μ2) / σ1 / σ2
    C3 = (y - μ2)**2 / σ2**2
    return A * np.exp(B * (C1 - C2 + C3))
```

```python
μ1 = 0
μ2 = 5
σ1 = np.sqrt(5)
σ2 = np.sqrt(1)
ρ = .2 / np.sqrt(5 * 1)
```

```python
x = np.linspace(-10, 10, 1_000)
y = np.linspace(-10, 10, 1_000)
x_mesh, y_mesh = np.meshgrid(x, y, indexing="ij")
```

**Joint Distribution**

Let's visualize the **population** joint density.

```python
# %matplotlib notebook

fig = plt.figure()
ax = plt.axes(projection='3d')

surf = ax.plot_surface(x_mesh, y_mesh, func(x_mesh, y_mesh), cmap='viridis')
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/8.%20Elementary%20Probability%20with%20Matrices_files/8.%20Elementary%20Probability%20with%20Matrices_61_0.png)
</div>

```python
# %matplotlib notebook

fig = plt.figure()
ax = plt.axes(projection='3d')

curve = ax.contour(x_mesh, y_mesh, func(x_mesh, y_mesh), zdir='x')
plt.ylabel('y')
ax.set_zlabel('f')
ax.set_xticks([])
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/8.%20Elementary%20Probability%20with%20Matrices_files/8.%20Elementary%20Probability%20with%20Matrices_62_0.png)
</div>

Now we'll generate samples using a built-in `numpy` function and compute a **sample** marginal distribution from the sample mean and variance.

```python
μ= np.array([0, 5])
σ= np.array([[5, .2], [.2, 1]])
n = 1_000_000
data = np.random.multivariate_normal(μ, σ, n)
x = data[:, 0]
y = data[:, 1]
```

**Marginal distribution**

```python
plt.hist(x, bins=1_000, alpha=0.6)
μx_hat, σx_hat = np.mean(x), np.std(x)
print(μx_hat, σx_hat)
x_sim = np.random.normal(μx_hat, σx_hat, 1_000_000)
plt.hist(x_sim, bins=1_000, alpha=0.4, histtype="step")
plt.show()
```

    0.0014492885934760192 2.236446251431038

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/8.%20Elementary%20Probability%20with%20Matrices_files/8.%20Elementary%20Probability%20with%20Matrices_66_1.png)
</div>

```python
plt.hist(y, bins=1_000, density=True, alpha=0.6)
μy_hat, σy_hat = np.mean(y), np.std(y)
print(μy_hat, σy_hat)
y_sim = np.random.normal(μy_hat, σy_hat, 1_000_000)
plt.hist(y_sim, bins=1_000, density=True, alpha=0.4, histtype="step")
plt.show()
```

    5.000013481025038 1.0005020946563545

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/8.%20Elementary%20Probability%20with%20Matrices_files/8.%20Elementary%20Probability%20with%20Matrices_67_1.png)
</div>

**Conditional distribution**

The theoretical conditional distribution is given by

$$
\begin{aligned} \\
[X|Y &= y ]\sim \mathbb{N}\bigg[\mu_X+\rho\sigma_X\frac{y-\mu_Y}{\sigma_Y},\sigma_X^2(1-\rho^2)\bigg] \\
[Y|X &= x ]\sim \mathbb{N}\bigg[\mu_Y+\rho\sigma_Y\frac{x-\mu_X}{\sigma_X},\sigma_Y^2(1-\rho^2)\bigg]
\end{aligned}
$$

To approximate the joint density, we'll discretize it and represent it as a matrix.

We can then calculate the discretized marginal density using matrix algebra, noting that

$$
\textrm{Prob}\{X=i|Y=j\}=\frac{f_{ij}}{\sum_{i}f_{ij}}
$$

Let's set $ y=0 $.

```python
# discretized marginal density
x = np.linspace(-10, 10, 1_000_000)
z = func(x, y=0) / np.sum(func(x, y=0))
plt.plot(x, z)
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/8.%20Elementary%20Probability%20with%20Matrices_files/8.%20Elementary%20Probability%20with%20Matrices_69_0.png)
</div>

We compute the mean and variance as follows:

$$
\begin{aligned}
\mathbb{E}\left[X\vert Y=j\right] & =\sum_{i}iProb\{X=i\vert Y=j\}=\sum_{i}i\frac{f_{ij}}{\sum_{i}f_{ij}} \\
\mathbb{D}\left[X\vert Y=j\right] &=\sum_{i}\left(i-\mu_{X\vert Y=j}\right)^{2}\frac{f_{ij}}{\sum_{i}f_{ij}}
\end{aligned}
$$

Now, let's sample from a normal distribution using these calculated mean and variance values to assess the accuracy of our approximation.

```python
# discretized mean
μx = np.dot(x, z)

# discretized standard deviation
σx = np.sqrt(np.dot((x - μx)**2, z))

# sample
zz = np.random.normal(μx, σx, 1_000_000)
plt.hist(zz, bins=300, density=True, alpha=0.3, range=[-10, 10])
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/8.%20Elementary%20Probability%20with%20Matrices_files/8.%20Elementary%20Probability%20with%20Matrices_71_0.png)
</div>

Now, let's set $ x=1 $.

```python
y = np.linspace(0, 10, 1_000_000)
z = func(x=1, y=y) / np.sum(func(x=1, y=y))
plt.plot(y,z)
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/8.%20Elementary%20Probability%20with%20Matrices_files/8.%20Elementary%20Probability%20with%20Matrices_73_0.png)
</div>

```python
# discretized mean and standard deviation
μy = np.dot(y,z)
σy = np.sqrt(np.dot((y - μy)**2, z))

# sample
zz = np.random.normal(μy,σy,1_000_000)
plt.hist(zz, bins=100, density=True, alpha=0.3)
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/8.%20Elementary%20Probability%20with%20Matrices_files/8.%20Elementary%20Probability%20with%20Matrices_74_0.png)
</div>

We'll compare these results with the analytically computed parameters and observe that they are quite similar.

```python
print(μx, σx)
print(μ1 + ρ * σ1 * (0 - μ2) / σ2, np.sqrt(σ1**2 * (1 - ρ**2)))

print(μy, σy)
print(μ2 + ρ * σ2 * (1 - μ1) / σ1, np.sqrt(σ2**2 * (1 - ρ**2)))
```

    -0.9997518414498721 2.226584133169757
    -1.0 2.227105745132009
    5.039999456960841 0.9959851265795615
    5.04 0.9959919678390986

## Sum of Two Independent Random Variables

Consider two independent discrete random variables $ X $ and $ Y $ with values in $ \bar{X} $ and $ \bar{Y} $, respectively.

Let's define a new random variable $ Z=X+Y $.

Clearly, $ Z $ takes values from $ \bar{Z} $ defined as:

$$
\begin{aligned}
\bar{X} & =\{0,1,\ldots,I-1\};\qquad f_i= \textrm{Prob} \{X=i\}\\
\bar{Y} & =\{0,1,\ldots,J-1\};\qquad g_j= \textrm{Prob}\{Y=j\}\\
\bar{Z}& =\{0,1,\ldots,I+J-2\};\qquad h_k= \textrm{Prob} \{X+Y=k\}
\end{aligned}
$$

Given that X and Y are independent, we have:

$$
\begin{aligned}
h_k & =\textrm{Prob}\{X=0,Y=k\}+\textrm{Prob}\{X=1,Y=k-1\}+\ldots+\textrm{Prob}\{X=k,Y=0\}\\
h_k& =f_0g_k+f_1g_{k-1}+\ldots+f_{k-1}g_1+f_kg_0 \qquad \text{for}\quad k=0,1,\ldots,I+J-2
\end{aligned}
$$

This can be expressed more concisely as:

$$
h_k=\sum_{i=0}^{k} f_ig_{k-i} \equiv f*g
$$

where $ f \* g $ denotes the **convolution** of the sequences $ f $ and $ g $.

For continuous random variables $ X $ and $ Y $ with densities $ f*{X} $ and $ g*{Y} $, the density of $ Z=X+Y $ is given by:

$$
f_{Z}(z)=\int_{-\infty}^{\infty} f_{X}(x) f_{Y}(z-x) dx \equiv f_{X}*g_{Y}
$$

where $ f*{X}\*g*{Y} $ represents the **convolution** of the functions $ f_X $ and $ g_Y $.

## Transition Probability Matrix

Let's examine a joint probability distribution of two discrete random variables.

Consider $ X $ and $ Y $ with joint distribution

$$
\textrm{Prob}\{X=i,Y=j\} = \rho_{ij}
$$

where $ i = 0,\dots,I-1; j = 0,\dots,J-1 $ and

$$
\sum_i\sum_j \rho_{ij} = 1, \quad \rho_{ij} \geqslant 0.
$$

The associated conditional distribution is

$$
\textrm{Prob}\{Y=i\vert X=j\} = \frac{\rho_{ij}}{ \sum_{i}\rho_{ij}}
= \frac{\textrm{Prob}\{Y=j,... X=i\}}{\textrm{Prob}\{ X=i\}}
$$

We can define a transition probability matrix as

$$
p_{ij}=\textrm{Prob}\{Y=j|X=i\}= \frac{\rho_{ij}}{ \sum_{j}\rho_{ij}}
$$

which can be represented as

$$
\left[
\begin{matrix}
p_{11} & p_{12}\\
p_{21} & p_{22}
\end{matrix}
\right]
$$

Here, the first row gives the probability of $ Y=j, j=0,1 $ given $ X=0 $, and the second row gives the probability of $ Y=j, j=0,1 $ given $ X=1 $.

Note that:

- $ \sum*{j}\rho*{ij}= \frac{ \sum*{j}\rho*{ij}}{ \sum*{j}\rho*{ij}}=1 $, so each row of $ \rho $ represents a probability distribution (this is not true for each column).

## Coupling

Let's start with a joint distribution

$$
\begin{aligned}
f_{ij} & =\textrm{Prob}\{X=i,Y=j\}\\
i& =0, \cdots，I-1\\
j& =0, \cdots，J-1\\
& \text{stacked to an }I×J\text{ matrix}\\
& e.g. \quad I=1, J=1
\end{aligned}
$$

which can be represented as

$$
\left[
\begin{matrix}
f_{11} & f_{12}\\
f_{21} & f_{22}
\end{matrix}
\right]
$$

We've shown that unique marginal distributions can be derived from this joint distribution.

Now, let's consider the reverse direction. Given two marginal distributions, we can typically construct multiple joint distributions that are consistent with these marginals.

Each of these joint distributions is called a **coupling** of the two marginal distributions.

Starting with marginal distributions

$$
\begin{aligned}
\text{Prob} \{X=i\} &= \sum_{j}f_{ij}=\mu_{i}, i=0, \cdots, I-1\\
\text{Prob} \{Y=j\}&= \sum_{j}f_{ij}=\nu_{j}, j=0, \cdots, J-1
\end{aligned}
$$

Given marginal distributions $ \mu $ for $ X $ and $ \nu $ for $ Y $, a joint distribution $ f\_{ij} $ is termed a **coupling** of $ \mu $ and $ \nu $.

**Example:**

Consider this bivariate example:

$$
\begin{aligned}
\text{Prob} \{X=0\}= & 1-q =\mu_{0}\\
\text{Prob} \{X=1\}=& q =\mu_{1}\\
\text{Prob} \{Y=0\}=& 1-r =\nu_{0}\\
\text{Prob} \{Y=1\}= & r =\nu_{1}\\
\text{where } 0 \leq q < r \leq 1
\end{aligned}
$$

Let's construct two different couplings.

The first coupling of our marginal distributions is the joint distribution

$$
f_{ij}=
\left[
\begin{matrix}
(1-q)(1-r)& (1-q)r\\
q(1-r) & qr\\
\end{matrix}
\right]
$$

To verify this is a coupling, we check:

$$
\begin{aligned}
(1-q)(1-r)+(1-q)r+q(1-r)+qr &=1\\
\mu_{0}= (1-q)(1-r)+(1-q)r & =1-q\\
\mu_{1}= q(1-r)+qr & =q\\
\nu_{0}= (1-q)(1-r)+(1-r)q& =1-r\\
\mu_{1}= r(1-q)+qr& =r
\end{aligned}
$$

A second coupling of our marginal distributions is the joint distribution

$$
f_{ij}=
\left[
\begin{matrix}
(1-r)&r-q\\
0 & q\\
\end{matrix}
\right]
$$

To verify this is a coupling, we note:

$$
\begin{aligned}
1-r+r-q+q &=1\\
\mu_{0}& = 1-q\\
\mu_{1}& = q\\
\nu_{0}& = 1-r\\
\nu_{1}& = r
\end{aligned}
$$

Thus, our two proposed joint distributions have identical marginal distributions, but the joint distributions differ.

This demonstrates that multiple joint distributions $ [f_{ij}] $ can share the same marginals.

**Remark:**

- Couplings play a significant role in optimal transport problems and Markov processes.

## Copula Functions

Consider $ N $ random variables $ X_1, X_2, \dots, X_n $ where

- their marginal distributions are $ F_1(x_1), F_2(x_2),\dots, F_N(x_N) $, and
- their joint distribution is $ H(x_1,x_2,\dots,x_N) $

A **copula function** $ C(\cdot) $ exists that satisfies

$$
H(x_1,x_2,\dots,x_N) = C(F_1(x_1), F_2(x_2),\dots,F_N(x_N)).
$$

We can derive

$$
C(u_1,u_2,\dots,u_n) = H[F^{-1}_1(u_1),F^{-1}_2(u_2),\dots,F^{-1}_N(u_N)]
$$

Conversely, given univariate **marginal distributions**
$ F_1(x_1), F_2(x_2),\dots,F_N(x_N) $ and a copula function $ C(\cdot) $, the function $ H(x_1,x_2,\dots,x_N) = C(F_1(x_1), F_2(x_2),\dots,F_N(x_N)) $ is a **coupling** of $ F_1(x_1), F_2(x_2),\dots,F_N(x_N) $.

Thus, for given marginal distributions, we can use a copula function to determine a joint distribution when the associated univariate random variables are not independent.

Copula functions are frequently used to describe **dependence** among random variables.

**Discrete marginal distribution**

As mentioned earlier, for two given marginal distributions, multiple couplings can exist.

For instance, consider two random variables $ X, Y $ with distributions

$$
\begin{aligned}
\text{Prob}(X = 0)& = 0.6,\\
\text{Prob}(X = 1) &= 0.4,\\
\text{Prob}(Y = 0)& = 0.3,\\
\text{Prob}(Y = 1) &= 0.7,
\end{aligned}
$$

For these two random variables, more than one coupling is possible.

Let's first generate X and Y.

```python
# define parameters
mu = np.array([0.6, 0.4])
nu = np.array([0.3, 0.7])

# number of draws
draws = 1_000_000

# generate draws from uniform distribution
p = np.random.rand(draws)

# generate draws of X and Y via uniform distribution
x = np.ones(draws)
y = np.ones(draws)
x[p <= mu[0]] = 0
x[p > mu[0]] = 1
y[p <= nu[0]] = 0
y[p > nu[0]] = 1
```

```python
# calculate parameters from draws
q_hat = sum(x[x == 1])/draws
r_hat = sum(y[y == 1])/draws

# print output
print("distribution for x")
xmtb = pt.PrettyTable()
xmtb.field_names = ['x_value', 'x_prob']
xmtb.add_row([0, 1-q_hat])
xmtb.add_row([1, q_hat])
print(xmtb)

print("distribution for y")
ymtb = pt.PrettyTable()
ymtb.field_names = ['y_value', 'y_prob']
ymtb.add_row([0, 1-r_hat])
ymtb.add_row([1, r_hat])
print(ymtb)
```

    distribution for x
    +---------+----------+
    | x_value |  x_prob  |
    +---------+----------+
    |    0    | 0.599182 |
    |    1    | 0.400818 |
    +---------+----------+
    distribution for y
    +---------+---------------------+
    | y_value |        y_prob       |
    +---------+---------------------+
    |    0    | 0.29937800000000003 |
    |    1    |       0.700622      |
    +---------+---------------------+

Now, let's take our two marginal distributions for $ X $ and $ Y $, and construct two distinct couplings.

For the first joint distribution:

$$
\textrm{Prob}(X=i,Y=j) = f_{ij}
$$

where

$$
[f_{ij}] = \left[\begin{array}{cc}
0.18 & 0.42\\
0.12 & 0.28
\end{array}\right]
$$

Let's use Python to create this joint distribution and verify that its marginal distributions match our requirements.

```python
# define parameters
f1 = np.array([[0.18, 0.42], [0.12, 0.28]])
f1_cum = np.cumsum(f1)

# number of draws
draws1 = 1_000_000

# generate draws from uniform distribution
p = np.random.rand(draws1)

# generate draws of first copuling via uniform distribution
c1 = np.vstack([np.ones(draws1), np.ones(draws1)])
# X=0, Y=0
c1[0, p <= f1_cum[0]] = 0
c1[1, p <= f1_cum[0]] = 0
# X=0, Y=1
c1[0, (p > f1_cum[0])*(p <= f1_cum[1])] = 0
c1[1, (p > f1_cum[0])*(p <= f1_cum[1])] = 1
# X=1, Y=0
c1[0, (p > f1_cum[1])*(p <= f1_cum[2])] = 1
c1[1, (p > f1_cum[1])*(p <= f1_cum[2])] = 0
# X=1, Y=1
c1[0, (p > f1_cum[2])*(p <= f1_cum[3])] = 1
c1[1, (p > f1_cum[2])*(p <= f1_cum[3])] = 1
```

```python
# calculate parameters from draws
f1_00 = sum((c1[0, :] == 0)*(c1[1, :] == 0))/draws1
f1_01 = sum((c1[0, :] == 0)*(c1[1, :] == 1))/draws1
f1_10 = sum((c1[0, :] == 1)*(c1[1, :] == 0))/draws1
f1_11 = sum((c1[0, :] == 1)*(c1[1, :] == 1))/draws1

# print output of first joint distribution
print("first joint distribution for c1")
c1_mtb = pt.PrettyTable()
c1_mtb.field_names = ['c1_x_value', 'c1_y_value', 'c1_prob']
c1_mtb.add_row([0, 0, f1_00])
c1_mtb.add_row([0, 1, f1_01])
c1_mtb.add_row([1, 0, f1_10])
c1_mtb.add_row([1, 1, f1_11])
print(c1_mtb)
```

    first joint distribution for c1
    +------------+------------+----------+
    | c1_x_value | c1_y_value | c1_prob  |
    +------------+------------+----------+
    |     0      |     0      | 0.180346 |
    |     0      |     1      | 0.419526 |
    |     1      |     0      | 0.120096 |
    |     1      |     1      | 0.280032 |
    +------------+------------+----------+

```python
# calculate parameters from draws
c1_q_hat = sum(c1[0, :] == 1)/draws1
c1_r_hat = sum(c1[1, :] == 1)/draws1

# print output
print("marginal distribution for x")
c1_x_mtb = pt.PrettyTable()
c1_x_mtb.field_names = ['c1_x_value', 'c1_x_prob']
c1_x_mtb.add_row([0, 1-c1_q_hat])
c1_x_mtb.add_row([1, c1_q_hat])
print(c1_x_mtb)

print("marginal distribution for y")
c1_ymtb = pt.PrettyTable()
c1_ymtb.field_names = ['c1_y_value', 'c1_y_prob']
c1_ymtb.add_row([0, 1-c1_r_hat])
c1_ymtb.add_row([1, c1_r_hat])
print(c1_ymtb)
```

    marginal distribution for x
    +------------+-----------+
    | c1_x_value | c1_x_prob |
    +------------+-----------+
    |     0      |  0.599872 |
    |     1      |  0.400128 |
    +------------+-----------+
    marginal distribution for y
    +------------+-----------+
    | c1_y_value | c1_y_prob |
    +------------+-----------+
    |     0      |  0.300442 |
    |     1      |  0.699558 |
    +------------+-----------+

Now, let's construct another joint distribution that is also a coupling of $ X $ and $ Y $

$$
[f_{ij}] = \left[\begin{array}{cc}
0.3 & 0.3\\
0 & 0.4
\end{array}\right]
$$

```python
# define parameters
f2 = np.array([[0.3, 0.3], [0, 0.4]])
f2_cum = np.cumsum(f2)

# number of draws
draws2 = 1_000_000

# generate draws from uniform distribution
p = np.random.rand(draws2)

# generate draws of first coupling via uniform distribution
c2 = np.vstack([np.ones(draws2), np.ones(draws2)])
# X=0, Y=0
c2[0, p <= f2_cum[0]] = 0
c2[1, p <= f2_cum[0]] = 0
# X=0, Y=1
c2[0, (p > f2_cum[0])*(p <= f2_cum[1])] = 0
c2[1, (p > f2_cum[0])*(p <= f2_cum[1])] = 1
# X=1, Y=0
c2[0, (p > f2_cum[1])*(p <= f2_cum[2])] = 1
c2[1, (p > f2_cum[1])*(p <= f2_cum[2])] = 0
# X=1, Y=1
c2[0, (p > f2_cum[2])*(p <= f2_cum[3])] = 1
c2[1, (p > f2_cum[2])*(p <= f2_cum[3])] = 1
```

```python
# calculate parameters from draws
f2_00 = sum((c2[0, :] == 0)*(c2[1, :] == 0))/draws2
f2_01 = sum((c2[0, :] == 0)*(c2[1, :] == 1))/draws2
f2_10 = sum((c2[0, :] == 1)*(c2[1, :] == 0))/draws2
f2_11 = sum((c2[0, :] == 1)*(c2[1, :] == 1))/draws2

# print output of second joint distribution
print("first joint distribution for c2")
c2_mtb = pt.PrettyTable()
c2_mtb.field_names = ['c2_x_value', 'c2_y_value', 'c2_prob']
c2_mtb.add_row([0, 0, f2_00])
c2_mtb.add_row([0, 1, f2_01])
c2_mtb.add_row([1, 0, f2_10])
c2_mtb.add_row([1, 1, f2_11])
print(c2_mtb)
```

    first joint distribution for c2
    +------------+------------+----------+
    | c2_x_value | c2_y_value | c2_prob  |
    +------------+------------+----------+
    |     0      |     0      | 0.300426 |
    |     0      |     1      | 0.300706 |
    |     1      |     0      |   0.0    |
    |     1      |     1      | 0.398868 |
    +------------+------------+----------+

```python
# calculate parameters from draws
c2_q_hat = sum(c2[0, :] == 1)/draws2
c2_r_hat = sum(c2[1, :] == 1)/draws2

# print output
print("marginal distribution for x")
c2_x_mtb = pt.PrettyTable()
c2_x_mtb.field_names = ['c2_x_value', 'c2_x_prob']
c2_x_mtb.add_row([0, 1-c2_q_hat])
c2_x_mtb.add_row([1, c2_q_hat])
print(c2_x_mtb)

print("marginal distribution for y")
c2_ymtb = pt.PrettyTable()
c2_ymtb.field_names = ['c2_y_value', 'c2_y_prob']
c2_ymtb.add_row([0, 1-c2_r_hat])
c2_ymtb.add_row([1, c2_r_hat])
print(c2_ymtb)
```

    marginal distribution for x
    +------------+-----------+
    | c2_x_value | c2_x_prob |
    +------------+-----------+
    |     0      |  0.601132 |
    |     1      |  0.398868 |
    +------------+-----------+
    marginal distribution for y
    +------------+---------------------+
    | c2_y_value |      c2_y_prob      |
    +------------+---------------------+
    |     0      | 0.30042599999999997 |
    |     1      |       0.699574      |
    +------------+---------------------+

We have confirmed that both joint distributions, $ c_1 $ and $ c_2 $, have identical marginal distributions for $ X $ and $ Y $, respectively.

Therefore, they are both valid couplings of $ X $ and $ Y $.

## Time Series

Consider a scenario with two time periods:

- $ t=0 $ "today"
- $ t=1 $ "tomorrow"

Let $ X(0) $ be a random variable realized at $ t=0 $, and $ X(1) $ be a random variable realized at $ t=1 $.

Assume that

$$
\begin{aligned}
\text{Prob} \{X(0)=i,X(1)=j\} &=f_{ij}≥0，i=0,\cdots,I-1\\
\sum_{i}\sum_{j}f_{ij}&=1
\end{aligned}
$$

$ f\_{ij} $ represents a joint distribution over $ [X(0), X(1)] $.

A conditional distribution is given by

$$
\text{Prob} \{X(1)=j|X(0)=i\}= \frac{f_{ij}}{ \sum_{j}f_{ij}}
$$

**Remark:**

- This formula is fundamental to the theory of optimal time series prediction.
