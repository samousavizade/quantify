---
title: Randomized Response Surveys
draft: false
summary: test
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

# Surveys with Randomized Responses

## Synopsis

Social pressures can prevent individuals from admitting to potentially embarrassing activities or views.

When people are hesitant to take part in a survey about sensitive personal matters, they might refuse to participate, or if they do, they might provide false answers to delicate questions.

These issues create **selection** biases that pose challenges in interpreting and designing surveys.

To demonstrate how researchers have approached estimating the prevalence of such sensitive activities and opinions, this presentation outlines a classic method by S. L. Warner [[Warner, 1965](/courses/Introduction-to-Quantitative-Economics/References#id10)].

Warner employed basic probability to devise a way to safeguard the privacy of **individual** survey respondents while still gauging the proportion of a **group** of individuals who possess a socially stigmatized trait or engage in a socially stigmatized behavior.

Warner's concept was to introduce **noise** between the respondent's answer and the **signal** that the survey creator ultimately receives.

Understanding the noise structure reassures the respondent that the survey creator cannot directly observe their answer.

Statistical aspects of the noise injection process provide the respondent with **plausible deniability**.

Similar ideas form the basis of modern **differential privacy** systems.

(See [https://en.wikipedia.org/wiki/Differential_privacy](https://en.wikipedia.org/wiki/Differential_privacy))

## Warner's Approach

As usual, let's import the Python modules we'll be utilizing.

```python
import numpy as np
import pandas as pd
```

Assume that every individual in a population belongs to either Group A or Group B.

Our goal is to estimate the proportion $ \pi $ belonging to Group A while protecting individual respondents' confidentiality.

Warner [[Warner, 1965](/courses/Introduction-to-Quantitative-Economics/References#id10)] suggested and examined the following procedure.

- Select a random sample of $ n $ individuals with replacement from the population and interview each one.
- Prepare a **randomizing device** that points to Letter A with $ p $ probability and to Letter B with $ (1-p) $ probability.
- Each participant uses the randomizing device and observes an outcome (A or B) that the interviewer does **not see**.
- The participant states whether they belong to the group indicated by the device.
- If the device points to the group the participant belongs to, they answer yes; otherwise, they answer no.
- The participant responds truthfully.

Warner developed maximum likelihood estimators for the population proportion in set A.

Let

- $ \pi $ : Actual probability of A in the population
- $ p $ : Probability that the device points to A
- $ X\_{i}=\begin{cases}1,\text{ if the } i\text{th} \ \text{ participant says yes}\\0,\text{ if the } i\text{th} \ \text{ participant says no}\end{cases} $

Arrange the sample set so that the first $ n_1 $ say yes, while the remaining $ n-n_1 $ say no.

The likelihood function of a sample set is

$$
L=\left[\pi p + (1-\pi)(1-p)\right]^{n_{1}}\left[(1-\pi) p +\pi (1-p)\right]^{n-n_{1}} \tag{15.1}
$$

The logarithm of the likelihood function is:

$$
\log(L)= n_1 \log \left[\pi p + (1-\pi)(1-p)\right] + (n-n_{1}) \log \left[(1-\pi) p +\pi (1-p)\right] \tag{15.2}
$$

The first-order necessary condition for maximizing the log likelihood function with respect to $ \pi $ is:

$$
\frac{(n-n_1)(2p-1)}{(1-\pi) p +\pi (1-p)}=\frac{n_1 (2p-1)}{\pi p + (1-\pi)(1-p)}
$$

or

$$
\pi p + (1-\pi)(1-p)=\frac{n_1}{n} \tag{15.3}
$$

If $ p \neq \frac{1}{2} $, then the maximum likelihood estimator (MLE) of $ \pi $ is:

$$
\hat{\pi}=\frac{p-1}{2p-1}+\frac{n_1}{(2p-1)n} \tag{15.4}
$$

We determine the mean and variance of the MLE estimator $ \hat \pi $ to be:

$$
\begin{aligned}
\mathbb{E}(\hat{\pi})&= \frac{1}{2 p-1}\left[p-1+\frac{1}{n} \sum_{i=1}^{n} \mathbb{E} X_i \right] \\
&=\frac{1}{2 p-1} \left[ p -1 + \pi p + (1-\pi)(1-p)\right] \\
&=\pi
\end{aligned} \tag{15.5}
$$

and

$$
\begin{aligned}
Var(\hat{\pi})&=\frac{n Var(X_i)}{(2p - 1 )^2 n^2} \\
&= \frac{\left[\pi p + (1-\pi)(1-p)\right]\left[(1-\pi) p +\pi (1-p)\right]}{(2p - 1 )^2 n^2}\\
&=\frac{\frac{1}{4}+(2 p^2 - 2 p +\frac{1}{2})(- 2 \pi^2 + 2 \pi -\frac{1}{2})}{(2p - 1 )^2 n^2}\\
&=\frac{1}{n}\left[\frac{1}{16(p-\frac{1}{2})^2}-(\pi-\frac{1}{2})^2 \right]
\end{aligned} \tag{15.6}
$$

Equation [(15.5)](#equation-eq-five) shows that $ \hat{\pi} $ is an **unbiased estimator** of $ \pi $ while equation [(15.6)](#equation-eq-six) provides the estimator's variance.

To calculate a confidence interval, first rewrite [(15.6)](#equation-eq-six) as:

$$
Var(\hat{\pi})=\frac{\frac{1}{4}-(\pi-\frac{1}{2})^2}{n}+\frac{\frac{1}{16(p-\frac{1}{2})^2}-\frac{1}{4}}{n} \tag{15.7}
$$

This equation indicates that $ \hat{\pi} $'s variance can be expressed as the sum of the variance due to sampling plus the variance due to the randomizing device.

From the above expressions, we can observe that:

- When $ p $ is $ \frac{1}{2} $, expression [(15.1)](#equation-eq-one) simplifies to a constant.
- When $ p $ is $ 1 $ or $ 0 $, the randomized estimate becomes an estimator without randomized sampling.

We will only consider situations where $ p \in (\frac{1}{2},1) $

(a scenario where $ p \in (0,\frac{1}{2}) $ is symmetrical).

From expressions [(15.5)](#equation-eq-five) and [(15.7)](#equation-eq-seven) we can infer that:

- The MSE of $ \hat{\pi} $ decreases as $ p $ increases.

## Contrasting Two Survey Designs

Let's compare the aforementioned randomized-response method with a simplified non-randomized response method.

In our non-randomized response method, we assume that:

- Members of Group A respond truthfully with probability $ T_a $ while members of Group B respond truthfully with probability $ T_b $
- $ Y_i $ is $ 1 $ or $ 0 $ based on whether the sample's $ i\text{th} $ member reports being in Group A or not.

We can then estimate $ \pi $ as:

$$
\hat{\pi}=\frac{\sum_{i=1}^{n}Y_i}{n} \tag{15.8}
$$

We compute the expectation, bias, and variance of the estimator to be:

$$
\begin{aligned}
\mathbb{E}(\hat{\pi})&=\pi T_a + \left[ (1-\pi)(1-T_b)\right]\\
\end{aligned} \tag{15.9}
$$

$$
\begin{aligned}
Bias(\hat{\pi})&=\mathbb{E}(\hat{\pi}-\pi)\\
&=\pi [T_a + T_b -2 ] + [1- T_b] \\
\end{aligned} \tag{15.10}
$$

$$
\begin{aligned}
Var(\hat{\pi})&=\frac{ \left[ \pi T_a + (1-\pi)(1-T_b)\right] \left[1- \pi T_a -(1-\pi)(1-T_b)\right] }{n}
\end{aligned} \tag{15.11}
$$

It's helpful to define a

$$
\text{MSE Ratio}=\frac{\text{Mean Square Error Randomized}}{\text{Mean Square Error Regular}}
$$

We can calculate MSE Ratios for various survey designs associated with different parameter values.

The following Python code computes objects we want to examine to make comparisons
under different values of $ \pi_A $ and $ n $:

```python
class Comparison:
    def __init__(self, A, n):
        self.A = A
        self.n = n
        TaTb = np.array([[0.95,  1], [0.9,   1], [0.7,    1],
                         [0.5,   1], [1,  0.95], [1,    0.9],
                         [1,   0.7], [1,   0.5], [0.95, 0.95],
                         [0.9, 0.9], [0.7, 0.7], [0.5,  0.5]])
        self.p_arr = np.array([0.6, 0.7, 0.8, 0.9])
        self.p_map = dict(zip(self.p_arr, [f"MSE Ratio: p = {x}" for x in self.p_arr]))
        self.template = pd.DataFrame(columns=self.p_arr)
        self.template[['T_a','T_b']] = TaTb
        self.template['Bias'] = None

    def theoretical(self):
        A = self.A
        n = self.n
        df = self.template.copy()
        df['Bias'] = A * (df['T_a'] + df['T_b'] - 2) + (1 - df['T_b'])
        for p in self.p_arr:
            df[p] = (1 / (16 * (p - 1/2)**2) - (A - 1/2)**2) / n / \
                    (df['Bias']**2 + ((A * df['T_a'] + (1 - A) * (1 - df['T_b'])) * (1 - A * df['T_a'] - (1 - A) * (1 - df['T_b'])) / n))
            df[p] = df[p].round(2)
        df = df.set_index(["T_a", "T_b", "Bias"]).rename(columns=self.p_map)
        return df

    def MCsimulation(self, size=1000, seed=123456):
        A = self.A
        n = self.n
        df = self.template.copy()
        np.random.seed(seed)
        sample = np.random.rand(size, self.n) <= A
        random_device = np.random.rand(size, n)
        mse_rd = {}
        for p in self.p_arr:
            spinner = random_device <= p
            rd_answer = sample * spinner + (1 - sample) * (1 - spinner)
            n1 = rd_answer.sum(axis=1)
            pi_hat = (p - 1) / (2 * p - 1) + n1 / n / (2 * p - 1)
            mse_rd[p] = np.sum((pi_hat - A)**2)
        for inum, irow in df.iterrows():
            truth_a = np.random.rand(size, self.n) <= irow.T_a
            truth_b = np.random.rand(size, self.n) <= irow.T_b
            trad_answer = sample * truth_a + (1 - sample) * (1 - truth_b)
            pi_trad = trad_answer.sum(axis=1) / n
            df.loc[inum, 'Bias'] = pi_trad.mean() - A
            mse_trad = np.sum((pi_trad - A)**2)
            for p in self.p_arr:
                df.loc[inum, p] = (mse_rd[p] / mse_trad).round(2)
        df = df.set_index(["T_a", "T_b", "Bias"]).rename(columns=self.p_map)
        return df
```

Let's apply the code for parameter values

- $ \pi_A=0.6 $
- $ n=1000 $

We can generate MSE Ratios theoretically using the above formulas.

We can also conduct Monte Carlo simulations of a MSE Ratio.

```python
cp1 = Comparison(0.6, 1000)
df1_theoretical = cp1.theoretical()
df1_theoretical
```

| \*\*\*\* | \*\*\*\* | \*\*\*\* | **MSE Ratio: p = 0.6** | **MSE Ratio: p = 0.7** | **MSE Ratio: p = 0.8** | **MSE Ratio: p = 0.9** |
| -------- | -------- | -------- | ---------------------- | ---------------------- | ---------------------- | ---------------------- |
| **T_a**  | T_b      | Bias     |                        |                        |                        |                        |
| **0.95** | 1.00     | -0.03    | 5.45                   | 1.36                   | 0.60                   | 0.33                   |
| **0.90** | 1.00     | -0.06    | 1.62                   | 0.40                   | 0.18                   | 0.10                   |
| **0.70** | 1.00     | -0.18    | 0.19                   | 0.05                   | 0.02                   | 0.01                   |
| **0.50** | 1.00     | -0.30    | 0.07                   | 0.02                   | 0.01                   | 0.00                   |
| **1.00** | 0.95     | 0.02     | 9.82                   | 2.44                   | 1.08                   | 0.60                   |
| **0.90** | 0.04     | 3.41     | 0.85                   | 0.37                   | 0.21                   |
| **0.70** | 0.12     | 0.43     | 0.11                   | 0.05                   | 0.03                   |
| **0.50** | 0.20     | 0.16     | 0.04                   | 0.02                   | 0.01                   |
| **0.95** | 0.95     | -0.01    | 18.25                  | 4.54                   | 2.00                   | 1.11                   |
| **0.90** | 0.90     | -0.02    | 9.70                   | 2.41                   | 1.06                   | 0.59                   |
| **0.70** | 0.70     | -0.06    | 1.62                   | 0.40                   | 0.18                   | 0.10                   |
| **0.50** | 0.50     | -0.10    | 0.61                   | 0.15                   | 0.07                   | 0.04                   |

```python
df1_mc = cp1.MCsimulation()
df1_mc
```

| \*\*\*\* | \*\*\*\* | \*\*\*\*  | **MSE Ratio: p = 0.6** | **MSE Ratio: p = 0.7** | **MSE Ratio: p = 0.8** | **MSE Ratio: p = 0.9** |
| -------- | -------- | --------- | ---------------------- | ---------------------- | ---------------------- | ---------------------- |
| **T_a**  | T_b      | Bias      |                        |                        |                        |                        |
| **0.95** | 1.00     | -0.030060 | 5.76                   | 1.36                   | 0.63                   | 0.35                   |
| **0.90** | 1.00     | -0.060045 | 1.73                   | 0.41                   | 0.19                   | 0.1                    |
| **0.70** | 1.00     | -0.179530 | 0.21                   | 0.05                   | 0.02                   | 0.01                   |
| **0.50** | 1.00     | -0.300077 | 0.07                   | 0.02                   | 0.01                   | 0.0                    |
| **1.00** | 0.95     | 0.019770  | 10.59                  | 2.5                    | 1.15                   | 0.64                   |
| **0.90** | 0.040050 | 3.63      | 0.86                   | 0.39                   | 0.22                   |
| **0.70** | 0.120052 | 0.46      | 0.11                   | 0.05                   | 0.03                   |
| **0.50** | 0.199746 | 0.17      | 0.04                   | 0.02                   | 0.01                   |
| **0.95** | 0.95     | -0.010137 | 18.65                  | 4.41                   | 2.02                   | 1.12                   |
| **0.90** | 0.90     | -0.020103 | 10.48                  | 2.48                   | 1.14                   | 0.63                   |
| **0.70** | 0.70     | -0.060488 | 1.71                   | 0.4                    | 0.19                   | 0.1                    |
| **0.50** | 0.50     | -0.099341 | 0.66                   | 0.16                   | 0.07                   | 0.04                   |

The theoretical calculations accurately predict Monte Carlo results.

We observe that in many cases, particularly when the bias is substantial, the MSE of the randomized-sampling methods is lower than that of the non-randomized sampling method.

These differences become more pronounced as $ p $ increases.

By adjusting parameters $ \pi_A $ and $ n $, we can explore outcomes in various scenarios.

For instance, for another situation described in Warner [[Warner, 1965](/courses/Introduction-to-Quantitative-Economics/References#id10)]:

- $ \pi_A=0.5 $
- $ n=1000 $

we can employ the code

```python
cp2 = Comparison(0.5, 1000)
df2_theoretical = cp2.theoretical()
df2_theoretical
```

<div>
  <table border="1" className="dataframe">
    <thead>
      <tr style="text-align: right;">
        <th></th>
        <th></th>
        <th></th>
        <th>MSE Ratio: p = 0.6</th>
        <th>MSE Ratio: p = 0.7</th>
        <th>MSE Ratio: p = 0.8</th>
        <th>MSE Ratio: p = 0.9</th>
      </tr>
      <tr>
        <th>T_a</th>
        <th>T_b</th>
        <th>Bias</th>
        <th></th>
        <th></th>
        <th></th>
        <th></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th>0.95</th>
        <th>1.00</th>
        <th>-0.025</th>
        <td>7.15</td>
        <td>1.79</td>
        <td>0.79</td>
        <td>0.45</td>
      </tr>
      <tr>
        <th>0.90</th>
        <th>1.00</th>
        <th>-0.050</th>
        <td>2.27</td>
        <td>0.57</td>
        <td>0.25</td>
        <td>0.14</td>
      </tr>
      <tr>
        <th>0.70</th>
        <th>1.00</th>
        <th>-0.150</th>
        <td>0.27</td>
        <td>0.07</td>
        <td>0.03</td>
        <td>0.02</td>
      </tr>
      <tr>
        <th>0.50</th>
        <th>1.00</th>
        <th>-0.250</th>
        <td>0.10</td>
        <td>0.02</td>
        <td>0.01</td>
        <td>0.01</td>
      </tr>
      <tr>
        <th rowSpan="4" valign="top">
          1.00
        </th>
        <th>0.95</th>
        <th>0.025</th>
        <td>7.15</td>
        <td>1.79</td>
        <td>0.79</td>
        <td>0.45</td>
      </tr>
      <tr>
        <th>0.90</th>
        <th>0.050</th>
        <td>2.27</td>
        <td>0.57</td>
        <td>0.25</td>
        <td>0.14</td>
      </tr>
      <tr>
        <th>0.70</th>
        <th>0.150</th>
        <td>0.27</td>
        <td>0.07</td>
        <td>0.03</td>
        <td>0.02</td>
      </tr>
      <tr>
        <th>0.50</th>
        <th>0.250</th>
        <td>0.10</td>
        <td>0.02</td>
        <td>0.01</td>
        <td>0.01</td>
      </tr>
      <tr>
        <th>0.95</th>
        <th>0.95</th>
        <th>0.000</th>
        <td>25.00</td>
        <td>6.25</td>
        <td>2.78</td>
        <td>1.56</td>
      </tr>
      <tr>
        <th>0.90</th>
        <th>0.90</th>
        <th>0.000</th>
        <td>25.00</td>
        <td>6.25</td>
        <td>2.78</td>
        <td>1.56</td>
      </tr>
      <tr>
        <th>0.70</th>
        <th>0.70</th>
        <th>0.000</th>
        <td>25.00</td>
        <td>6.25</td>
        <td>2.78</td>
        <td>1.56</td>
      </tr>
      <tr>
        <th>0.50</th>
        <th>0.50</th>
        <th>0.000</th>
        <td>25.00</td>
        <td>6.25</td>
        <td>2.78</td>
        <td>1.56</td>
      </tr>
    </tbody>
  </table>
</div>

```python
df2_mc = cp2.MCsimulation()
df2_mc
```

<div>
  <table border="1" className="dataframe">
    <thead>
      <tr style="text-align: right;">
        <th></th>
        <th></th>
        <th></th>
        <th>MSE Ratio: p = 0.6</th>
        <th>MSE Ratio: p = 0.7</th>
        <th>MSE Ratio: p = 0.8</th>
        <th>MSE Ratio: p = 0.9</th>
      </tr>
      <tr>
        <th>T_a</th>
        <th>T_b</th>
        <th>Bias</th>
        <th></th>
        <th></th>
        <th></th>
        <th></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th>0.95</th>
        <th>1.00</th>
        <th>-0.025230</th>
        <td>7.0</td>
        <td>1.69</td>
        <td>0.75</td>
        <td>0.44</td>
      </tr>
      <tr>
        <th>0.90</th>
        <th>1.00</th>
        <th>-0.050279</th>
        <td>2.23</td>
        <td>0.54</td>
        <td>0.24</td>
        <td>0.14</td>
      </tr>
      <tr>
        <th>0.70</th>
        <th>1.00</th>
        <th>-0.149866</th>
        <td>0.27</td>
        <td>0.07</td>
        <td>0.03</td>
        <td>0.02</td>
      </tr>
      <tr>
        <th>0.50</th>
        <th>1.00</th>
        <th>-0.250211</th>
        <td>0.1</td>
        <td>0.02</td>
        <td>0.01</td>
        <td>0.01</td>
      </tr>
      <tr>
        <th rowSpan="4" valign="top">
          1.00
        </th>
        <th>0.95</th>
        <th>0.024410</th>
        <td>7.38</td>
        <td>1.78</td>
        <td>0.79</td>
        <td>0.46</td>
      </tr>
      <tr>
        <th>0.90</th>
        <th>0.049839</th>
        <td>2.26</td>
        <td>0.54</td>
        <td>0.24</td>
        <td>0.14</td>
      </tr>
      <tr>
        <th>0.70</th>
        <th>0.149769</th>
        <td>0.27</td>
        <td>0.07</td>
        <td>0.03</td>
        <td>0.02</td>
      </tr>
      <tr>
        <th>0.50</th>
        <th>0.249851</th>
        <td>0.1</td>
        <td>0.02</td>
        <td>0.01</td>
        <td>0.01</td>
      </tr>
      <tr>
        <th>0.95</th>
        <th>0.95</th>
        <th>-0.000260</th>
        <td>24.29</td>
        <td>5.86</td>
        <td>2.59</td>
        <td>1.52</td>
      </tr>
      <tr>
        <th>0.90</th>
        <th>0.90</th>
        <th>-0.000109</th>
        <td>25.73</td>
        <td>6.2</td>
        <td>2.74</td>
        <td>1.61</td>
      </tr>
      <tr>
        <th>0.70</th>
        <th>0.70</th>
        <th>-0.000439</th>
        <td>25.75</td>
        <td>6.21</td>
        <td>2.74</td>
        <td>1.61</td>
      </tr>
      <tr>
        <th>0.50</th>
        <th>0.50</th>
        <th>0.000768</th>
        <td>24.91</td>
        <td>6.01</td>
        <td>2.65</td>
        <td>1.56</td>
      </tr>
    </tbody>
  </table>
</div>

We can also revisit a calculation in the final section of Warner [[Warner, 1965](/courses/Introduction-to-Quantitative-Economics/References#id10)] where

- $ \pi_A=0.6 $
- $ n=2000 $

We use the code

```python
cp3 = Comparison(0.6, 2000)
df3_theoretical = cp3.theoretical()
df3_theoretical
```

<div>
  <table border="1" className="dataframe">
    <thead>
      <tr style="text-align: right;">
        <th></th>
        <th></th>
        <th></th>
        <th>MSE Ratio: p = 0.6</th>
        <th>MSE Ratio: p = 0.7</th>
        <th>MSE Ratio: p = 0.8</th>
        <th>MSE Ratio: p = 0.9</th>
      </tr>
      <tr>
        <th>T_a</th>
        <th>T_b</th>
        <th>Bias</th>
        <th></th>
        <th></th>
        <th></th>
        <th></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th>0.95</th>
        <th>1.00</th>
        <th>-0.03</th>
        <td>3.05</td>
        <td>0.76</td>
        <td>0.33</td>
        <td>0.19</td>
      </tr>
      <tr>
        <th>0.90</th>
        <th>1.00</th>
        <th>-0.06</th>
        <td>0.84</td>
        <td>0.21</td>
        <td>0.09</td>
        <td>0.05</td>
      </tr>
      <tr>
        <th>0.70</th>
        <th>1.00</th>
        <th>-0.18</th>
        <td>0.10</td>
        <td>0.02</td>
        <td>0.01</td>
        <td>0.01</td>
      </tr>
      <tr>
        <th>0.50</th>
        <th>1.00</th>
        <th>-0.30</th>
        <td>0.03</td>
        <td>0.01</td>
        <td>0.00</td>
        <td>0.00</td>
      </tr>
      <tr>
        <th rowSpan="4" valign="top">
          1.00
        </th>
        <th>0.95</th>
        <th>0.02</th>
        <td>6.03</td>
        <td>1.50</td>
        <td>0.66</td>
        <td>0.37</td>
      </tr>
      <tr>
        <th>0.90</th>
        <th>0.04</th>
        <td>1.82</td>
        <td>0.45</td>
        <td>0.20</td>
        <td>0.11</td>
      </tr>
      <tr>
        <th>0.70</th>
        <th>0.12</th>
        <td>0.22</td>
        <td>0.05</td>
        <td>0.02</td>
        <td>0.01</td>
      </tr>
      <tr>
        <th>0.50</th>
        <th>0.20</th>
        <td>0.08</td>
        <td>0.02</td>
        <td>0.01</td>
        <td>0.00</td>
      </tr>
      <tr>
        <th>0.95</th>
        <th>0.95</th>
        <th>-0.01</th>
        <td>14.12</td>
        <td>3.51</td>
        <td>1.55</td>
        <td>0.86</td>
      </tr>
      <tr>
        <th>0.90</th>
        <th>0.90</th>
        <th>-0.02</th>
        <td>5.98</td>
        <td>1.49</td>
        <td>0.66</td>
        <td>0.36</td>
      </tr>
      <tr>
        <th>0.70</th>
        <th>0.70</th>
        <th>-0.06</th>
        <td>0.84</td>
        <td>0.21</td>
        <td>0.09</td>
        <td>0.05</td>
      </tr>
      <tr>
        <th>0.50</th>
        <th>0.50</th>
        <th>-0.10</th>
        <td>0.31</td>
        <td>0.08</td>
        <td>0.03</td>
        <td>0.02</td>
      </tr>
    </tbody>
  </table>
</div>

```python
df3_mc = cp3.MCsimulation()
df3_mc
```

<div>
  <table border="1" className="dataframe">
    <thead>
      <tr style="text-align: right;">
        <th></th>
        <th></th>
        <th></th>
        <th>MSE Ratio: p = 0.6</th>
        <th>MSE Ratio: p = 0.7</th>
        <th>MSE Ratio: p = 0.8</th>
        <th>MSE Ratio: p = 0.9</th>
      </tr>
      <tr>
        <th>T_a</th>
        <th>T_b</th>
        <th>Bias</th>
        <th></th>
        <th></th>
        <th></th>
        <th></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th>0.95</th>
        <th>1.00</th>
        <th>-0.030316</th>
        <td>3.27</td>
        <td>0.8</td>
        <td>0.34</td>
        <td>0.19</td>
      </tr>
      <tr>
        <th>0.90</th>
        <th>1.00</th>
        <th>-0.060352</th>
        <td>0.91</td>
        <td>0.22</td>
        <td>0.09</td>
        <td>0.05</td>
      </tr>
      <tr>
        <th>0.70</th>
        <th>1.00</th>
        <th>-0.180087</th>
        <td>0.11</td>
        <td>0.03</td>
        <td>0.01</td>
        <td>0.01</td>
      </tr>
      <tr>
        <th>0.50</th>
        <th>1.00</th>
        <th>-0.299849</th>
        <td>0.04</td>
        <td>0.01</td>
        <td>0.0</td>
        <td>0.0</td>
      </tr>
      <tr>
        <th rowSpan="4" valign="top">
          1.00
        </th>
        <th>0.95</th>
        <th>0.019734</th>
        <td>6.7</td>
        <td>1.64</td>
        <td>0.69</td>
        <td>0.39</td>
      </tr>
      <tr>
        <th>0.90</th>
        <th>0.039766</th>
        <td>2.01</td>
        <td>0.49</td>
        <td>0.21</td>
        <td>0.12</td>
      </tr>
      <tr>
        <th>0.70</th>
        <th>0.119789</th>
        <td>0.24</td>
        <td>0.06</td>
        <td>0.02</td>
        <td>0.01</td>
      </tr>
      <tr>
        <th>0.50</th>
        <th>0.200138</th>
        <td>0.09</td>
        <td>0.02</td>
        <td>0.01</td>
        <td>0.0</td>
      </tr>
      <tr>
        <th>0.95</th>
        <th>0.95</th>
        <th>-0.010475</th>
        <td>14.78</td>
        <td>3.61</td>
        <td>1.53</td>
        <td>0.85</td>
      </tr>
      <tr>
        <th>0.90</th>
        <th>0.90</th>
        <th>-0.020373</th>
        <td>6.32</td>
        <td>1.54</td>
        <td>0.65</td>
        <td>0.36</td>
      </tr>
      <tr>
        <th>0.70</th>
        <th>0.70</th>
        <th>-0.059945</th>
        <td>0.92</td>
        <td>0.23</td>
        <td>0.1</td>
        <td>0.05</td>
      </tr>
      <tr>
        <th>0.50</th>
        <th>0.50</th>
        <th>-0.100103</th>
        <td>0.34</td>
        <td>0.08</td>
        <td>0.03</td>
        <td>0.02</td>
      </tr>
    </tbody>
  </table>
</div>

Clearly, as $ n $ increases, the randomized response method performs better in more situations.
