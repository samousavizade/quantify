---
title: Introduction to Artificial Neural Networks
draft: false
summary: test
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

# A Primer on Artificial Neural Networks

```python
!pip install --upgrade jax jaxlib
!conda install -y -c plotly plotly plotly-orca retrying
```

> **Note**When using Google Colab, the preceding cell will generate an error. This is due to Google Colab not utilizing Anaconda for Python package management. However, this lesson will still function as `plotly` is pre-installed on Google Colab.

## Synopsis

Key aspects of **machine learning** and **artificial intelligence** involve:

- Using a known function to estimate an unknown one
- Deriving the known function from a dataset of input and output variables

This session explores the architecture of a basic **artificial neural network** (ANN), which is commonly employed to estimate a function $ f $ that maps $ x $ from space $ X $ to $ y $ in space $ Y $.

To elucidate fundamental concepts, we'll examine a case where $ x $ and $ y $ are individual values.

We'll elaborate on these essential neural network components:

- neurons
- activation functions
- neuron networks
- Neural networks as function compositions
- back-propagation and its connection to calculus' chain rule

## An In-depth (but Narrow) Artificial Neural Network

We'll describe a "deep" neural network with a width of one.

**Deep** indicates that the network combines numerous functions arranged in a graph's nodes.

**Width** refers to the number of variables on the right side of the function being approximated.

A width of one means the network only combines single-variable functions.

Let $ x \in \mathbb{R} $ and $ y \in \mathbb{R} $ be scalars.

We assume $ y $ is a non-linear function of $ x $:

$$
y = f(x)
$$

Our goal is to approximate $ f(x) $ using a recursively defined function.

For a network of depth $ N \geq 1 $, each **layer** $ i =1, \ldots N $ comprises:

- an input $ x_i $
- an **affine function** $ w_i x_i + bI $, where $ w_i $ is a scalar **weight** for input $ x_i $ and $ b_i $ is a scalar **bias**
- an **activation function** $ h*i $ that processes $ (w_i x_i + b_i) $ and produces output $ x*{i+1} $

One example of an activation function $ h $ is the **sigmoid** function

$$
h (z) = \frac{1}{1 + e^{-z}}
$$

Another popular choice is the **rectified linear unit** (ReLU) function

$$
h(z) = \max (0, z)
$$

The identity function is also used

$$
h(z) = z
$$

In our activation functions, we'll use the sigmoid for layers $ 1 $ to $ N-1 $ and the identity for layer $ N $.

To approximate $ f(x) $, we construct $ \hat f(x) $ as follows.

Let

$$
l_{i}\left(x\right)=w_{i}x+b_{i} .
$$

We build $ \hat f $ by iterating on compositions of $ h_i \circ l_i $:

$$
f(x)\approx\hat{f}(x)=h_{N}\circ l_{N}\circ h_{N-1}\circ l_{1}\circ\cdots\circ h_{1}\circ l_{1}(x)
$$

If $ N >1 $, we term this a "deep" neural net.

A larger integer $ N $ results in a "deeper" neural net.

Clearly, knowing the parameters $ \{w*i, b_i\}*{i=1}^N $ allows us to compute
$ \hat f(x) $ for a given $ x = \tilde x $ by iterating on the recursion

$$
x_{i+1} = h_i \circ l_i(x_i) , \quad, i = 1, \ldots N \tag{14.1}
$$

starting from $ x_1 = \tilde x $.

The final $ x\_{N+1} $ from this iteration equals $ \hat f(\tilde x) $.

## Parameter Adjustment

Let's consider a neural network as described above with width 1, depth $ N $, and activation functions $ h\_{i} $ for $ 1\leqslant i\leqslant N $ mapping $ \mathbb{R} $ to itself.

Let $ \left\{ \left(w*{i},b*{i}\right)\right\} \_{i=1}^{N} $ be a sequence of weights and biases.

As mentioned, for an input $ x*{1} $, our approximating function $ \hat f $ at $ x_1 $ equals the "output" $ x*{N+1} $ from our network, computed by iterating $ x*{i+1}=h*{i}\left(w*{i}x*{i}+b\_{i}\right) $.

For a given **prediction** $ \hat{y} (x) $ and **target** $ y= f(x) $, consider the loss function

$$
\mathcal{L} \left(\hat{y},y\right)(x)=\frac{1}{2}\left(\hat{y}-y\right)^{2}(x) .
$$

This criterion depends on the parameters $ \left\{ \left(w*{i},b*{i}\right)\right\} \_{i=1}^{N} $
and the point $ x $.

We aim to solve:

$$
\min_{\left\{ \left(w_{i},b_{i}\right)\right\} _{i=1}^{N}} \int {\mathcal L}\left(x_{N+1},y\right)(x) d \mu(x)
$$

where $ \mu(x) $ is a measure of points $ x \in \mathbb{R} $ where we want $ \hat f(x) $ to closely approximate $ f(x) $.

We stack weights and biases into a parameter vector $ p $:

$$
p = \begin{bmatrix}
w_1 \cr
b_1 \cr
w_2 \cr
b_2 \cr
\vdots \cr
w_N \cr
b_N
\end{bmatrix}
$$

Applying a simplified **stochastic gradient descent** algorithm to find a function's zero leads to this parameter update rule:

$$
p_{k+1}=p_k-\alpha\frac{d \mathcal{L}}{dx_{N+1}}\frac{dx_{N+1}}{dp_k} \tag{14.2}
$$

where $ \frac{d {\mathcal L}}{dx*{N+1}}=-\left(x*{N+1}-y\right) $ and $ \alpha > 0 $ is a step size.

(Refer to [this](https://en.wikipedia.org/wiki/Gradient_descent#Description) and [this](https://en.wikipedia.org/wiki/Newton%27s_method) for insights on how stochastic gradient descent
relates to Newton's method.)

To implement one step of this update rule, we need the derivative vector $ \frac{dx\_{N+1}}{dp_k} $.

In neural network terminology, this step is known as **back propagation**.

## Back Propagation and the Chain Rule

Thanks to properties of

- the chain and product rules from differential calculus, and
- lower triangular matrices

back propagation can be accomplished in one step by

- inverting a lower triangular matrix, and
- matrix multiplication

(This concept is from the final 7 minutes of an excellent YouTube video by MIT's Alan Edelman)

Here's how it works.

Define the derivative of $ h(z) $ with respect to $ z $ at $ z = z_i $ as $ \delta_i $:

$$
\delta_i = \frac{d}{d z} h(z)|_{z=z_i}
$$

or

$$
\delta_{i}=h'\left(w_{i}x_{i}+b_{i}\right).
$$

Repeated application of the chain rule and product rule to our recursion [(14.1)](#equation-eq-recursion) yields:

$$
dx_{i+1}=\delta_{i}\left(dw_{i}x_{i}+w_{i}dx_{i}+b_{i}\right)
$$

After setting $ dx\_{1}=0 $, we get this system of equations:

$$
\left(\begin{array}{c}
dx_{2}\\
\vdots\\
dx_{N+1}
\end{array}\right)=\underbrace{\left(\begin{array}{ccccc}
\delta_{1}w_{1} & \delta_{1} & 0 & 0 & 0\\
0 & 0 & \ddots & 0 & 0\\
0 & 0 & 0 & \delta_{N}w_{N} & \delta_{N}
\end{array}\right)}_{D}\left(\begin{array}{c}
dw_{1}\\
db_{1}\\
\vdots\\
dw_{N}\\
db_{N}
\end{array}\right)+\underbrace{\left(\begin{array}{cccc}
0 & 0 & 0 & 0\\
w_{2} & 0 & 0 & 0\\
0 & \ddots & 0 & 0\\
0 & 0 & w_{N} & 0
\end{array}\right)}_{L}\left(\begin{array}{c}
dx_{2}\\
\vdots\\
dx_{N+1}
\end{array}\right)
$$

or

$$
d x = D dp + L dx
$$

which implies

$$
dx = (I -L)^{-1} D dp
$$

which in turn implies

$$
\left(\begin{array}{c}
dx_{N+1}/dw_{1}\\
dx_{N+1}/db_{1}\\
\vdots\\
dx_{N+1}/dw_{N}\\
dx_{N+1}/db_{N}
\end{array}\right)=e_{N}\left(I-L\right)^{-1}D.
$$

We can then solve this problem by applying our $ p $ update multiple times for a set of input-output pairs $ \left\{ \left(x*{1}^{i},y^{i}\right)\right\} *{i=1}^{M} $ called our "training set".

## Training Dataset

Selecting a training set is equivalent to choosing a measure $ \mu $ in our function approximation problem formulated as a minimization task.

In this context, we'll use a uniform grid of, for example, 50 or 200 points.

There are various approaches to tackle the minimization problem outlined above:

- batch gradient descent, using an average gradient across the training set
- stochastic gradient descent, randomly sampling points and using individual gradients
- a hybrid approach (known as "mini-batch gradient descent")

The update rule [(14.2)](#equation-eq-sgd) described earlier represents a stochastic gradient descent algorithm.

```python
from IPython.display import Image
import jax.numpy as jnp
from jax import grad, jit, jacfwd, vmap
from jax import random
import jax
import plotly.graph_objects as go
```

```python
# A helper function to randomly initialize weights and biases
# for a dense neural network layer
def random_layer_params(m, n, key, scale=1.):
    w_key, b_key = random.split(key)
    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))

# Initialize all layers for a fully-connected neural network with sizes "sizes"
def init_network_params(sizes, key):
    keys = random.split(key, len(sizes))
    return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]
```

```python
def compute_xδw_seq(params, x):
    # Initialize arrays
    δ = jnp.zeros(len(params))
    xs = jnp.zeros(len(params) + 1)
    ws = jnp.zeros(len(params))
    bs = jnp.zeros(len(params))

    h = jax.nn.sigmoid

    xs = xs.at[0].set(x)
    for i, (w, b) in enumerate(params[:-1]):
        output = w * xs[i] + b
        activation = h(output[0, 0])

        # Store elements
        δ = δ.at[i].set(grad(h)(output[0, 0]))
        ws = ws.at[i].set(w[0, 0])
        bs = bs.at[i].set(b[0])
        xs = xs.at[i+1].set(activation)

    final_w, final_b = params[-1]
    preds = final_w * xs[-2] + final_b

    # Store elements
    δ = δ.at[-1].set(1.)
    ws = ws.at[-1].set(final_w[0, 0])
    bs = bs.at[-1].set(final_b[0])
    xs = xs.at[-1].set(preds[0, 0])

    return xs, δ, ws, bs


def loss(params, x, y):
    xs, δ, ws, bs = compute_xδw_seq(params, x)
    preds = xs[-1]

    return 1 / 2 * (y - preds) ** 2
```

```python
# Parameters
N = 3  # Number of layers
layer_sizes = [1, ] * (N + 1)
param_scale = 0.1
step_size = 0.01
params = init_network_params(layer_sizes, random.PRNGKey(1))
```

```python
x = 5
y = 3
xs, δ, ws, bs = compute_xδw_seq(params, x)
```

```python
dxs_ad = jacfwd(lambda params, x: compute_xδw_seq(params, x)[0], argnums=0)(params, x)
dxs_ad_mat = jnp.block([dx.reshape((-1, 1)) for dx_tuple in dxs_ad for dx in dx_tuple ])[1:]
```

```python
jnp.block([[δ * xs[:-1]], [δ]])
```

    Array([[8.5726520e-03, 4.0850646e-04, 6.1021698e-01],
           [1.7145304e-03, 2.3785222e-01, 1.0000000e+00]], dtype=float32)

```python
L = jnp.diag(δ * ws, k=-1)
L = L[1:, 1:]

D = jax.scipy.linalg.block_diag(*[row.reshape((1, 2)) for row in jnp.block([[δ * xs[:-1]], [δ]]).T])

dxs_la = jax.scipy.linalg.solve_triangular(jnp.eye(N) - L, D, lower=True)
```

```python
# Check that the `dx` generated by the linear algebra method
# are the same as the ones generated using automatic differentiation
jnp.max(jnp.abs(dxs_ad_mat - dxs_la))
```

    Array(0., dtype=float32)

```python
grad_loss_ad = jnp.block([dx.reshape((-1, 1)) for dx_tuple in grad(loss)(params, x, y) for dx in dx_tuple ])
```

```python
# Check that the gradient of the loss is the same for both approaches
jnp.max(jnp.abs(-(y - xs[-1]) * dxs_la[-1] - grad_loss_ad))
```

    Array(1.4901161e-08, dtype=float32)

```python
@jit
def update_ad(params, x, y):
    grads = grad(loss)(params, x, y)
    return [(w - step_size * dw, b - step_size * db)
          for (w, b), (dw, db) in zip(params, grads)]

@jit
def update_la(params, x, y):
    xs, δ, ws, bs = compute_xδw_seq(params, x)
    N = len(params)
    L = jnp.diag(δ * ws, k=-1)
    L = L[1:, 1:]

    D = jax.scipy.linalg.block_diag(*[row.reshape((1, 2)) for row in jnp.block([[δ * xs[:-1]], [δ]]).T])

    dxs_la = jax.scipy.linalg.solve_triangular(jnp.eye(N) - L, D, lower=True)

    grads = -(y - xs[-1]) * dxs_la[-1]

    return [(w - step_size * dw, b - step_size * db)
            for (w, b), (dw, db) in zip(params, grads.reshape((-1, 2)))]
```

```python
# Check that both updates are the same
update_la(params, x, y)
```

    [(Array([[-1.3489482]], dtype=float32), Array([0.37956238], dtype=float32)),
     (Array([[-0.00782906]], dtype=float32), Array([0.44972023], dtype=float32)),
     (Array([[0.22937916]], dtype=float32), Array([-0.04793657], dtype=float32))]

```python
update_ad(params, x, y)
```

    [(Array([[-1.3489482]], dtype=float32), Array([0.37956238], dtype=float32)),
     (Array([[-0.00782906]], dtype=float32), Array([0.44972023], dtype=float32)),
     (Array([[0.22937916]], dtype=float32), Array([-0.04793657], dtype=float32))]

## Case Study 1

Examine the function

$$
f\left(x\right)=-3x+2
$$

on $ \left[0.5,3\right] $.

We employ a uniform grid of 200 points and update the parameters for each grid point 300 times.

$ h\_{i} $ is the sigmoid activation function for all layers except the last, where we use the identity function, and $ N=3 $.

Weights are initialized randomly.

```python
def f(x):
    return -3 * x + 2

M = 200
grid = jnp.linspace(0.5, 3, num=M)
f_val = f(grid)
```

```python
indices = jnp.arange(M)
key = random.PRNGKey(0)

def train(params, grid, f_val, key, num_epochs=300):
    for epoch in range(num_epochs):
        key, _ = random.split(key)
        random_permutation = random.permutation(random.PRNGKey(1), indices)
        for x, y in zip(grid[random_permutation], f_val[random_permutation]):
            params = update_la(params, x, y)

    return params
```

```python
# Parameters
N = 3  # Number of layers
layer_sizes = [1, ] * (N + 1)
params_ex1 = init_network_params(layer_sizes, key)
```

```python
%%time
params_ex1 = train(params_ex1, grid, f_val, key, num_epochs=500)
```

    CPU times: user 2.03 s, sys: 13.4 ms, total: 2.04 s
    Wall time: 2.04 s

```python
predictions = vmap(compute_xδw_seq, in_axes=(None, 0))(params_ex1, grid)[0][:, -1]
```

```python
fig = go.Figure()
fig.add_trace(go.Scatter(x=grid, y=f_val, name=r'$-3x+2$'))
fig.add_trace(go.Scatter(x=grid, y=predictions, name='Approximation'))

# Export to PNG file
Image(fig.to_image(format="png"))
# fig.show() will provide interactive plot when running
# notebook locally
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/14.%20Introduction%20to%20Artificial%20Neural%20Networks_files/14.%20Introduction%20to%20Artificial%20Neural%20Networks_28_0.png)
</div>

## Network Depth Considerations

It's intriguing to ponder how increasing the neural net's depth for the above example affects approximation quality

- Excessive depth can lead to the [vanishing gradient problem](http://neuralnetworksanddeeplearning.com/chap5.html)
- Other factors like step size and epoch count can be equally or more crucial than layer count in this scenario
- Indeed, since $ f $ is a linear function of $ x $, a single-layer network with an identity activation would likely be optimal

## Case Study 2

We use the same configuration as the previous example with

$$
f\left(x\right)=\log\left(x\right)
$$

```python
def f(x):
    return jnp.log(x)

grid = jnp.linspace(0.5, 3, num=M)
f_val = f(grid)
```

```python
# Parameters
N = 1  # Number of layers
layer_sizes = [1, ] * (N + 1)
params_ex2_1 = init_network_params(layer_sizes, key)
```

```python
# Parameters
N = 2  # Number of layers
layer_sizes = [1, ] * (N + 1)
params_ex2_2 = init_network_params(layer_sizes, key)
```

```python
# Parameters
N = 3  # Number of layers
layer_sizes = [1, ] * (N + 1)
params_ex2_3 = init_network_params(layer_sizes, key)
```

```python
params_ex2_1 = train(params_ex2_1, grid, f_val, key, num_epochs=300)
```

```python
params_ex2_2 = train(params_ex2_2, grid, f_val, key, num_epochs=300)
```

```python
params_ex2_3 = train(params_ex2_3, grid, f_val, key, num_epochs=300)
```

```python
predictions_1 = vmap(compute_xδw_seq, in_axes=(None, 0))(params_ex2_1, grid)[0][:, -1]
predictions_2 = vmap(compute_xδw_seq, in_axes=(None, 0))(params_ex2_2, grid)[0][:, -1]
predictions_3 = vmap(compute_xδw_seq, in_axes=(None, 0))(params_ex2_3, grid)[0][:, -1]
```

```python
fig = go.Figure()
fig.add_trace(go.Scatter(x=grid, y=f_val, name=r'$\log{x}$'))
fig.add_trace(go.Scatter(x=grid, y=predictions_1, name='One-layer neural network'))
fig.add_trace(go.Scatter(x=grid, y=predictions_2, name='Two-layer neural network'))
fig.add_trace(go.Scatter(x=grid, y=predictions_3, name='Three-layer neural network'))

# Export to PNG file
Image(fig.to_image(format="png"))
# fig.show() will provide interactive plot when running
# notebook locally
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/14.%20Introduction%20to%20Artificial%20Neural%20Networks_files/14.%20Introduction%20to%20Artificial%20Neural%20Networks_39_0.png)
</div>

```python
## to check that gpu is activated in environment

from jax.lib import xla_bridge
print(xla_bridge.get_backend().platform)
```

    cpu

> **Note** Cloud Environment: This lecture platform is constructed in a server environment lacking `gpu` access.
> If you run this lecture locally, this information indicates whether your code is executed via
> the `cpu` or the `gpu`
