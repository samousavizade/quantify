---
title: Multivariate Hypergeometric Distribution
draft: false
summary: test
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

# Distribution of Multivariate Hypergeometric Variables

## Synopsis

This presentation explores how a grant allocator employed a **multivariate hypergeometric distribution** to evaluate the impartiality of a research grant allocation process.

We will delve into:

- characteristics of the multivariate hypergeometric distribution
- primary and secondary moments of a multivariate hypergeometric distribution
- employing a Monte Carlo simulation of a multivariate normal distribution to assess the accuracy of a normal approximation
- the grant allocator's dilemma and the rationale for using the multivariate hypergeometric distribution

## The Grant Allocator's Dilemma

A grant allocator responsible for distributing research funds faces the following scenario.

To maintain anonymity and focus on the relevant aspects, we'll refer to research proposals as **balls** and the continents where authors reside as **colors**.

There are $ K_i $ balls (proposals) of color $ i $.

The total number of distinct colors (continents) is $ c $.

Thus, $ i = 1, 2, \ldots, c $

The total number of balls is $ N = \sum\_{i=1}^c K_i $.

All $ N $ balls are placed in a container.

$ n $ balls are then randomly selected.

The selection process is intended to be **unbiased**, meaning that **ball quality**, a random variable presumed independent of **ball color**, determines whether a ball is chosen.

Consequently, the selection procedure is expected to randomly draw $ n $ balls from the container.

The $ n $ selected balls represent successful proposals and receive research funding.

The remaining $ N-n $ balls do not receive any funding.

### Specifics of the Grant Allocation Process Under Examination

Let $ k_i $ represent the number of balls of color $ i $ that are selected.

The totals must balance, so $ \sum\_{i=1}^c k_i = n $.

Assuming the selection process evaluates proposals based on merit and that quality is unrelated to the author's continent of residence, the allocator views the outcome as a random vector

$$
X = \begin{pmatrix} k_1 \cr k_2 \cr \vdots \cr k_c \end{pmatrix}.
$$

To determine if the selection process is **unbiased**, the allocator needs to analyze whether the specific realization of $ X $ can reasonably be considered a random draw from the probability distribution implied by the **unbiased** hypothesis.

The appropriate probability distribution is described [here](https://en.wikipedia.org/wiki/Hypergeometric_distribution).

Let's now apply this to the allocator's situation, continuing with the colored balls analogy.

The allocator has a container with $ N = 238 $ balls.

157 are blue, 11 are green, 46 are yellow, and 24 are black.

Therefore, $ (K_1, K_2, K_3, K_4) = (157 , 11 , 46 , 24) $ and $ c = 4 $.

15 balls are drawn without replacement.

So $ n = 15 $.

The allocator wants to understand the probability distribution of outcomes

$$
X = \begin{pmatrix} k_1 \cr k_2 \cr \vdots \cr k_4 \end{pmatrix}.
$$

Specifically, they want to determine if a particular outcome - represented by a $ 4 \times 1 $ vector of integers showing the numbers of blue, green, yellow, and black balls, respectively - suggests evidence against the hypothesis that the selection process is _fair_, meaning _unbiased_ and truly random draws without replacement from the population of $ N $ balls.

The appropriate tool for the allocator's task is the **multivariate hypergeometric distribution**.

### Multivariate Hypergeometric Distribution

Let's begin with some necessary imports.

```python
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (11, 5)  #set default figure size
import numpy as np
from scipy.special import comb
from scipy.stats import normaltest
from numba import njit, prange
```

To summarize, we assume there are $ c $ types of items in a container.

If there are $ K*{i} $ type $ i $ items in the container and we make
$ n $ random draws without replacement, then the numbers of type
$ i $ items in the sample $ (k*{1},k*{2},\dots,k*{c}) $
follow the multivariate hypergeometric distribution.

Recall that $ N=\sum*{i=1}^{c} K*{i} $ is
the total number of items in the container and $ n=\sum*{i=1}^{c}k*{i} $.

**Notation**

We use the following notation for **binomial coefficients**: $ {m \choose q} = \frac{m!}{(m-q)!} $.

The multivariate hypergeometric distribution has these key properties:

**Probability mass function**:

$$
\Pr \{X_{i}=k_{i} \ \forall i\} =
\frac {\prod _{i=1}^{c}{\binom {K_{i}}{k_{i}}}}{\binom {N}{n}}
$$

**Mean**:

$$
{\displaystyle \operatorname {E} (X_{i})=n{\frac {K_{i}}{N}}}
$$

**Variances and covariances**:

$$
{\displaystyle \operatorname {Var} (X_{i})=n{\frac {N-n}{N-1}}\;{\frac {K_{i}}{N}}\left(1-{\frac {K_{i}}{N}}\right)}
$$

$$
{\displaystyle \operatorname {Cov} (X_{i},X_{j})=-n{\frac {N-n}{N-1}}\;{\frac {K_{i}}{N}}{\frac {K_{j}}{N}}}
$$

To facilitate our work, we'll create an `Urn` class.

```python
class Urn:

    def __init__(self, K_arr):
        """
        Initialization given the number of each type i object in the urn.

        Parameters
        ----------
        K_arr: ndarray(int)
            number of each type i object.
        """

        self.K_arr = np.array(K_arr)
        self.N = np.sum(K_arr)
        self.c = len(K_arr)

    def pmf(self, k_arr):
        """
        Probability mass function.

        Parameters
        ----------
        k_arr: ndarray(int)
            number of observed successes of each object.
        """

        K_arr, N = self.K_arr, self.N

        k_arr = np.atleast_2d(k_arr)
        n = np.sum(k_arr, 1)

        num = np.prod(comb(K_arr, k_arr), 1)
        denom = comb(N, n)

        pr = num / denom

        return pr

    def moments(self, n):
        """
        Compute the mean and variance-covariance matrix for
        multivariate hypergeometric distribution.

        Parameters
        ----------
        n: int
            number of draws.
        """

        K_arr, N, c = self.K_arr, self.N, self.c

        # mean
        μ = n * K_arr / N

        # variance-covariance matrix
        Σ = np.full((c, c), n * (N - n) / (N - 1) / N ** 2)
        for i in range(c-1):
            Σ[i, i] *= K_arr[i] * (N - K_arr[i])
            for j in range(i+1, c):
                Σ[i, j] *= - K_arr[i] * K_arr[j]
                Σ[j, i] = Σ[i, j]

        Σ[-1, -1] *= K_arr[-1] * (N - K_arr[-1])

        return μ, Σ

    def simulate(self, n, size=1, seed=None):
        """
        Simulate a sample from multivariate hypergeometric
        distribution where at each draw we take n objects
        from the urn without replacement.

        Parameters
        ----------
        n: int
            number of objects for each draw.
        size: int(optional)
            sample size.
        seed: int(optional)
            random seed.
        """

        K_arr = self.K_arr

        gen = np.random.Generator(np.random.PCG64(seed))
        sample = gen.multivariate_hypergeometric(K_arr, n, size=size)

        return sample
```

## Application

### Initial Example

Let's apply this to an example from
[Wikipedia](https://en.wikipedia.org/wiki/Hypergeometric_distribution#Multivariate_hypergeometric_distribution):

Imagine a container with 5 black, 10 white, and 15 red marbles. If
six marbles are selected without replacement, the probability of choosing
exactly two of each color is

$$
P(2{\text{ black}},2{\text{ white}},2{\text{ red}})={{{5 \choose 2}{10 \choose 2}{15 \choose 2}} \over {30 \choose 6}}=0.079575596816976
$$

```python
# construct the urn
K_arr = [5, 10, 15]
urn = Urn(K_arr)
```

Now let's use the Urn Class method `pmf` to calculate the probability of the outcome $ X = \begin{pmatrix} 2 & 2 & 2 \end{pmatrix} $

```python
k_arr = [2, 2, 2] # array of number of observed successes
urn.pmf(k_arr)
```

    array([0.0795756])

We can utilize the code to compute probabilities for a list of potential outcomes by
creating a 2-dimensional
array `k_arr`, and `pmf` will return an array of probabilities for
each case.

```python
k_arr = [[2, 2, 2], [1, 3, 2]]
urn.pmf(k_arr)
```

    array([0.0795756, 0.1061008])

Now let's determine the mean vector and variance-covariance matrix.

```python
n = 6
μ, Σ = urn.moments(n)
```

```python
μ
```

    array([1., 2., 3.])

```python
Σ
```

    array([[ 0.68965517, -0.27586207, -0.4137931 ],
           [-0.27586207,  1.10344828, -0.82758621],
           [-0.4137931 , -0.82758621,  1.24137931]])

### Revisiting The Grant Allocator's Dilemma

Let's now address the grant allocator's situation.

Here, the array of
numbers of $ i $ items in the container is
$ \left(157, 11, 46, 24\right) $.

```python
K_arr = [157, 11, 46, 24]
urn = Urn(K_arr)
```

Let's calculate the probability of the outcome $ \left(10, 1, 4, 0 \right) $.

```python
k_arr = [10, 1, 4, 0]
urn.pmf(k_arr)
```

    array([0.01547738])

We can compute probabilities for three potential outcomes by creating 3-dimensional
arrays `k_arr` and using the `pmf` method of the `Urn` class.

```python
k_arr = [[5, 5, 4 ,1], [10, 1, 2, 2], [13, 0, 2, 0]]
urn.pmf(k_arr)
```

    array([6.21412534e-06, 2.70935969e-02, 1.61839976e-02])

Now let's determine the mean and variance-covariance matrix of $ X $ when $ n=6 $.

```python
n = 6 # number of draws
μ, Σ = urn.moments(n)
```

```python
# mean
μ
```

    array([3.95798319, 0.27731092, 1.15966387, 0.60504202])

```python
# variance-covariance matrix
Σ
```

    array([[ 1.31862604, -0.17907267, -0.74884935, -0.39070401],
           [-0.17907267,  0.25891399, -0.05246715, -0.02737417],
           [-0.74884935, -0.05246715,  0.91579029, -0.11447379],
           [-0.39070401, -0.02737417, -0.11447379,  0.53255196]])

We can simulate a large sample and confirm that sample means and covariances closely approximate the population means and covariances.

```python
size = 10_000_000
sample = urn.simulate(n, size=size)
```

```python
# mean
np.mean(sample, 0)
```

    array([3.9585446, 0.2770548, 1.1594163, 0.6049843])

```python
# variance covariance matrix
np.cov(sample.T)
```

    array([[ 1.31873618, -0.1792183 , -0.74837331, -0.39114457],
           [-0.1792183 ,  0.25854706, -0.05199906, -0.02732971],
           [-0.74837331, -0.05199906,  0.91462543, -0.11425307],
           [-0.39114457, -0.02732971, -0.11425307,  0.53272735]])

Clearly, the sample means and covariances provide good approximations of their population counterparts.

### Assessing the Normal Approximation

To evaluate the accuracy of a multivariate normal approximation to the multivariate hypergeometric distribution, we generate a large sample from a multivariate normal distribution using the mean vector and covariance matrix of the corresponding multivariate hypergeometric distribution. We then compare this simulated distribution with the actual multivariate hypergeometric distribution.

```python
sample_normal = np.random.multivariate_normal(μ, Σ, size=size)
```

```python
def bivariate_normal(x, y, μ, Σ, i, j):

    μ_x, μ_y = μ[i], μ[j]
    σ_x, σ_y = np.sqrt(Σ[i, i]), np.sqrt(Σ[j, j])
    σ_xy = Σ[i, j]

    x_μ = x - μ_x
    y_μ = y - μ_y

    ρ = σ_xy / (σ_x * σ_y)
    z = x_μ**2 / σ_x**2 + y_μ**2 / σ_y**2 - 2 * ρ * x_μ * y_μ / (σ_x * σ_y)
    denom = 2 * np.pi * σ_x * σ_y * np.sqrt(1 - ρ**2)

    return np.exp(-z / (2 * (1 - ρ**2))) / denom
```

```python
@njit
def count(vec1, vec2, n):
    size = sample.shape[0]

    count_mat = np.zeros((n+1, n+1))
    for i in prange(size):
        count_mat[vec1[i], vec2[i]] += 1

    return count_mat
```

```python
c = urn.c
fig, axs = plt.subplots(c, c, figsize=(14, 14))

# grids for ploting the bivariate Gaussian
x_grid = np.linspace(-2, n+1, 100)
y_grid = np.linspace(-2, n+1, 100)
X, Y = np.meshgrid(x_grid, y_grid)

for i in range(c):
    axs[i, i].hist(sample[:, i], bins=np.arange(0, n, 1), alpha=0.5, density=True, label='hypergeom')
    axs[i, i].hist(sample_normal[:, i], bins=np.arange(0, n, 1), alpha=0.5, density=True, label='normal')
    axs[i, i].legend()
    axs[i, i].set_title('$k_{' +str(i+1) +'}$')
    for j in range(c):
        if i == j:
            continue

        # bivariate Gaussian density function
        Z = bivariate_normal(X, Y, μ, Σ, i, j)
        cs = axs[i, j].contour(X, Y, Z, 4, colors="black", alpha=0.6)
        axs[i, j].clabel(cs, inline=1, fontsize=10)

        # empirical multivariate hypergeometric distrbution
        count_mat = count(sample[:, i], sample[:, j], n)
        axs[i, j].pcolor(count_mat.T/size, cmap='Blues')
        axs[i, j].set_title('$(k_{' +str(i+1) +'}, k_{' + str(j+1) + '})$')

plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/11.%20Multivariate%20Hypergeometric%20Distribution_files/11.%20Multivariate%20Hypergeometric%20Distribution_40_0.png)
</div>

The diagonal plots show the marginal distributions of $ k_i $ for
each $ i $ using histograms.

Observe the notable differences between the hypergeometric distribution and the approximating normal distribution.

The off-diagonal plots illustrate the empirical joint distribution of
$ k_i $ and $ k_j $ for each pair $ (i, j) $.

Darker blue indicates a higher concentration of data points in the corresponding cell. (Note that $ k_i $ is on the x-axis and $ k_j $ is on the y-axis).

The contour maps depict the bivariate Gaussian density function of $ \left(k_i, k_j\right) $ using the population mean and covariance given by slices of $ \mu $ and $ \Sigma $ that we calculated earlier.

Let's also examine the normality of each $ k_i $ using `scipy.stats.normaltest`, which implements D'Agostino and Pearson's
test combining skew and kurtosis to create a comprehensive test of normality.

The null hypothesis states that the sample follows a normal distribution.

> `normaltest` produces an array of p-values associated with tests for each $ k_i $ sample.

```python
test_multihyper = normaltest(sample)
test_multihyper.pvalue
```

    array([0., 0., 0., 0.])

As we can observe, all p-values are nearly $ 0 $, strongly rejecting the null hypothesis.

In contrast, the sample from the normal distribution does not reject the null hypothesis.

```python
test_normal = normaltest(sample_normal)
test_normal.pvalue
```

    array([0.46620482, 0.9324187 , 0.72680171, 0.07303443])

The key takeaway is that the normal approximation is not perfect.
