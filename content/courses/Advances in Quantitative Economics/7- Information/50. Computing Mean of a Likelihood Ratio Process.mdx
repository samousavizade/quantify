---
title: Computing Mean of a Likelihood Ratio Process
draft: false
summary: test
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

# Calculating Average of a Likelihood Ratio Process

## Summary

In a previous lecture, we discussed an unusual characteristic of a likelihood ratio process: its mean remains one for all $ t \geq 0 $ despite converging to zero almost surely.

While this property is easily verified analytically, it's challenging to demonstrate through computer simulations using the law of large numbers, which involves examining sample averages from repeated simulations.

To address this challenge, this lecture employs **significance sampling** to speed up the convergence of sample averages to population means.

We use significance sampling to estimate the average of a cumulative likelihood ratio $ L\left(\omega^t\right) = \prod\_{i=1}^t \ell \left(\omega_i\right) $.

We begin by importing necessary Python libraries.

```python
import numpy as np
from numba import njit, vectorize, prange
import matplotlib.pyplot as plt
from math import gamma
```

## Statistical Expectation of Likelihood Ratio

In a previous lecture, we examined a likelihood ratio $ \ell \left(\omega_t\right) $

$$
\ell \left( \omega_t \right) = \frac{f\left(\omega_t\right)}{g\left(\omega_t\right)}
$$

where $ f $ and $ g $ are densities for Beta distributions with parameters $ F_a $, $ F_b $, $ G_a $,... $ G_b $.

Assume that an i.i.d. random variable $ \omega_t \in \Omega $ is generated by $ g $.

The **cumulative likelihood ratio** $ L \left(\omega^t\right) $ is

$$
L\left(\omega^t\right) = \prod_{i=1}^t \ell \left(\omega_i\right)
$$

Our aim is to accurately estimate the statistical expectation $ E \left[ L\left(\omega^t\right) \right] $.

In a previous lecture, we demonstrated that $ E \left[ L\left(\omega^t\right) \right] $ equals $ 1 $ for all $ t $.
We want to verify how well this holds when we replace $ E $ with sample averages from simulations.

This proves more difficult than expected because for the Beta distributions mentioned above, $ L\left(\omega^t\right) $ has
a highly skewed distribution with an extended tail as $ t \rightarrow \infty $.

This characteristic makes it challenging to efficiently and accurately estimate the mean using standard Monte Carlo simulation techniques.

In this lecture, we explore how a standard Monte Carlo method fails and how **significance sampling**
offers a more computationally efficient approach to approximating the mean of the cumulative likelihood ratio.

We first examine the density functions `f` and `g` .

```python
# Parameters in the two beta distributions.
F_a, F_b = 1, 1
G_a, G_b = 3, 1.2

@vectorize
def p(w, a, b):
    r = gamma(a + b) / (gamma(a) * gamma(b))
    return r * w ** (a-1) * (1 - w) ** (b-1)

# The two density functions.
f = njit(lambda w: p(w, F_a, F_b))
g = njit(lambda w: p(w, G_a, G_b))
```

```python
w_range = np.linspace(1e-5, 1-1e-5, 1000)

plt.plot(w_range, g(w_range), label='g')
plt.plot(w_range, f(w_range), label='f')
plt.xlabel('$\omega$')
plt.legend()
plt.title('density functions $f$ and $g$')
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/50.%20Computing%20Mean%20of%20a%20Likelihood%20Ratio%20Process_files/50.%20Computing%20Mean%20of%20a%20Likelihood%20Ratio%20Process_6_0.png)
</div>

The likelihood ratio is defined as `l(w)=f(w)/g(w)`.

```python
l = njit(lambda w: f(w) / g(w))
```

```python
plt.plot(w_range, l(w_range))
plt.title('$\ell(\omega)$')
plt.xlabel('$\omega$')
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/50.%20Computing%20Mean%20of%20a%20Likelihood%20Ratio%20Process_files/50.%20Computing%20Mean%20of%20a%20Likelihood%20Ratio%20Process_9_0.png)
</div>

The graph above illustrates that as $ \omega \rightarrow 0 $, $ f \left(\omega\right) $ remains constant while $ g \left(\omega\right) \rightarrow 0 $, causing the likelihood ratio to approach infinity.

A Monte Carlo approximation of $ \hat{E} \left[L\left(\omega^t\right)\right] = \hat{E} \left[\prod_{i=1}^t \ell \left(\omega_i\right)\right] $ would involve repeatedly drawing $ \omega $ from $ g $, calculating the likelihood ratio $ \ell(\omega) = \frac{f(\omega)}{g(\omega)} $ for each draw, then averaging these results.

Since $ g(\omega) \rightarrow 0 $ as $ \omega \rightarrow 0 $,... this simulation method undersamples a crucial part of the sample space $ [0,1] $ that needs frequent visits to accurately approximate the statistical expectation of the likelihood ratio $ \ell(\omega) $.

We demonstrate this numerically below.

## Significance sampling

We address this issue by using a _distribution change_ known as **significance sampling**.

Instead of drawing from $ g $ to generate data during the simulation, we employ an alternative
distribution $ h $ to generate draws of $ \omega $.

The goal is to design $ h $ to oversample the region of $ \Omega $ where
$ \ell \left(\omega_t\right) $ has high values but low density under $ g $.

After creating a sample this way, we must then weight each observation by the likelihood ratio of $ g $ and $ h $ when computing the empirical mean
of the likelihood ratio.

This process ensures we properly account for using $ h $ instead of $ g $ to simulate data.

To illustrate, suppose we're interested in $ {E}\left[\ell\left(\omega\right)\right] $.

We could simply compute:

$$
\hat{E}^g \left[\ell\left(\omega\right)\right] = \frac{1}{N} \sum_{i=1}^{N} \ell(w_i^g)
$$

where $ \omega_i^g $ indicates that $ \omega_i $ is drawn from $ g $.

However, using our insight from significance sampling, we could instead calculate:

$$
\hat{E}^h \left[\ell\left(\omega\right) \frac{g(w)}{h(w)} \right] = \frac{1}{N} \sum_{i=1}^{N} \ell(w_i^h) \frac{g(w_i^h)}{h(w_i^h)}
$$

where $ w_i $ is now drawn from significance distribution $ h $.

Note that these two are identical population objects:

$$
E^g\left[\ell\left(\omega\right)\right] = \int_\Omega \ell(\omega) g(\omega) d\omega = \int_\Omega \ell(\omega) \frac{g(\omega)}{h(\omega)} h(\omega) d\omega = E^h\left[\ell\left(\omega\right) \frac{g(\omega)}{h(\omega)}\right]
$$

## Choosing a Sampling Distribution

Since we need to use an $ h $ with greater mass in areas where $ g $ has low mass, we select $ h=Beta(0.5, 0.5) $ as our significance distribution.

The graphs compare $ g $ and $ h $.

```python
g_a, g_b = G_a, G_b
h_a, h_b = 0.5, 0.5
```

```python
w_range = np.linspace(1e-5, 1-1e-5, 1000)

plt.plot(w_range, g(w_range), label=f'g=Beta({g_a}, {g_b})')
plt.plot(w_range, p(w_range, 0.5, 0.5), label=f'h=Beta({h_a}, {h_b})')
plt.title('real data generating process $g$ and importance distribution $h$')
plt.legend()
plt.ylim([0., 3.])
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/50.%20Computing%20Mean%20of%20a%20Likelihood%20Ratio%20Process_files/50.%20Computing%20Mean%20of%20a%20Likelihood%20Ratio%20Process_14_0.png)
</div>

## Estimating a total likelihood ratio

We now explore how to use significance sampling to estimate
$ {E} \left[L(\omega^t)\right] = \left[\prod_{i=1}^T \ell \left(\omega_i\right)\right] $.

As before, we plan to draw sequences $ \omega^t $ from $ q $ and then adjust the likelihood ratio accordingly:

$$
\hat{E}^p \left[L\left(\omega^t\right)\right] = \hat{E}^p \left[\prod_{t=1}^T \ell \left(\omega_t\right)\right] = \hat{E}^q \left[\prod_{t=1}^T \ell \left(\omega_t\right) \frac{p\left(\omega_{t}\right)}{q\left(\omega_{t}\right)}\right] =
\frac{1}{N} \sum_{i=1}^{N}\left( \prod_{t=1}^{T} \ell(\omega_{i,t}^h)\frac{p\left(\omega_{i,t}^h\right)}{q\left(\omega_{i,t}^h\right)}\right)
$$

where the final equality uses $ \omega\_{i,t}^h $ drawn from the significance distribution $ q $.

Here $ \frac{p\left(\omega*{i,t}^q\right)}{q\left(\omega*{i,t}^q\right)} $ is the weight assigned to each data point $ \omega\_{i,t}^q $.

Below we create a Python function for calculating the significance sampling estimates given any beta distributions $ p $, $ q $.

```python
@njit(parallel=True)
def estimate(p_a, p_b, q_a, q_b, T=1, N=10000):

    μ_L = 0
    for i in prange(N):

        L = 1
        weight = 1
        for t in range(T):
            w = np.random.beta(q_a, q_b)
            l = f(w) / g(w)

            L *= l
            weight *= p(w, p_a, p_b) / p(w, q_a, q_b)

        μ_L += L * weight

    μ_L /= N

    return μ_L
```

Let's examine the case when $ T=1 $, which involves estimating $ E_0\left[\ell\left(\omega\right)\right] $

For the standard Monte Carlo estimate, we set $ p=g $ and $ q=g $.

```python
estimate(g_a, g_b, g_a, g_b, T=1, N=10000)
```

    OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.





    0.9327033618603431

For our significance sampling estimate, we set $ q = h $.

```python
estimate(g_a, g_b, h_a, h_b, T=1, N=10000)
```

    1.0060794961569006

Clearly, even at T=1, our significance sampling estimate is closer to $ 1 $ than the Monte Carlo estimate.

Larger differences emerge when calculating expectations over longer sequences, $ E_0\left[L\left(\omega^t\right)\right] $.

With $ T=10 $, we find that the Monte Carlo method significantly underestimates the mean, while significance sampling
still produces an estimate close to its theoretical value of one.

```python
estimate(g_a, g_b, g_a, g_b, T=10, N=10000)
```

    0.8305386874185359

```python
estimate(g_a, g_b, h_a, h_b, T=10, N=10000)
```

    0.9454402445583421

## Distribution of Sample Average

Next, we examine the bias and efficiency of the Monte Carlo and significance sampling approaches.

The following code generates distributions of estimates using both Monte Carlo and significance sampling methods.

```python
@njit(parallel=True)
def simulate(p_a, p_b, q_a, q_b, N_simu, T=1):

    μ_L_p = np.empty(N_simu)
    μ_L_q = np.empty(N_simu)

    for i in prange(N_simu):
        μ_L_p[i] = estimate(p_a, p_b, p_a, p_b, T=T)
        μ_L_q[i] = estimate(p_a, p_b, q_a, q_b, T=T)

    return μ_L_p, μ_L_q
```

First, we consider estimating $ {E} \left[\ell\left(\omega\right)\right] $ by setting T=1.

We run 1000 simulations for each method.

```python
N_simu = 1000
μ_L_p, μ_L_q = simulate(g_a, g_b, h_a, h_b, N_simu)
```

```python
# standard Monte Carlo (mean and std)
np.nanmean(μ_L_p), np.nanvar(μ_L_p)
```

    (0.9988576671203021, 0.008034320570198204)

```python
# importance sampling (mean and std)
np.nanmean(μ_L_q), np.nanvar(μ_L_q)
```

    (0.9998255923197714, 2.3725831272302702e-05)

While both methods tend to provide a mean estimate of $ {E} \left[\ell\left(\omega\right)\right] $ close to $ 1 $, the significance sampling estimates show less variance.

Next, we show distributions of estimates for $ \hat{E} \left[L\left(\omega^t\right)\right] $, for cases where $ T=1, 5, 10, 20 $.

```python
fig, axs = plt.subplots(2, 2, figsize=(14, 10))

μ_range = np.linspace(0, 2, 100)

for i, t in enumerate([1, 5, 10, 20]):
    row = i // 2
    col = i % 2

    μ_L_p, μ_L_q = simulate(g_a, g_b, h_a, h_b, N_simu, T=t)
    μ_hat_p, μ_hat_q = np.nanmean(μ_L_p), np.nanmean(μ_L_q)
    σ_hat_p, σ_hat_q = np.nanvar(μ_L_p), np.nanvar(μ_L_q)

    axs[row, col].set_xlabel('$μ_L$')
    axs[row, col].set_ylabel('frequency')
    axs[row, col].set_title(f'$T$={t}')
    n_p, bins_p, _ = axs[row, col].hist(μ_L_p, bins=μ_range, color='r', alpha=0.5, label='$g$ generating')
    n_q, bins_q, _ = axs[row, col].hist(μ_L_q, bins=μ_range, color='b', alpha=0.5, label='$h$ generating')
    axs[row, col].legend(loc=4)

    for n, bins, μ_hat, σ_hat in [[n_p, bins_p, μ_hat_p, σ_hat_p],
                                  [n_q, bins_q, μ_hat_q, σ_hat_q]]:
        idx = np.argmax(n)
        axs[row, col].text(bins[idx], n[idx], '$\hat{μ}$='+f'{μ_hat:.4g}'+', $\hat{σ}=$'+f'{σ_hat:.4g}')

plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/50.%20Computing%20Mean%20of%20a%20Likelihood%20Ratio%20Process_files/50.%20Computing%20Mean%20of%20a%20Likelihood%20Ratio%20Process_31_0.png)
</div>

The simulation exercises above demonstrate that the significance sampling estimates are unbiased for all $ T $
while the standard Monte Carlo estimates are biased downwards.

Evidently, the bias increases as $ T $ increases.

## Further Considerations on Sampling Distribution Selection

Previously, we arbitrarily chose $ h = Beta(0.5,0.5) $ as the significance distribution.

Is there an optimal significance distribution?

In our specific case, we know in advance that $ E_0 \left[ L\left(\omega^t\right) \right] = 1 $.

We can leverage this knowledge to our advantage.

Thus, suppose we simply use $ h = f $.

When estimating the mean of the likelihood ratio (T=1), we get:

$$
\hat{E}^f \left[\ell(\omega) \frac{g(\omega)}{f(\omega)} \right] = \hat{E}^f \left[\frac{f(\omega)}{g(\omega)} \frac{g(\omega)}{f(\omega)} \right] = \frac{1}{N} \sum_{i=1}^{N} \ell(w_i^f) \frac{g(w_i^f)}{f(w_i^f)} = 1
$$

```python
μ_L_p, μ_L_q = simulate(g_a, g_b, F_a, F_b, N_simu)
```

```python
# importance sampling (mean and std)
np.nanmean(μ_L_q), np.nanvar(μ_L_q)
```

    (1.0, 0.0)

We could also use other distributions as our significance distribution.

Below we select a few and compare their sampling characteristics.

```python
a_list = [0.5, 1., 2.]
b_list = [0.5, 1.2, 5.]
```

```python
w_range = np.linspace(1e-5, 1-1e-5, 1000)

plt.plot(w_range, g(w_range), label=f'p=Beta({g_a}, {g_b})')
plt.plot(w_range, p(w_range, a_list[0], b_list[0]), label=f'g=Beta({a_list[0]}, {b_list[0]})')
plt.plot(w_range, p(w_range, a_list[1], b_list[1]), label=f'g=Beta({a_list[1]}, {b_list[1]})')
plt.plot(w_range, p(w_range, a_list[2], b_list[2]), label=f'g=Beta({a_list[2]}, {b_list[2]})')
plt.title('real data generating process $g$ and importance distribution $h$')
plt.legend()
plt.ylim([0., 3.])
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/50.%20Computing%20Mean%20of%20a%20Likelihood%20Ratio%20Process_files/50.%20Computing%20Mean%20of%20a%20Likelihood%20Ratio%20Process_38_0.png)
</div>

We examine two additional distributions.

Recall that $ h_1 $ is the original $ Beta(0.5,0.5) $ distribution we used earlier.

$ h_2 $ is the $ Beta(1,1.2) $ distribution.

Notice how $ h_2 $ has a similar shape to $ g $ at higher distribution values but more mass at lower values.

We suspect that $ h_2 $ should be an effective significance sampling distribution.

$ h_3 $ is the $ Beta(2,5) $ distribution.

Observe that $ h_3 $ has zero mass at values very close to 0 and at values close to 1.

We suspect that $ h_3 $ will be an ineffective significance sampling distribution.

We first simulate and plot the distribution of estimates for $ \hat{E} \left[L\left(\omega^t\right)\right] $ using $ h_2 $ as the significance sampling distribution.

```python
h_a = a_list[1]
h_b = b_list[1]

fig, axs = plt.subplots(1,2, figsize=(14, 10))

μ_range = np.linspace(0, 2, 100)

for i, t in enumerate([1, 20]):


    μ_L_p, μ_L_q = simulate(g_a, g_b, h_a, h_b, N_simu, T=t)
    μ_hat_p, μ_hat_q = np.nanmean(μ_L_p), np.nanmean(μ_L_q)
    σ_hat_p, σ_hat_q = np.nanvar(μ_L_p), np.nanvar(μ_L_q)

    axs[i].set_xlabel('$μ_L$')
    axs[i].set_ylabel('frequency')
    axs[i].set_title(f'$T$={t}')
    n_p, bins_p, _ = axs[i].hist(μ_L_p, bins=μ_range, color='r', alpha=0.5, label='$g$ generating')
    n_q, bins_q, _ = axs[i].hist(μ_L_q, bins=μ_range, color='b', alpha=0.5, label='$h_2$ generating')
    axs[i].legend(loc=4)

    for n, bins, μ_hat, σ_hat in [[n_p, bins_p, μ_hat_p, σ_hat_p],
                                  [n_q, bins_q, μ_hat_q, σ_hat_q]]:
        idx = np.argmax(n)
        axs[i].text(bins[idx], n[idx], '$\hat{μ}$='+f'{μ_hat:.4g}'+', $\hat{σ}=$'+f'{σ_hat:.4g}')

plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/50.%20Computing%20Mean%20of%20a%20Likelihood%20Ratio%20Process_files/50.%20Computing%20Mean%20of%20a%20Likelihood%20Ratio%20Process_40_0.png)
</div>

Our simulations suggest that indeed $ h_2 $ is a very effective significance sampling distribution for our problem.

Even at $ T=20 $, the mean is very close to $ 1 $ and the variance is small.

```python
h_a = a_list[2]
h_b = b_list[2]

fig, axs = plt.subplots(1,2, figsize=(14, 10))

μ_range = np.linspace(0, 2, 100)

for i, t in enumerate([1, 20]):


    μ_L_p, μ_L_q = simulate(g_a, g_b, h_a, h_b, N_simu, T=t)
    μ_hat_p, μ_hat_q = np.nanmean(μ_L_p), np.nanmean(μ_L_q)
    σ_hat_p, σ_hat_q = np.nanvar(μ_L_p), np.nanvar(μ_L_q)

    axs[i].set_xlabel('$μ_L$')
    axs[i].set_ylabel('frequency')
    axs[i].set_title(f'$T$={t}')
    n_p, bins_p, _ = axs[i].hist(μ_L_p, bins=μ_range, color='r', alpha=0.5, label='$g$ generating')
    n_q, bins_q, _ = axs[i].hist(μ_L_q, bins=μ_range, color='b', alpha=0.5, label='$h_3$ generating')
    axs[i].legend(loc=4)

    for n, bins, μ_hat, σ_hat in [[n_p, bins_p, μ_hat_p, σ_hat_p],
                                  [n_q, bins_q, μ_hat_q, σ_hat_q]]:
        idx = np.argmax(n)
        axs[i].text(bins[idx], n[idx], '$\hat{μ}$='+f'{μ_hat:.4g}'+', $\hat{σ}=$'+f'{σ_hat:.4g}')

plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/50.%20Computing%20Mean%20of%20a%20Likelihood%20Ratio%20Process_files/50.%20Computing%20Mean%20of%20a%20Likelihood%20Ratio%20Process_42_0.png)
</div>

However, $ h_3 $ is clearly an ineffective significance sampling distribution for our problem,
with a mean estimate far from $ 1 $ for $ T = 20 $.

Notice that even at $ T = 1 $, the mean estimate with significance sampling is more biased than simply sampling with $ g $ itself.

Thus, our simulations indicate that we would achieve better results by using Monte Carlo
approximations under $ g $ rather than using $ h_3 $ as a significance sampling distribution for our problem.
