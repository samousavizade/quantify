---
title: Likelihood Ratio Processes
draft: false
summary: test
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

# Processes of Likelihood Ratios

## Summary

This lecture explores likelihood ratio processes and their applications.

Some key points we'll cover include:

- An unusual property of likelihood ratio processes
- The role of likelihood ratio processes in frequentist hypothesis testing
- How a **receiver operator characteristic curve** encapsulates information about false alarm probability and power in frequentist hypothesis testing
- The decision rule developed by the United States Navy during World War II, which Captain Garret L. Schyler questioned and asked Milton Friedman to explain.

Let's begin by importing some Python tools.

```python
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (11, 5)  #set default figure size
import numpy as np
from numba import vectorize, njit
from math import gamma
from scipy.integrate import quad
```

## Likelihood Ratio Process

A non-negative random variable $ W $ follows one of two probability density functions, either $ f $ or $ g $.

Nature decides once and for all, before time begins, whether to draw a sequence of IID samples from either $ f $ or $ g $.

We'll occasionally use $ q $ to represent the density that nature chose permanently, so $ q $ is either $ f $ or $ g $, indefinitely.

Nature is aware of which density it consistently draws from, but we, the observers, are not.

We are familiar with both $ f $ and $ g $, but we don't know which density nature selected.

However, we want to find out.

To achieve this, we rely on observations.

We observe a sequence $ \{w*t\}*{t=1}^T $ of $ T $ IID draws from either $ f $ or $ g $.

We aim to use these observations to determine whether nature chose $ f $ or $ g $.

A **likelihood ratio process** is a valuable tool for this purpose.

To start, we define a key component of a likelihood ratio process, namely, the time $ t $ likelihood ratio as the random variable

$$
\ell (w_t)=\frac{f\left(w_t\right)}{g\left(w_t\right)},\quad t\geq1.
$$

We assume that $ f $ and $ g $ both assign positive probabilities to the same intervals of possible realizations of the random variable $ W $.

This means that under the $ g $ density, $ \ell (w*t)= \frac{f\left(w*{t}\right)}{g\left(w\_{t}\right)} $ is clearly a non-negative random variable with a mean of 1.

A **likelihood ratio process** for sequence $ \left\{ w*{t}\right\} *{t=1}^{\infty} $ is defined as

$$
L\left(w^{t}\right)=\prod_{i=1}^{t} \ell (w_i),
$$

where $ w^t=\{ w_1,\dots,w_t\} $ represents a history of observations up to and including time $ t $.

For brevity, we'll sometimes write $ L_t = L(w^t) $.

Note that the likelihood process follows the _recursion_ or _multiplicative decomposition_

$$
L(w^t) = \ell (w_t) L (w^{t-1}) .
$$

The likelihood ratio and its logarithm are essential tools for making inferences using a classic frequentist approach developed by Neyman and Pearson [[Neyman and Pearson, 1933](/courses/Introduction-to-Quantitative-Economics/References#id255)].

To help us understand how this works, the following Python code evaluates $ f $ and $ g $ as two different beta distributions, then calculates and simulates an associated likelihood ratio process by generating a sequence $ w^t $ from one of the two probability distributions, for example, a sequence of IID draws from $ g $.

```python
# Parameters in the two beta distributions.
F_a, F_b = 1, 1
G_a, G_b = 3, 1.2

@vectorize
def p(x, a, b):
    r = gamma(a + b) / (gamma(a) * gamma(b))
    return r * x** (a-1) * (1 - x) ** (b-1)

# The two density functions.
f = njit(lambda x: p(x, F_a, F_b))
g = njit(lambda x: p(x, G_a, G_b))
```

```python
@njit
def simulate(a, b, T=50, N=500):
    '''
    Generate N sets of T observations of the likelihood ratio,
    return as N x T matrix.

    '''

    l_arr = np.empty((N, T))

    for i in range(N):

        for j in range(T):
            w = np.random.beta(a, b)
            l_arr[i, j] = f(w) / g(w)

    return l_arr
```

## Nature Consistently Samples from Density g

First, we simulate the likelihood ratio process when nature consistently draws from $ g $.

```python
l_arr_g = simulate(G_a, G_b)
l_seq_g = np.cumprod(l_arr_g, axis=1)
```

```python
N, T = l_arr_g.shape

for i in range(N):

    plt.plot(range(T), l_seq_g[i, :], color='b', lw=0.8, alpha=0.5)

plt.ylim([0, 3])
plt.title("$L(w^{t})$ paths");
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/49.%20Likelihood%20Ratio%20Processes_files/49.%20Likelihood%20Ratio%20Processes_10_0.png)
</div>

Clearly, as the sample length $ T $ increases, most of the probability mass shifts towards zero

To visualize this more distinctly, we plot the fraction of paths $ L\left(w^{t}\right) $ that fall within the interval $ \left[0, 0.01\right] $ over time.

```python
plt.plot(range(T), np.sum(l_seq_g <= 0.01, axis=0) / N)
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/49.%20Likelihood%20Ratio%20Processes_files/49.%20Likelihood%20Ratio%20Processes_12_1.png)
</div>

Despite the apparent convergence of most probability mass to a very small interval near $ 0 $, the unconditional mean of $ L\left(w^t\right) $ under probability density $ g $ remains exactly 1 for all $ t $.

To confirm this assertion, first note that as mentioned earlier, the unconditional mean $ E\left[\ell \left(w_{t}\right)\bigm|q=g\right] $ is 1 for all $ t $:

$$
\begin{aligned}
E\left[\ell \left(w_{t}\right)\bigm|q=g\right] &=\int\frac{f\left(w_{t}\right)}{g\left(w_{t}\right)}g\left(w_{t}\right)dw_{t} \\
&=\int f\left(w_{t}\right)dw_{t} \\
&=1,
\end{aligned}
$$

which directly implies

$$
\begin{aligned}
E\left[L\left(w^{1}\right)\bigm|q=g\right] &=E\left[\ell \left(w_{1}\right)\bigm|q=g\right]\\
&=1.\\
\end{aligned}
$$

Since $ L(w^t) = \ell(w*t) L(w^{t-1}) $ and $ \{w_t\}*{t=1}^t $ is an IID sequence, we have

$$
\begin{aligned}
E\left[L\left(w^{t}\right)\bigm|q=g\right] &=E\left[L\left(w^{t-1}\right)\ell \left(w_{t}\right)\bigm|q=g\right] \\
&=E\left[L\left(w^{t-1}\right)E\left[\ell \left(w_{t}\right)\bigm|q=g,w^{t-1}\right]\bigm|q=g\right] \\
&=E\left[L\left(w^{t-1}\right)E\left[\ell \left(w_{t}\right)\bigm|q=g\right]\bigm|q=g\right] \\
&=E\left[L\left(w^{t-1}\right)\bigm|q=g\right] \\
\end{aligned}
$$

for any $ t \geq 1 $.

Mathematical induction suggests $ E\left[L\left(w^{t}\right)\bigm|q=g\right]=1 $ for all $ t \geq 1 $.

## Unusual Characteristic

How can $ E\left[L\left(w^{t}\right)\bigm|q=g\right]=1 $ be true when most of the probability mass of the likelihood ratio process is accumulating near $ 0 $ as $ t \rightarrow + \infty $?

The explanation must be that as $ t \rightarrow + \infty $, the distribution of $ L_t $ becomes increasingly heavy-tailed: sufficient mass shifts to larger and larger values of $ L_t $ to maintain the mean of $ L_t $ at one, despite most of the probability mass gathering near $ 0 $.

To demonstrate this unusual property, we simulate numerous paths and calculate the unconditional mean of $ L\left(w^t\right) $ by averaging across these many paths at each $ t $.

```python
l_arr_g = simulate(G_a, G_b, N=50000)
l_seq_g = np.cumprod(l_arr_g, axis=1)
```

It would be beneficial to use simulations to verify that unconditional means $ E\left[L\left(w^{t}\right)\right] $ equal unity by averaging across sample paths.

However, it would be too computationally intensive for us to do that here simply by applying a standard Monte Carlo simulation approach.

The reason is that the distribution of $ L\left(w^{t}\right) $ is extremely skewed for large values of $ t $.

Because the probability density in the right tail is close to $ 0 $, it requires too much computer time to sample enough points from the right tail.

There, we describe an alternative method to compute the mean of a likelihood ratio by calculating the mean of a _different_ random variable by sampling from a _different_ probability distribution.

## Nature Consistently Samples from Density f

Now consider that before time $ 0 $, nature permanently decided to draw repeatedly from density $ f $.

While the mean of the likelihood ratio $ \ell \left(w\_{t}\right) $ under density $ g $ is $ 1 $, its mean under the density $ f $ is greater than one.

To demonstrate this, we calculate

$$
\begin{aligned}
E\left[\ell \left(w_{t}\right)\bigm|q=f\right] &=\int\frac{f\left(w_{t}\right)}{g\left(w_{t}\right)}f\left(w_{t}\right)dw_{t} \\
&=\int\frac{f\left(w_{t}\right)}{g\left(w_{t}\right)}\frac{f\left(w_{t}\right)}{g\left(w_{t}\right)}g\left(w_{t}\right)dw_{t} \\
&=\int \ell \left(w_{t}\right)^{2}g\left(w_{t}\right)dw_{t} \\
&=E\left[\ell \left(w_{t}\right)^{2}\mid q=g\right] \\
&=E\left[\ell \left(w_{t}\right)\mid q=g\right]^{2}+Var\left(\ell \left(w_{t}\right)\mid q=g\right) \\
&>E\left[\ell \left(w_{t}\right)\mid q=g\right]^{2} = 1 \\
\end{aligned}
$$

This in turn implies that the unconditional mean of the likelihood ratio process $ L(w^t) $ diverges towards $ + \infty $.

The simulations below confirm this conclusion.

Please note the scale of the $ y $ axis.

```python
l_arr_f = simulate(F_a, F_b, N=50000)
l_seq_f = np.cumprod(l_arr_f, axis=1)
```

```python
N, T = l_arr_f.shape
plt.plot(range(T), np.mean(l_seq_f, axis=0))
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/49.%20Likelihood%20Ratio%20Processes_files/49.%20Likelihood%20Ratio%20Processes_19_1.png)
</div>

We also plot the probability that $ L\left(w^t\right) $ falls into the interval $ [10000, \infty) $ as a function of time and observe how quickly probability mass diverges to $ +\infty $.

```python
plt.plot(range(T), np.sum(l_seq_f > 10000, axis=0) / N)
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/49.%20Likelihood%20Ratio%20Processes_files/49.%20Likelihood%20Ratio%20Processes_21_1.png)
</div>

## Test of Likelihood Ratio

We now describe how to use the methodology of Neyman and Pearson [[Neyman and Pearson, 1933](/courses/Introduction-to-Quantitative-Economics/References#id255)] to test the hypothesis that history $ w^t $ is generated by repeated IID draws from density $ g $.

Let $ q $ represent the data generating process, so that $ q=f \text{ or } g $.

After observing a sample $ \{W*i\}*{i=1}^t $, we want to determine whether nature is drawing from $ g $ or from $ f $ by conducting a (frequentist) hypothesis test.

We specify

- Null hypothesis $ H_0 $: $ q=f $,
- Alternative hypothesis $ H_1 $: $ q=g $.

Neyman and Pearson proved that the optimal way to test this hypothesis is to use a **likelihood ratio test** that takes the form:

- reject $ H_0 $ if $ L(W^t) < c $,
- accept $ H_0 $ otherwise.

where $ c $ is a given discrimination threshold, to be chosen in a way we'll soon describe.

This test is _optimal_ in the sense that it is a **uniformly most powerful** test.

To understand what this means, we need to define probabilities of two important events that allow us to characterize a test associated with a given threshold $ c $.

The two probabilities are:

- Probability of detection (= power = 1 minus probability of Type II error):
  $$
  1-\beta \equiv \Pr\left\{ L\left(w^{t}\right) < c \mid q=g\right\}
  $$
- Probability of false alarm (= significance level = probability of Type I error):
  $$
  \alpha \equiv \Pr\left\{ L\left(w^{t}\right) < c \mid q=f\right\}
  $$

If $ c $ is set very low, we are unlikely to get a false alarm, but we may also be less likely to detect $ q=g $.

If $ c $ is set very high, we are more likely to detect $ q=g $, but we are also more likely to experience a false alarm.

A **receiver operating characteristic curve** (ROC) shows the trade-off between probability of detection and probability of false alarm.

Roughly speaking, the ROC curve is created by varying the critical value $ c $ between $ 0 $ and $ \infty $.

```python
c = 1
```

## Kullback–Leibler Divergence

If we fix $ h(w) $ as a reference density, the Kullback–Leibler divergence of $ g(w) $ from $ h(w) $ is defined as

$$
K_g = \int \ln \left(\frac{g(w)}{h(w)}\right)g(w)dw.
$$

Similarly, the Kullback–Leibler divergence of $ f(w) $ from $ h(w) $ is

$$
K_f = \int \ln \left(\frac{f(w)}{h(w)}\right)f(w)dw.
$$

The Kullback–Leibler divergence measures how one probability distribution diverges from a second, expected probability distribution.

If $ K_g < K_f $, $ g $ is closer to $ h $ than $ f $ is.

- In that scenario, we will observe that $ L\left(w^t\right) \rightarrow + \infty $

We will now experiment with an $ h $ that is also a beta distribution

We will start by setting parameters $ G_a $ and $ G_b $ so that
$ h $ is closer to $ g $

```python
fig, axs = plt.subplots(2, 2, figsize=(12, 8))
fig.suptitle('distribution of $log(L(w^t))$ under f or g', fontsize=15)

for i, t in enumerate([1, 7, 14, 21]):
    nr = i // 2
    nc = i % 2

    axs[nr, nc].axvline(np.log(c), color="k", ls="--")

    hist_f, x_f = np.histogram(np.log(l_seq_f[:, t]), 200, density=True)
    hist_g, x_g = np.histogram(np.log(l_seq_g[:, t]), 200, density=True)

    axs[nr, nc].plot(x_f[1:], hist_f, label="dist under f")
    axs[nr, nc].plot(x_g[1:], hist_g, label="dist under g")

    for i, (x, hist, label) in enumerate(zip([x_f, x_g], [hist_f, hist_g], ["Type I error", "Type II error"])):
        ind = x[1:] <= np.log(c) if i == 0 else x[1:] > np.log(c)
        axs[nr, nc].fill_between(x[1:][ind], hist[ind], alpha=0.5, label=label)

    axs[nr, nc].legend()
    axs[nr, nc].set_title(f"t={t}")

plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/49.%20Likelihood%20Ratio%20Processes_files/49.%20Likelihood%20Ratio%20Processes_25_0.png)
</div>

The graph below shows more clearly that, when we hold the threshold
$ c $ fixed, the probability of detection monotonically increases with increases in
$ t $ and that the probability of a false alarm monotonically decreases.

```python
PD = np.empty(T)
PFA = np.empty(T)

for t in range(T):
    PD[t] = np.sum(l_seq_g[:, t] < c) / N
    PFA[t] = np.sum(l_seq_f[:, t] < c) / N

plt.plot(range(T), PD, label="Probability of detection")
plt.plot(range(T), PFA, label="Probability of false alarm")
plt.xlabel("t")
plt.title("$c=1$")
plt.legend()
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/49.%20Likelihood%20Ratio%20Processes_files/49.%20Likelihood%20Ratio%20Processes_27_0.png)
</div>

For a given sample size $ t $, the threshold $ c $ uniquely pins down probabilities
of both types of error.

If for a fixed $ t $ we now free up and move $ c $, we will sweep out the probability
of detection as a function of the probability of false alarm.

This produces what is called a [receiver operating characteristic
curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).

Below, we plot receiver operating characteristic curves for different
sample sizes $ t $.

```python
PFA = np.arange(0, 100, 1)

for t in range(1, 15, 4):
    percentile = np.percentile(l_seq_f[:, t], PFA)
    PD = [np.sum(l_seq_g[:, t] < p) / N for p in percentile]

    plt.plot(PFA / 100, PD, label=f"t={t}")

plt.scatter(0, 1, label="perfect detection")
plt.plot([0, 1], [0, 1], color='k', ls='--', label="random detection")

plt.arrow(0.5, 0.5, -0.15, 0.15, head_width=0.03)
plt.text(0.35, 0.7, "better")
plt.xlabel("Probability of false alarm")
plt.ylabel("Probability of detection")
plt.legend()
plt.title("Receiver Operating Characteristic Curve")
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/49.%20Likelihood%20Ratio%20Processes_files/49.%20Likelihood%20Ratio%20Processes_29_0.png)
</div>

Notice that as $ t $ increases, we are assured a larger probability
of detection and a smaller probability of false alarm associated with
a given discrimination threshold $ c $.

As $ t \rightarrow + \infty $, we approach the perfect detection
curve that is indicated by a right angle hinging on the blue dot.

For a given sample size $ t $, the discrimination threshold $ c $ determines a point on the receiver operating
characteristic curve.

It is up to the test designer to trade off probabilities of
making the two types of errors.

But we know how to choose the smallest sample size to achieve given targets for
the probabilities.

Typically, frequentists aim for a high probability of detection that
respects an upper bound on the probability of false alarm.

Below we show an example in which we fix the probability of false alarm at
$ 0.05 $.

The required sample size for making a decision is then determined by a
target probability of detection, for example, $ 0.9 $, as depicted in the following graph.

```python
PFA = 0.05
PD = np.empty(T)

for t in range(T):

    c = np.percentile(l_seq_f[:, t], PFA * 100)
    PD[t] = np.sum(l_seq_g[:, t] < c) / N

plt.plot(range(T), PD)
plt.axhline(0.9, color="k", ls="--")

plt.xlabel("t")
plt.ylabel("Probability of detection")
plt.title(f"Probability of false alarm={PFA}")
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/49.%20Likelihood%20Ratio%20Processes_files/49.%20Likelihood%20Ratio%20Processes_31_0.png)
</div>

The United States Navy evidently used a procedure like this to select a sample size $ t $ for doing quality
control tests during World War II.

A Navy Captain who had been ordered to perform tests of this kind had doubts about it that he
presented to Milton Friedman.

## Kullback–Leibler Divergence

Now let’s consider a case in which neither $ g $ nor $ f $
generates the data.

Instead, a third distribution $ h $ does.

Let’s watch how how the cumulated likelihood ratios $ f/g $ behave
when $ h $ governs the data.

A key tool here is called **Kullback–Leibler divergence**.

It is also called **relative entropy**.

It measures how one probability distribution differs from another.

In our application, we want to measure how $ f $ or $ g $
diverges from $ h $

The two Kullback–Leibler divergences pertinent for us are $ K_f $
and $ K_g $ defined as

$$
\begin{aligned}
K_{f}   &=E_{h}\left[\log\left(\frac{f\left(w\right)}{h\left(w\right)}\right)\frac{f\left(w\right)}{h\left(w\right)}\right] \\
&=\int\log\left(\frac{f\left(w\right)}{h\left(w\right)}\right)\frac{f\left(w\right)}{h\left(w\right)}h\left(w\right)dw \\
&=\int\log\left(\frac{f\left(w\right)}{h\left(w\right)}\right)f\left(w\right)dw
\end{aligned}
$$

$$
\begin{aligned}
K_{g}   &=E_{h}\left[\log\left(\frac{g\left(w\right)}{h\left(w\right)}\right)\frac{g\left(w\right)}{h\left(w\right)}\right] \\
&=\int\log\left(\frac{g\left(w\right)}{h\left(w\right)}\right)\frac{g\left(w\right)}{h\left(w\right)}h\left(w\right)dw \\
&=\int\log\left(\frac{g\left(w\right)}{h\left(w\right)}\right)g\left(w\right)dw
\end{aligned}
$$

When $ K_g < K_f $, $ g $ is closer to $ h $ than $ f $
is.

- In that case we’ll find that $ L\left(w^t\right) \rightarrow 0 $.

When $ K_g > K_f $, $ f $ is closer to $ h $ than $ g $
is.

- In that case we’ll find that
  $ L\left(w^t\right) \rightarrow + \infty $

We’ll now experiment with an $ h $ is also a beta distribution

We’ll start by setting parameters $ G_a $ and $ G_b $ so that
$ h $ is closer to $ g $

```python
H_a, H_b = 3.5, 1.8

h = njit(lambda x: p(x, H_a, H_b))
```

```python
x_range = np.linspace(0, 1, 100)
plt.plot(x_range, f(x_range), label='f')
plt.plot(x_range, g(x_range), label='g')
plt.plot(x_range, h(x_range), label='h')

plt.legend()
plt.show()
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/49.%20Likelihood%20Ratio%20Processes_files/49.%20Likelihood%20Ratio%20Processes_35_0.png)
</div>

Let’s compute the Kullback–Leibler discrepancies by quadrature integration.

```python
def KL_integrand(w, q, h):

    m = q(w) / h(w)

    return np.log(m) * q(w)
```

```python
def compute_KL(h, f, g):

    Kf, _ = quad(KL_integrand, 0, 1, args=(f, h))
    Kg, _ = quad(KL_integrand, 0, 1, args=(g, h))

    return Kf, Kg
```

```python
Kf, Kg = compute_KL(h, f, g)
Kf, Kg
```

    (0.7902536603660166, 0.08554075759988751)

We have $ K_g < K_f $.

Next, we can verify our conjecture about $ L\left(w^t\right) $ by simulation.

```python
l_arr_h = simulate(H_a, H_b)
l_seq_h = np.cumprod(l_arr_h, axis=1)
```

The figure below plots over time the fraction of paths
$ L\left(w^t\right) $ that fall in the interval $ [0,0.01] $.

Notice that it converges to 1 as expected when $ g $ is closer to
$ h $ than $ f $ is.

```python
N, T = l_arr_h.shape
plt.plot(range(T), np.sum(l_seq_h <= 0.01, axis=0) / N)
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/49.%20Likelihood%20Ratio%20Processes_files/49.%20Likelihood%20Ratio%20Processes_43_1.png)
</div>

We can also try an $ h $ that is closer to $ f $ than is
$ g $ so that now $ K_g $ is larger than $ K_f $.

```python
H_a, H_b = 1.2, 1.2
h = njit(lambda x: p(x, H_a, H_b))
```

```python
Kf, Kg = compute_KL(h, f, g)
Kf, Kg
```

    (0.012392497544526706, 0.35377684280997634)

```python
l_arr_h = simulate(H_a, H_b)
l_seq_h = np.cumprod(l_arr_h, axis=1)
```

Now probability mass of $ L\left(w^t\right) $ falling above
$ 10000 $ diverges to $ +\infty $.

```python
N, T = l_arr_h.shape
plt.plot(range(T), np.sum(l_seq_h > 10000, axis=0) / N)
```

<div
  className="my-1 overflow-hidden px-2 xl:my-1 xl:w-1/2 xl:px-2"
  style={{
    display: 'block',
    marginLeft: 'auto',
    marginRight: 'auto',
    width: '90%',
    alignItems: 'center',
    justifyContent: 'center',
  }}
>
  ![png](/static/courses/Advances%20in%20Quantitative%20Economics/49.%20Likelihood%20Ratio%20Processes_files/49.%20Likelihood%20Ratio%20Processes_49_1.png)
</div>
