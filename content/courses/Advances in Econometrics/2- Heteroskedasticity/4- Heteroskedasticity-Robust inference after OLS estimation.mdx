---
title: Heteroskedasticity-Robust Inference After OLS Estimation
draft: false
summary: test
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

## Heteroskedasticity-Robust Inference After OLS Estimation

### **Heteroskedasticity-Robust Inference**

Heteroskedasticity in a regression model implies that the variance of the error terms is not constant across observations. This can lead to inefficient OLS estimates and biased standard errors, which affect hypothesis testing and confidence intervals. To address these issues, heteroskedasticity-robust standard errors are used to provide valid inference even when heteroskedasticity is present.

The heteroskedasticity-robust variance-covariance matrix for the OLS estimator is given by:

$$
\hat{V}_{\text{robust}} = (X'X)^{-1} \left( \sum_{i=1}^{n} \hat{u}_i^2 x_i x_i' \right) (X'X)^{-1}
$$

where:

- $$ X $$ is the matrix of independent variables,
- $$ \hat{u}\_i $$ is the OLS residual for observation $$ i $$,
- $$ x_i $$ is the vector of independent variables for observation $$ i $$.

This adjustment ensures that the standard errors are consistent even when heteroskedasticity is present, allowing for reliable hypothesis tests and confidence intervals.

### **Computing Heteroskedasticity-Robust LM Tests**

The Lagrange Multiplier (LM) test, also known as the score test, can be adapted to be robust to heteroskedasticity. This involves using robust standard errors in calculating the test statistic. The LM test is used to test hypotheses about the parameters in a regression model, particularly when testing for omitted variables or incorrect functional form.

To compute a heteroskedasticity-robust LM test:

1. **Estimate the restricted model**: Estimate the model under the null hypothesis and obtain the residuals.

2. **Auxiliary regression**: Regress these residuals on all variables in both the restricted and unrestricted models.

3. **Compute the test statistic**: The robust LM statistic is given by:

$$
LM_{\text{robust}} = n R^2_{\text{aux}}
$$

where $$ R^2\_{\text{aux}} $$ is the R-squared from the auxiliary regression and $$ n $$ is the sample size.

4. **Inference**: The robust LM statistic follows a chi-square distribution with degrees of freedom equal to the number of restrictions under the null hypothesis.

These robust methods ensure that inference remains valid even when heteroskedasticity is present in the data, allowing researchers to draw reliable conclusions from their econometric analyses.

## Example of Heteroskedasticity-Robust Inference

To illustrate heteroskedasticity-robust inference, consider a simple linear regression model where we want to examine the relationship between individuals' income and their expenditure on luxury goods. Suppose we have collected data on income and expenditure for a sample of individuals.

### **Model Specification**

The model can be specified as:

$$
\text{Expenditure}_i = \beta_0 + \beta_1 \text{Income}_i + u_i
$$

where:

- $$ \text{Expenditure}\_i $$ is the expenditure on luxury goods for individual $$ i $$,
- $$ \text{Income}\_i $$ is the income of individual $$ i $$,
- $$ u_i $$ is the error term.

### **Presence of Heteroskedasticity**

In this context, heteroskedasticity might arise if the variability of expenditure increases with income. For example, wealthier individuals might have more varied spending habits, leading to larger deviations from the average expenditure as income increases.

### **Consequences for OLS**

If heteroskedasticity is present, the ordinary least squares (OLS) estimates of the coefficients ($$ \beta_0 $$ and $$ \beta_1 $$) remain unbiased, but the standard errors are biased. This bias in standard errors can lead to incorrect conclusions in hypothesis testing, such as underestimating or overestimating the significance of income on expenditure.
