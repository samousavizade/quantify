---
title: Q Learning
draft: false
summary: test
---

import TOCInline from 'pliny/ui/TOCInline';

<TOCInline toc={props.toc} asDisclosure />

# Q-Learning

In the previous section, we discussed the Value Iteration algorithm which requires accessing the complete Markov decision process (MDP), e.g., the transition and reward functions. In this section, we will look at Q-Learning ([Watkins and Dayan, 1992](https://link.springer.com/article/10.1007/BF00992698 'Watkins, C. J., & Dayan, P. (1992). Q-learning. Machine Learning, 8(3–4), 279–292.')) which is an algorithm to learn the value function without necessarily knowing the MDP. This algorithm embodies the central idea behind reinforcement learning: it will enable the robot to obtain its own data.

## 1. The Q-Learning Algorithm

Recall that value iteration for the action-value function in [Value Iteration](https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-031219-041220) corresponds to the update

$$
Q_{k+1}(s,a) = r(s,a) + \gamma \sum_{s' \in S} P(s' \mid s,a) \max_{a' \in A} Q_k(s',a');
$$

for all $s \in S$ and $a \in A$.

As we discussed, implementing this algorithm requires knowing the MDP, specifically the transition function $P(s' \mid s,a)$. The key idea behind Q-Learning is to replace the summation over all $s' \in S$ in the above expression by a summation over the states visited by the robot. This allows us to subvert the need to know the transition function.

### Detailed Explanation of Q-Learning

Q-Learning is an off-policy reinforcement learning algorithm that seeks to find the best action to take given the current state. It does this by learning a function $Q(s, a)$ which represents the expected utility of taking action $a$ in state $s$, and then following the optimal policy thereafter.

The update rule for Q-Learning is given by:

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$

where:

- $Q(s, a)$ is the current estimate of the action-value function.
- $\alpha$ is the learning rate, determining to what extent newly acquired information overrides old information.
- $r$ is the reward received after taking action $a$ in state $s$.
- $\gamma$ is the discount factor, which trades off the importance of sooner versus later rewards.
- $s'$ is the new state after taking action $a$.

### Conceptual Insights

- **Exploration vs. Exploitation**: Q-Learning relies on balancing exploration (trying new actions to discover their effects) and exploitation (choosing actions that are known to yield high rewards). This is often managed using strategies like epsilon-greedy, where with probability $\epsilon$, a random action is chosen, and with probability $1-\epsilon$, the action with the highest Q-value is chosen.

- **Convergence**: Under certain conditions, such as all state-action pairs being visited infinitely often and the learning rate $\alpha$ being sufficiently small, Q-Learning is guaranteed to converge to the optimal action-value function.

- **Off-Policy Learning**: Unlike on-policy methods, Q-Learning learns the value of the optimal policy independently of the agent's actions. This means it can learn from actions that are outside the current policy, making it more flexible in certain environments.

By leveraging these concepts, Q-Learning provides a robust framework for agents to learn optimal policies in environments where the model is unknown or difficult to compute.

## 2. An Optimization Problem Underlying Q-Learning

Let us imagine that the robot uses a policy $\pi_e(a \mid s)$ to take actions. Just like the previous chapter, it collects a dataset of $n$ trajectories of $T$ timesteps each $\{ (s_t^i, a_t^i)_{t=0,\ldots,T-1}\}_{i=1,\ldots, n}$. Recall that value iteration is really a set of constraints that ties together the action-value $Q^*(s, a)$ of different states and actions to each other. We can implement an approximate version of value iteration using the data that the robot has collected using $\pi_e$ as

$$
\hat{Q} = \min_Q \frac{1}{nT} \sum_{i=1}^n \sum_{t=0}^{T-1} \left(Q(s_t^i, a_t^i) - r(s_t^i, a_t^i) - \gamma \max_{a'} Q(s_{t+1}^i, a')\right)^2 \overset{\text{def}}{=} \ell(Q).
$$

Let us first observe the similarities and differences between this expression and value iteration above. If the robot's policy $\pi_e$ were equal to the optimal policy $\pi^*$, and if it collected an infinite amount of data, then this optimization problem would be identical to the optimization problem underlying value iteration. But while value iteration requires us to know $P(s' \mid s, a)$, the optimization objective does not have this term. We have not cheated: as the robot uses the policy $\pi_e$ to take an action $a_t^i$ at state $s_t^i$, the next state $s_{t+1}^i$ is a sample drawn from the transition function. So the optimization objective also has access to the transition function, but implicitly in terms of the data collected by the robot.

The variables of our optimization problem are $Q(s, a)$ for all $s \in \mathcal{S}$ and $a \in \mathcal{A}$. We can minimize the objective using gradient descent. For every pair $(s_t^i, a_t^i)$ in our dataset, we can write

$$
Q(s_t^i, a_t^i) \leftarrow Q(s_t^i, a_t^i) - \alpha \nabla_{Q(s_t^i, a_t^i)} \ell(Q) = (1 - \alpha) Q(s_t^i, a_t^i) - \alpha \left(r(s_t^i, a_t^i) + \gamma \max_{a'} Q(s_{t+1}^i, a')\right),
$$

where $\alpha$ is the learning rate. Typically in real problems, when the robot reaches the goal location, the trajectories end. The value of such a terminal state is zero because the robot does not take any further actions beyond this state. We should modify our update to handle such states as

$$
Q(s_t^i, a_t^i) \leftarrow (1 - \alpha) Q(s_t^i, a_t^i) - \alpha \left(r(s_t^i, a_t^i) + \gamma (1 - \mathbf{1}_{s_{t+1}^i \text{ is terminal}}) \max_{a'} Q(s_{t+1}^i, a')\right).
$$

where $\mathbf{1}_{s_{t+1}^i \text{ is terminal}}$ is an indicator variable that is one if $s_{t+1}^i$ is a terminal state and zero otherwise. The value of state-action tuples $(s, a)$ that are not a part of the dataset is set to $-\infty$. This algorithm is known as Q-Learning.

Given the solution of these updates $\hat{Q}$, which is an approximation of the optimal value function $Q^*$, we can obtain the optimal deterministic policy corresponding to this value function easily using

$$
\hat{\pi}(s) = \arg\max_a \hat{Q}(s, a).
$$

There can be situations when there are multiple deterministic policies that correspond to the same optimal value function; such ties can be broken arbitrarily because they have the same value function.

## 3. Exploration in Q-Learning

The policy used by the robot to collect data $\pi_e$ is critical to ensure that Q-Learning works well. Afterall, we have replaced the expectation over $s'$ using the transition function $P(s' \mid s, a)$ using the data collected by the robot. If the policy $\pi_e$ does not reach diverse parts of the state-action space, then it is easy to imagine our estimate $\hat{Q}$ will be a poor approximation of the optimal $Q^*$. It is also important to note that in such a situation, the estimate of $Q^*$ at _all_ _states_ $s \in \mathcal{S}$ will be bad, not just the ones visited by $\pi_e$. This is because the Q-Learning objective (or value iteration) is a constraint that ties together the value of all state-action pairs. It is therefore critical to pick the correct policy $\pi_e$ to collect data.

We can mitigate this concern by picking a completely random policy $\pi_e$ that samples actions uniformly randomly from $\mathcal{A}$. Such a policy would visit all states, but it will take a large number of trajectories before it does so.

We thus arrive at the second key idea in Q-Learning, namely exploration. Typical implementations of Q-Learning tie together the current estimate of $Q$ and the policy $\pi_e$ to set

$$
\pi_e(a \mid s) = \begin{cases}
\arg\max_{a'} \hat{Q}(s, a') & \text{with prob. } 1 - \epsilon \\
\text{uniform}(\mathcal{A}) & \text{with prob. } \epsilon,
\end{cases}
$$

where $\epsilon$ is called the "exploration parameter" and is chosen by the user. The policy $\pi_e$ is called an exploration policy. This particular $\pi_e$ is called an $\epsilon$-greedy exploration policy because it chooses the optimal action (under the current estimate $\hat{Q}$) with probability $1 - \epsilon$ but explores randomly with the remainder probability $\epsilon$. We can also use the so-called softmax exploration policy

$$
\pi_e(a \mid s) = \frac{e^{\hat{Q}(s, a) / T}}{\sum_{a'} e^{\hat{Q}(s, a') / T}};
$$

where the hyper-parameter $T$ is called temperature. A large value of $\epsilon$ in $\epsilon$-greedy policy functions similarly to a large value of temperature $T$ for the softmax policy.

It is important to note that when we pick an exploration that depends upon the current estimate of the action-value function $\hat{Q}$, we need to resolve the optimization problem periodically. Typical implementations of Q-Learning make one mini-batch update using a few state-action pairs in the collected dataset (typically the ones collected from the previous timestep of the robot) after taking every action using $\pi_e$.

### Balancing Exploration and Exploitation

The choice of the exploration parameter $\epsilon$ (or temperature $T$) is crucial for the performance of Q-Learning. If $\epsilon$ is too high, the agent will spend too much time exploring and may not converge to the optimal policy. On the other hand, if $\epsilon$ is too low, the agent may get stuck in suboptimal policies.

A common strategy is to start with a high value of $\epsilon$ and gradually decrease it over time. This allows the agent to explore more in the beginning when it knows little about the environment, and exploit more later when it has learned a good policy. This is known as an $\epsilon$-greedy strategy with $\epsilon$-decay.

Another approach is to use more sophisticated exploration strategies like Upper Confidence Bound (UCB) or Thompson Sampling. These methods balance exploration and exploitation by considering not only the estimated Q-values but also the uncertainty in these estimates.

### Importance of Diverse Data Collection

The quality of the learned Q-function depends heavily on the diversity of the collected data. If the exploration policy only visits a small subset of the state-action space, the learned Q-function will be inaccurate for the unvisited states and actions.

This is particularly problematic in environments with sparse rewards, where most state-action pairs lead to no immediate reward. In such cases, random exploration may take a very long time to discover the rewarding states.

More sophisticated exploration strategies like curiosity-driven exploration or intrinsic motivation can help the agent to explore in a more directed manner, by seeking out novel or surprising states.

## 4. The "Self-correcting" Property of Q-Learning

The dataset collected by the robot during Q-Learning grows with time. Both the exploration policy $\pi_e$ and the estimate $\hat{Q}$ evolve as the robot collects more data. This gives us a key insight into why Q-Learning works well. Consider a state $s$: if a particular action $a$ has a large value under the current estimate $\hat{Q}(s,a)$, then both the $\epsilon$-greedy and the softmax exploration policies have a larger probability of picking this action. If this action actually is _not_ the ideal action, then the future states that arise from this action will have poor rewards. The next update of the Q-Learning objective will therefore reduce the value $\hat{Q}(s,a)$, which will reduce the probability of picking this action the next time the robot visits state $s$. Bad actions, e.g., ones whose value is overestimated in $\hat{Q}(s,a)$, are explored by the robot but their value is correct in the next update of the Q-Learning objective. Good actions, e.g., whose value $\hat{Q}(s,a)$ is large, are explored more often by the robot and thereby reinforced. This property can be used to show that Q-Learning can converge to the optimal policy even if it begins with a random policy $\pi_e$.

This ability to not only collect new data but also collect the right kind of data is the central feature of reinforcement learning algorithms, and this is what distinguishes them from supervised learning. Q-Learning, using deep neural networks (which we will see in the DQN chapter later), is responsible for the resurgence of reinforcement learning.

### **Detailed Explanation of the Self-correcting Property**

The self-correcting property of Q-Learning is a crucial aspect that contributes to its effectiveness and robustness. It allows the algorithm to recover from initial suboptimal estimates and converge towards the optimal action-value function $Q^*$ and optimal policy $\pi^*$. Let's delve deeper into how this property works:

1. **Exploration-Exploitation Trade-off**: The exploration policy $\pi_e$ balances exploration and exploitation. It encourages the agent to explore actions that currently have high estimated Q-values ($\hat{Q}(s, a)$) more frequently. This is achieved through strategies like $\epsilon$-greedy or softmax exploration.

2. **Overestimation Correction**: If an action $a$ in state $s$ has an overestimated Q-value $\hat{Q}(s, a)$, it will be selected more often by the exploration policy. However, when this action is executed, the resulting next states will likely have lower rewards than expected. This is because the true optimal Q-value $Q^*(s, a)$ is lower than the current estimate.

3. **Q-Value Update**: In the next update of the Q-Learning objective, the Q-value $\hat{Q}(s, a)$ will be adjusted based on the observed rewards and the estimated Q-values of the next states. The update rule will reduce the Q-value of the overestimated action, bringing it closer to the true optimal value.

4. **Probability Adjustment**: As a result of the Q-value update, the probability of selecting the overestimated action in state $s$ will decrease in subsequent visits to that state. The exploration policy will adapt and favor other actions that now have relatively higher Q-values.

5. **Iterative Refinement**: This process of exploring, observing rewards, updating Q-values, and adjusting action probabilities continues iteratively. Over time, the Q-values converge towards the optimal values, and the exploration policy gradually shifts towards the optimal policy.

The self-correcting property ensures that Q-Learning can recover from initial misestimations and converge to the optimal solution, even if it starts with a random or suboptimal policy. It allows the algorithm to gather informative data and refine its estimates based on the observed rewards and state transitions.

This property, combined with the use of powerful function approximators like deep neural networks, has led to significant breakthroughs in reinforcement learning, enabling agents to learn complex tasks and achieve remarkable performance in various domains.

## 5. Implementation of Q-Learning

We now show how to implement Q-Learning on FrozenLake from [Open AI\\
Gym](https://gym.openai.com). Note this is the same setup as we
consider in [Value Iteration](value-iter.html#sec-valueiter) experiment.

```python
%matplotlib inline
import random
import numpy as np
from d2l import torch as d2l

seed = 0  # Random number generator seed
gamma = 0.95  # Discount factor
num_iters = 256  # Number of iterations
alpha   = 0.9  # Learing rate
epsilon = 0.9  # Epsilon in epsilion gready algorithm
random.seed(seed)  # Set the random seed
np.random.seed(seed)

# Now set up the environment
env_info = d2l.make_env('FrozenLake-v1', seed=seed)

```

In the FrozenLake environment, the robot moves on a 4×4
grid (these are the states) with actions that are “up”
(↑), “down” (→), “left”
(←), and “right” (→). The environment
contains a number of holes (H) cells and frozen (F) cells as well as a
goal cell (G), all of which are unknown to the robot. To keep the
problem simple, we assume the robot has reliable actions,
i.e.  $P(s′∣s,a)=1$ for all
$s∈S,a∈A$. If the robot reaches the
goal, the trial ends and the robot receives a reward of 1
irrespective of the action; the reward at any other state is 0
for all actions. The objective of the robot is to learn a policy that
reaches the goal location (G) from a given start location (S) (this is
s0) to maximize the _return_.

We first implement ϵ-greedy method as follows:

```python
def e_greedy(env, Q, s, epsilon):
    if random.random() < epsilon:
        return env.action_space.sample()

    else:
        return np.argmax(Q[s,:])

```

We are now ready to implement Q-learning:

```python
def q_learning(env_info, gamma, num_iters, alpha, epsilon):
    env_desc = env_info['desc']  # 2D array specifying what each grid item means
    env = env_info['env']  # 2D array specifying what each grid item means
    num_states = env_info['num_states']
    num_actions = env_info['num_actions']

    Q  = np.zeros((num_states, num_actions))
    V  = np.zeros((num_iters + 1, num_states))
    pi = np.zeros((num_iters + 1, num_states))

    for k in range(1, num_iters + 1):
        # Reset environment
        state, done = env.reset(), False
        while not done:
            # Select an action for a given state and acts in env based on selected action
            action = e_greedy(env, Q, state, epsilon)
            next_state, reward, done, _ = env.step(action)

            # Q-update:
            y = reward + gamma * np.max(Q[next_state,:])
            Q[state, action] = Q[state, action] + alpha * (y - Q[state, action])

            # Move to the next state
            state = next_state
        # Record max value and max action for visualization purpose only
        for s in range(num_states):
            V[k,s]  = np.max(Q[s,:])
            pi[k,s] = np.argmax(Q[s,:])
    d2l.show_Q_function_progress(env_desc, V[:-1], pi[:-1])

q_learning(env_info=env_info, gamma=gamma, num_iters=num_iters, alpha=alpha, epsilon=epsilon)

```

![png](/static/courses/Introduction-to-Reinforcement-Learning/output_qlearning_1ebe2b_5_0.png)

This result shows that Q-learning can find the optimal solution for this
problem roughly after 250 iterations. However, when we compare this
result with the Value Iteration algorithm’s result (see
[Implementation of Value Iteration](value-iter.html#subsec-valueitercode)), we can see that the Value Iteration
algorithm needs way fewer iterations to find the optimal solution for
this problem. This happens because the Value Iteration algorithm has
access to the full MDP whereas Q-learning does not.

## 6. Summary

Q-learning is one of the most fundamental reinforcement-learning
algorithms. It has been at the epicenter of the recent success of
reinforcement learning, most notably in learning to play video games
([Mnih _et al._, 2013](https://arxiv.org/abs/1312.5602 'Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). Playing Atari with deep reinforcement learning. ArXiv:1312.5602.')). Implementing Q-learning does not require that
we know the Markov decision process (MDP), e.g., the transition and
reward functions, completely.
